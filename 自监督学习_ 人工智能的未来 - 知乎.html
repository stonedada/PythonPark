<!DOCTYPE html>
<!-- saved from url=(0038)https://zhuanlan.zhihu.com/p/270547809 -->
<html lang="zh" data-hairline="true" class="itcauecng" data-theme="light" data-rh="data-theme"><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8"><title>自监督学习: 人工智能的未来 - 知乎</title><meta name="viewport" content="width=device-width,initial-scale=1,maximum-scale=1"><meta name="renderer" content="webkit"><meta name="force-rendering" content="webkit"><meta http-equiv="X-UA-Compatible" content="IE=10,chrome=1"><meta name="google-site-verification" content="FTeR0c8arOPKh8c5DYh_9uu98_zJbaWw53J-Sch9MTg"><meta data-rh="true" name="keywords" content="深度学习（Deep Learning）,半监督学习,无监督学习"><meta data-rh="true" name="description" content="原创：知乎专栏 搜索推荐广告排序艺术，转载请联系作者。导读：什么是自监督学习？为什么自监督学习是AI的未来？自监督学习如何实现？本文将回顾下自监督学习的前世今生，介绍它在CV、NLP、Graph、RecSys、RL等领…"><meta data-rh="true" property="og:title" content="自监督学习: 人工智能的未来"><meta data-rh="true" property="og:url" content="https://zhuanlan.zhihu.com/p/270547809"><meta data-rh="true" property="og:description" content="原创：知乎专栏 搜索推荐广告排序艺术，转载请联系作者。导读：什么是自监督学习？为什么自监督学习是AI的未来？自监督学习如何实现？本文将回顾下自监督学习的前世今生，介绍它在CV、NLP、Graph、RecSys、RL等领…"><meta data-rh="true" property="og:image" content="https://picx.zhimg.com/v2-11303ae8c2a58596a23d75fe9cd6e1c4_720w.jpg?source=172ae18b"><meta data-rh="true" property="og:type" content="article"><meta data-rh="true" property="og:site_name" content="知乎专栏"><link data-rh="true" rel="apple-touch-icon" href="https://static.zhihu.com/heifetz/assets/apple-touch-icon-152.81060cab.png"><link data-rh="true" rel="apple-touch-icon" href="https://static.zhihu.com/heifetz/assets/apple-touch-icon-152.81060cab.png" sizes="152x152"><link data-rh="true" rel="apple-touch-icon" href="https://static.zhihu.com/heifetz/assets/apple-touch-icon-120.d5793cac.png" sizes="120x120"><link data-rh="true" rel="apple-touch-icon" href="https://static.zhihu.com/heifetz/assets/apple-touch-icon-76.7abf3393.png" sizes="76x76"><link data-rh="true" rel="apple-touch-icon" href="https://static.zhihu.com/heifetz/assets/apple-touch-icon-60.362a8eac.png" sizes="60x60"><link crossorigin="" rel="shortcut icon" type="image/x-icon" href="https://static.zhihu.com/heifetz/favicon.ico"><link crossorigin="" rel="search" type="application/opensearchdescription+xml" href="https://static.zhihu.com/heifetz/search.xml" title="知乎"><link rel="dns-prefetch" href="https://static.zhimg.com/"><link rel="dns-prefetch" href="https://pica.zhimg.com/"><link rel="dns-prefetch" href="https://pic1.zhimg.com/"><link rel="dns-prefetch" href="https://pic2.zhimg.com/"><link rel="dns-prefetch" href="https://pic3.zhimg.com/"><link rel="dns-prefetch" href="https://pic4.zhimg.com/"><link rel="dns-prefetch" href="https://static.zhihu.com/"><script nonce="" data-web-reporter-config="{&quot;platform&quot;:&quot;web&quot;,&quot;project&quot;:&quot;heifetz&quot;}">!function(e,t){"object"==typeof exports&&"undefined"!=typeof module?t(exports):"function"==typeof define&&define.amd?define(["exports"],t):t((e=e||self).webReporter={})}(this,function(e){"use strict";var t={},n=!1,o=function(){var e,o,r,a,i;return n||(e=document.querySelector("script[data-web-reporter-config]"),o=e&&e.dataset.webReporterConfig||"{}",r=JSON.parse(o),a=r.platform,i=r.project,t={platform:a,project:i},n=!0),t};function r(e){return a(function(){return localStorage.getItem(e)})()}function a(e){return function(){try{return e.apply(void 0,arguments)}catch(e){}}}var i=a(function(e,t){var n={platform:"web",project:o().project,clientTimestamp:+new Date};!function(e,t,n){"1"===r("weber:logenabled")&&console.log("[web-reporter]%o",{type:e,base:t,data:n})}(e,n,t),function(e,t){var n=btoa(JSON.stringify(t));if("undefined"!=typeof Blob&&window.navigator&&window.navigator.sendBeacon){var o=new Blob([n],{type:"text/plain"});navigator.sendBeacon(e,o)}else{var r=new XMLHttpRequest;r.open("POST",e),r.withCredentials=!1,r.setRequestHeader("Content-Type","text/plain;charset=UTF-8"),r.send(n)}}(r("weber:api")||"https://apm.zhihu.com/collector/web_json",{type:e,base:n,data:t})});e.report=i,Object.defineProperty(e,"__esModule",{value:!0})});
</script><link href="./自监督学习_ 人工智能的未来 - 知乎_files/6335.216a26f4.bb9440efae869acbcdc4.css" crossorigin="" rel="stylesheet"><link href="./自监督学习_ 人工智能的未来 - 知乎_files/column.216a26f4.5e58f51b4aa9cc5b7729.css" crossorigin="" rel="stylesheet"><link rel="stylesheet" type="text/css" href="./自监督学习_ 人工智能的未来 - 知乎_files/user-hover-card.216a26f4.7f1de7c441470974ce17.css" crossorigin="anonymous"><link rel="stylesheet" type="text/css" href="./自监督学习_ 人工智能的未来 - 知乎_files/Labels.216a26f4.81c9ce8725560c5bcc6a.css" crossorigin="anonymous"><link rel="stylesheet" type="text/css" href="./自监督学习_ 人工智能的未来 - 知乎_files/GoodsRecommendGoodsCardList.216a26f4.d95ce79191cdf8d7ac28.css" crossorigin="anonymous"><link rel="stylesheet" type="text/css" href="./自监督学习_ 人工智能的未来 - 知乎_files/1384.216a26f4.c913eb13125dfc571a49.css" crossorigin="anonymous"><link rel="stylesheet" type="text/css" href="./自监督学习_ 人工智能的未来 - 知乎_files/5258.216a26f4.fe1f5f6b7e97e2ffab59.css" crossorigin="anonymous"><link rel="stylesheet" type="text/css" href="./自监督学习_ 人工智能的未来 - 知乎_files/ECommerceAd.216a26f4.53d1c79dfbd546fb2ace.css" crossorigin="anonymous"><link rel="stylesheet" type="text/css" href="./自监督学习_ 人工智能的未来 - 知乎_files/5187.216a26f4.cd5ec548fba27ea709fd.css" crossorigin="anonymous"><link rel="stylesheet" type="text/css" href="./自监督学习_ 人工智能的未来 - 知乎_files/EditableV2.216a26f4.86147906b330fd7351a9.css" crossorigin="anonymous"><script nonce="">!function(){"use strict";!function(e,n){var r=[];function t(e){return function(){r.push([e,arguments])}}n.Raven={captureException:t("captureException"),captureMessage:t("captureMessage"),captureBreadcrumb:t("captureBreadcrumb")};var a,o,c,i,s,u="undefined"!=typeof DOMError;function d(e){var n=e instanceof Error||e instanceof ErrorEvent||u&&e instanceof DOMError||e instanceof DOMException;Raven.captureException(n?e:new Error(e.message||e.reason))}n.addEventListener("unhandledrejection",d),n.addEventListener("error",d,!0),a=e.src,o=e,c=function(){r.forEach(function(e){var n;(n=Raven)[e[0]].apply(n,e[1])}),n.removeEventListener("unhandledrejection",d),n.removeEventListener("error",d,!0)},i=document.head||document.getElementsByTagName("head")[0],(s=document.createElement("script")).crossOrigin=o.crossOrigin,s.dataset.sentryConfig=o["data-sentry-config"],s.onload=c,s.src=a,i.appendChild(s)}({"defer":true,"crossOrigin":"anonymous","src":"https://unpkg.zhimg.com/@cfe/sentry-script@1.3.1/dist/init.js","data-sentry-config":"{\"dsn\":\"https://2d8d764432cc4f6fb3bc78ab9528299d@crash2.zhihu.com/1224\",\"sampleRate\":0.1,\"release\":\"538-e2fa9a97\",\"ignoreErrorNames\":[\"NetworkError\",\"SecurityError\"],\"ignoreErrorsPreset\":\"ReactApp\",\"tags\":{\"app_name\":\"heifetz\"}}"},window)}();
</script><script crossorigin="anonymous" data-sentry-config="{&quot;dsn&quot;:&quot;https://2d8d764432cc4f6fb3bc78ab9528299d@crash2.zhihu.com/1224&quot;,&quot;sampleRate&quot;:0.1,&quot;release&quot;:&quot;538-e2fa9a97&quot;,&quot;ignoreErrorNames&quot;:[&quot;NetworkError&quot;,&quot;SecurityError&quot;],&quot;ignoreErrorsPreset&quot;:&quot;ReactApp&quot;,&quot;tags&quot;:{&quot;app_name&quot;:&quot;heifetz&quot;}}" src="./自监督学习_ 人工智能的未来 - 知乎_files/init.js.下载"></script><style data-emotion-css="uzm3ri">.css-uzm3ri{position:fixed;top:0;right:0;left:0;z-index:101;display:none;height:2px;pointer-events:none;background:#056DE8;-webkit-transform:translateX(-100%);-ms-transform:translateX(-100%);transform:translateX(-100%);}</style><style data-emotion-css="1l12z7y">.css-1l12z7y{box-shadow:0px 16px 32px rgba(0,0,0,0.04);}</style><style data-emotion-css="1hlrcxk">.css-1hlrcxk{-webkit-transition-property:fill;transition-property:fill;-webkit-transition-duration:0.25s;transition-duration:0.25s;-webkit-transition-timing-function:ease-in;transition-timing-function:ease-in;}</style><style data-emotion-css="icip60">.css-icip60{border-radius:2px;}</style><style data-emotion-css="1tzrnvf">.css-1tzrnvf{box-sizing:border-box;margin:0;min-width:0;max-width:100%;height:auto;background-color:#FFFFFF;width:30px;height:30px;border-radius:2px;}</style><style data-emotion-css="78p1r9">.css-78p1r9{box-sizing:border-box;margin:0;min-width:0;margin-left:auto;margin-right:auto;max-width:690px;margin-top:0;}@media screen and (min-width:40em){.css-78p1r9{margin-top:1em;}}</style><style data-emotion-css="m9mk0j">.css-m9mk0j{position:relative;padding-bottom:56.25%;height:0;border-radius:inherit;}</style><style data-emotion-css="vn9cxj">.css-vn9cxj{box-sizing:border-box;margin:0;min-width:0;position:relative;padding-bottom:56.25%;height:0;border-radius:inherit;}</style><style data-emotion-css="1ld0bim">.css-1ld0bim{position:absolute;top:0;left:0;width:100%;height:100%;border-radius:inherit;}</style><style data-emotion-css="1ujtx97">.css-1ujtx97{object-fit:cover;background-color:#F6F6F6;}</style><style data-emotion-css="uodor8">.css-uodor8{border-radius:50%;}</style><style data-emotion-css="1y9jkzv">.css-1y9jkzv{box-sizing:border-box;margin:0;min-width:0;max-width:100%;height:auto;background-color:#FFFFFF;width:38px;height:38px;border-radius:50%;}</style><style data-emotion-css="1cd9gw4">.css-1cd9gw4{margin-left:.3em;}</style><style data-emotion-css="n99yhz">.css-n99yhz{box-sizing:border-box;margin:0;min-width:0;color:#175199;display:inline-block;margin-left:.3em;}</style><style data-emotion-css="18biwo">.css-18biwo{display:-webkit-inline-box;display:-webkit-inline-flex;display:-ms-inline-flexbox;display:inline-flex;-webkit-align-items:center;-webkit-box-align:center;-ms-flex-align:center;align-items:center;}</style><style data-emotion-css="1ifz0go">.css-1ifz0go{overflow:visible!important;}</style><style data-emotion-css="2dtzk2">.css-2dtzk2{cursor:pointer;display:-webkit-inline-box;display:-webkit-inline-flex;display:-ms-inline-flexbox;display:inline-flex;-webkit-align-items:center;-webkit-box-align:center;-ms-flex-align:center;align-items:center;margin-left:.3em;}</style><style data-emotion-css="1m3x3v9">.css-1m3x3v9{width:1em;height:1em;}</style><style data-emotion-css="1yuhvjn">.css-1yuhvjn{margin-top:16px;}</style><style data-emotion-css="376mun">.css-376mun{position:relative;display:inline;}</style><style data-emotion-css="1hhle02">.css-1hhle02 .FileLinkCard{-webkit-align-items:center;-webkit-box-align:center;-ms-flex-align:center;align-items:center;background-color:rgba(246,246,246,0.88);border-radius:12px;box-sizing:border-box;display:-webkit-box;display:-webkit-flex;display:-ms-flexbox;display:flex;margin:1em auto;max-width:100%;overflow:hidden;padding:12px;position:relative;width:390px;}.css-1hhle02 .FileLinkCard-icon{-webkit-flex-shrink:0;-ms-flex-negative:0;flex-shrink:0;height:30px;width:30px;}.css-1hhle02 .FileLinkCard-info{margin-left:12px;}.css-1hhle02 .FileLinkCard-name{color:#121212;font-size:15px;font-weight:500;line-height:21px;display:-webkit-box;text-overflow:ellipsis;overflow:hidden;-webkit-box-orient:vertical;-webkit-line-clamp:2;}.css-1hhle02 .FileLinkCard-meta{color:#999999;font-size:12px;display:-webkit-box;display:-webkit-flex;display:-ms-flexbox;display:flex;line-height:14px;margin-top:5px;}.css-1hhle02 .FileLinkCard-source{white-space:pre;}.css-1hhle02 img[data-uncomfortable]{content:url(data:image/svg+xml,%3Csvg%20xmlns%3D%22http%3A%2F%2Fwww.w3.org%2F2000%2Fsvg%22%20viewBox%3D%220%200%20344.88888888888886%20194%22%3E%3CforeignObject%20width%3D%22344.88888888888886%22%20height%3D%22194%22%3E%0A%20%20%20%20%20%20%3Cdiv%20xmlns%3D%22http%3A%2F%2Fwww.w3.org%2F1999%2Fxhtml%22%20style%3D%22font-size%3A%2013px%3B%20font-family%3A%20-apple-system%2C%20BlinkMacSystemFont%2C%20Microsoft%20YaHei%2C%20sans-serif%3B%20color%3A%20%23fff%3B%20width%3A100%25%3B%20height%3A194px%3B%22%3E%0A%20%20%20%20%20%20%20%20%3Cdiv%20style%3D%22display%3A%20flex%3B%20flex-direction%3A%20column%3B%20align-items%3A%20center%3B%20justify-content%3A%20center%3B%20height%3A%20100%25%3B%22%3E%0A%20%20%20%20%20%20%20%20%20%20%3Csvg%20xmlns%3D%22http%3A%2F%2Fwww.w3.org%2F2000%2Fsvg%22%20width%3D%2218%22%20height%3D%2218%22%20viewBox%3D%220%200%2016%2016%22%20fill%3D%22currentColor%22%3E%3Cpath%20d%3D%22M8%203.65a7%207%200%2000-1.353.128.65.65%200%2011-.25-1.275A8.3%208.3%200%20018%202.35c2.387%200%204.172.954%205.357%202.125C14.511%205.615%2015.15%207.022%2015.15%208c0%20.621-.257%201.391-.699%202.134a7.076%207.076%200%2001-1.403%201.68l.495.46a.65.65%200%2011-.886.951l-.998-.929a.645.645%200%2001-.104-.097L9.73%2010.501a.647.647%200%2001-.29.301%203.15%203.15%200%2001-4.313-4.094.647.647%200%2001.234-.275L3.908%205.08a5.774%205.774%200%2000-1.283%201.522C2.282%207.198%202.15%207.707%202.15%208c0%20.522.41%201.616%201.407%202.6.965.954%202.43%201.75%204.443%201.75.468%200%20.905-.043%201.311-.12a.65.65%200%2001.243%201.277A8.322%208.322%200%20018%2013.65c-2.387%200-4.172-.954-5.357-2.125C1.49%2010.385.85%208.978.85%208c0-.598.238-1.333.648-2.046A7.054%207.054%200%20012.95%204.188l-.547-.509a.65.65%200%2011.886-.951l8.8%208.194a5.793%205.793%200%20001.244-1.453c.372-.624.516-1.163.516-1.469%200-.522-.41-1.616-1.407-2.6-.965-.954-2.43-1.75-4.443-1.75zM6.29%207.296a1.85%201.85%200%20002.534%202.36l-2.535-2.36zM8%204.85a.65.65%200%20100%201.3%201.85%201.85%200%20011.843%201.694.65.65%200%20101.296-.11A3.15%203.15%200%20008%204.85z%22%20fill-rule%3D%22evenodd%22%20clip-rule%3D%22evenodd%22%3E%3C%2Fpath%3E%3C%2Fsvg%3E%0A%20%20%20%20%20%20%20%20%20%20%3Cdiv%20style%3D%22margin%3A%20.6em%200%201.2em%22%3E%E8%AF%A5%E5%9B%BE%E7%89%87%E6%9C%89%E5%8F%AF%E8%83%BD%E4%BC%9A%E5%BC%95%E8%B5%B7%E4%B8%8D%E9%80%82%3C%2Fdiv%3E%0A%20%20%20%20%20%20%20%20%20%20%3Cbutton%20style%3D%22padding%3A%204px%201em%3B%20font-size%3A%201.1em%3B%20color%3A%20inherit%3B%20background%3A%20none%3B%20border%3A%201px%20solid%20rgba%28255%2C255%2C255%2C.5%29%3B%20border-radius%3A%209999px%3B%22%3E%E7%BB%A7%E7%BB%AD%E6%9F%A5%E7%9C%8B%3C%2Fbutton%3E%0A%20%20%20%20%20%20%20%20%3C%2Fdiv%3E%0A%20%20%20%20%20%20%3C%2Fdiv%3E%0A%20%20%20%20%3C%2FforeignObject%3E%3C%2Fsvg%3E);width:100%;height:194px;background:url(https://pic1.zhimg.com/v2-cf70d0759d787c70091857151c1cad4a.jpeg) no-repeat rgba(191,191,191,0.7);background-size:cover;cursor:pointer!important;}</style><style data-emotion-css="1wr1m8">.css-1wr1m8 .LinkCard.new{position:relative;display:-webkit-box;display:-webkit-flex;display:-ms-flexbox;display:flex;box-sizing:border-box;-webkit-flex-direction:row;-ms-flex-direction:row;flex-direction:row;-webkit-align-items:center;-webkit-box-align:center;-ms-flex-align:center;align-items:center;width:390px;min-height:84px;border-radius:8px;max-width:100%;overflow:hidden;margin:16px auto;padding:12px 12px 9px 12px;background-color:#F6F6F6;}.css-1wr1m8 .LinkCard.new,.css-1wr1m8 .LinkCard.new:hover{-webkit-text-decoration:none;text-decoration:none;border:none !important;color:inherit !important;}.css-1wr1m8 .LinkCard.new .LinkCard-contents{display:block;-webkit-flex:1 1 auto;-ms-flex:1 1 auto;flex:1 1 auto;position:relative;}.css-1wr1m8 .LinkCard.new .LinkCard-contents .loading{height:14px;background:#EBEBEB;border-radius:7px;}.css-1wr1m8 .LinkCard.new .LinkCard-contents.withTitle{margin-bottom:3px;}.css-1wr1m8 .LinkCard.new .LinkCard-title{display:-webkit-box;font-size:15px;font-weight:500;line-height:1.4;margin-bottom:2px;color:#121212;text-overflow:ellipsis;overflow:hidden;-webkit-box-orient:vertical;-webkit-line-clamp:1;}.css-1wr1m8 .LinkCard.new .LinkCard-title.two-line{line-height:20px;display:-webkit-box;text-overflow:ellipsis;overflow:hidden;-webkit-box-orient:vertical;-webkit-line-clamp:2;}.css-1wr1m8 .LinkCard.new .LinkCard-title.loading{margin-bottom:8px;width:80%;}.css-1wr1m8 .LinkCard.new .LinkCard-title.loading.withTitle{margin-bottom:6px;}.css-1wr1m8 .LinkCard.new .LinkCard-title.loadingTitle{margin-bottom:5px;}.css-1wr1m8 .LinkCard.new .LinkCard-excerpt{display:-webkit-box;text-overflow:ellipsis;font-size:13px;line-height:18px;color:#999999;margin-bottom:4px;overflow:hidden;-webkit-box-orient:vertical;-webkit-line-clamp:1;}.css-1wr1m8 .LinkCard.new .LinkCard-excerpt .LinkCard-author{color:#444444;}.css-1wr1m8 .LinkCard.new .LinkCard-desc{display:-webkit-box;font-size:13px;height:18px;line-height:18px;color:#999999;word-break:break-all;text-overflow:ellipsis;overflow:hidden;-webkit-box-orient:vertical;-webkit-line-clamp:1;}.css-1wr1m8 .LinkCard.new .LinkCard-desc .LinkCard-tag,.css-1wr1m8 .LinkCard.new .LinkCard-desc .tag{display:inline-block;font-size:11px;margin-left:8px;padding:0 4px;border-radius:3px;background:rgba(211,211,211,0.3);}.css-1wr1m8 .LinkCard.new .LinkCard-desc.loading{width:40%;}.css-1wr1m8 .LinkCard.new .LinkCard-desc svg{margin-right:2px;}.css-1wr1m8 .LinkCard.new .LinkCard-image{-webkit-flex:0 0 auto;-ms-flex:0 0 auto;flex:0 0 auto;background-color:#EBEBEB;background-size:cover;background-position:center;position:relative;display:block;width:60px;height:60px;margin-left:20px;object-fit:cover;border-radius:inherit;overflow:hidden;}.css-1wr1m8 .LinkCard.new .LinkCard-image.LinkCard-image--default{display:-webkit-box;display:-webkit-flex;display:-ms-flexbox;display:flex;-webkit-align-items:center;-webkit-box-align:center;-ms-flex-align:center;align-items:center;-webkit-box-pack:center;-webkit-justify-content:center;-ms-flex-pack:center;justify-content:center;background-color:#EBEBEB;color:#D3D3D3;}.css-1wr1m8 .LinkCard.new .LinkCard-image.LinkCard-image--default svg{color:#999999;}.css-1wr1m8 .LinkCard.new .LinkCard-image img{width:100%;height:100%;object-fit:cover;}.css-1wr1m8 .LinkCard.new .LinkCard-image .LinkCard-image--video{display:-webkit-box;display:-webkit-flex;display:-ms-flexbox;display:flex;-webkit-align-items:center;-webkit-box-align:center;-ms-flex-align:center;align-items:center;-webkit-box-pack:center;-webkit-justify-content:center;-ms-flex-pack:center;justify-content:center;position:absolute;top:50%;left:50%;-webkit-transform:translateX(-50%) translateY(-50%);-ms-transform:translateX(-50%) translateY(-50%);transform:translateX(-50%) translateY(-50%);width:24px;height:24px;border-radius:12px;background:rgba(255,255,255,0.9);pointer-events:none;}.css-1wr1m8 .LinkCard.new .LinkCard-image .LinkCard-image--video svg{color:#444444;}.css-1wr1m8 .LinkCard.new .LinkCard-richText .text{color:#444444;}.css-1wr1m8 .LinkCard.new .LinkCard-richText .bold{font-weight:600;}.css-1wr1m8 .LinkCard.new .LinkCard-richText .tag{margin-left:4px;}.css-1wr1m8 .LinkCard.old{position:relative;display:block;margin:1em auto;width:390px;box-sizing:border-box;border-radius:12px;max-width:100%;overflow:hidden;}.css-1wr1m8 .LinkCard.old,.css-1wr1m8 .LinkCard.old:hover{-webkit-text-decoration:none;text-decoration:none;border:none !important;color:inherit !important;}.css-1wr1m8 .LinkCard-ecommerceLoadingCard{position:relative;display:-webkit-box;display:-webkit-flex;display:-ms-flexbox;display:flex;-webkit-align-items:center;-webkit-box-align:center;-ms-flex-align:center;align-items:center;-webkit-box-pack:justify;-webkit-justify-content:space-between;-ms-flex-pack:justify;justify-content:space-between;padding:12px;border-radius:inherit;height:80px;box-sizing:border-box;background:rgba(246,246,246,0.88);color:#D3D3D3;}.css-1wr1m8 .LinkCard-ecommerceLoadingCardAvatarWrapper{width:60px;height:60px;background:#EBEBEB;display:-webkit-box;display:-webkit-flex;display:-ms-flexbox;display:flex;-webkit-align-items:center;-webkit-box-align:center;-ms-flex-align:center;align-items:center;-webkit-box-pack:center;-webkit-justify-content:center;-ms-flex-pack:center;justify-content:center;border-radius:6px;margin-right:10px;}.css-1wr1m8 .LinkCard-ecommerceLoadingCardNetwork{width:20px;height:20px;}.css-1wr1m8 .LinkCard-ecommerceLoadingCardLoadingbar{height:60px;-webkit-flex:1;-ms-flex:1;flex:1;display:-webkit-box;display:-webkit-flex;display:-ms-flexbox;display:flex;-webkit-flex-direction:column;-ms-flex-direction:column;flex-direction:column;}.css-1wr1m8 .LinkCard-ecommerceLoadingCardLoadingbar span{height:16px;display:inline-block;background:#EBEBEB;}.css-1wr1m8 .LinkCard-ecommerceLoadingCardLoadingbar span:nth-of-type(1){width:60px;margin-bottom:4px;}.css-1wr1m8 .LinkCard-ecommerceLoadingCardLoadingbar span:nth-of-type(2){width:127px;}</style><style data-emotion-css="1dnyyvy">.css-1dnyyvy .LinkCard.old{position:relative;display:block;margin:1em auto;width:390px;box-sizing:border-box;border-radius:12px;max-width:100%;overflow:hidden;}.css-1dnyyvy .LinkCard.old,.css-1dnyyvy .LinkCard.old:hover{-webkit-text-decoration:none;text-decoration:none;border:none !important;color:inherit !important;}.css-1dnyyvy .LinkCard-ecommerceLoadingCard{position:relative;display:-webkit-box;display:-webkit-flex;display:-ms-flexbox;display:flex;-webkit-align-items:center;-webkit-box-align:center;-ms-flex-align:center;align-items:center;-webkit-box-pack:justify;-webkit-justify-content:space-between;-ms-flex-pack:justify;justify-content:space-between;padding:12px;border-radius:inherit;height:80px;box-sizing:border-box;background:rgba(246,246,246,0.88);color:#D3D3D3;}.css-1dnyyvy .LinkCard-ecommerceLoadingCardAvatarWrapper{width:60px;height:60px;background:#EBEBEB;display:-webkit-box;display:-webkit-flex;display:-ms-flexbox;display:flex;-webkit-align-items:center;-webkit-box-align:center;-ms-flex-align:center;align-items:center;-webkit-box-pack:center;-webkit-justify-content:center;-ms-flex-pack:center;justify-content:center;border-radius:6px;margin-right:10px;}.css-1dnyyvy .LinkCard-ecommerceLoadingCardNetwork{width:20px;height:20px;}.css-1dnyyvy .LinkCard-ecommerceLoadingCardLoadingbar{height:60px;-webkit-flex:1;-ms-flex:1;flex:1;display:-webkit-box;display:-webkit-flex;display:-ms-flexbox;display:flex;-webkit-flex-direction:column;-ms-flex-direction:column;flex-direction:column;}.css-1dnyyvy .LinkCard-ecommerceLoadingCardLoadingbar span{height:16px;display:inline-block;background:#EBEBEB;}.css-1dnyyvy .LinkCard-ecommerceLoadingCardLoadingbar span:nth-of-type(1){width:60px;margin-bottom:4px;}.css-1dnyyvy .LinkCard-ecommerceLoadingCardLoadingbar span:nth-of-type(2){width:127px;}.css-1dnyyvy .LinkCard.new{position:relative;display:-webkit-box;display:-webkit-flex;display:-ms-flexbox;display:flex;box-sizing:border-box;-webkit-flex-direction:row;-ms-flex-direction:row;flex-direction:row;-webkit-align-items:center;-webkit-box-align:center;-ms-flex-align:center;align-items:center;width:390px;min-height:84px;border-radius:8px;max-width:100%;overflow:hidden;margin:16px auto;padding:12px 12px 9px 12px;background-color:#F6F6F6;}.css-1dnyyvy .LinkCard.new,.css-1dnyyvy .LinkCard.new:hover{-webkit-text-decoration:none;text-decoration:none;border:none !important;color:inherit !important;}.css-1dnyyvy .LinkCard.new .LinkCard-contents{display:block;-webkit-flex:1 1 auto;-ms-flex:1 1 auto;flex:1 1 auto;position:relative;}.css-1dnyyvy .LinkCard.new .LinkCard-contents .loading{height:14px;background:#EBEBEB;border-radius:7px;}.css-1dnyyvy .LinkCard.new .LinkCard-contents.withTitle{margin-bottom:3px;}.css-1dnyyvy .LinkCard.new .LinkCard-title{display:-webkit-box;font-size:15px;font-weight:500;line-height:1.4;margin-bottom:2px;color:#121212;text-overflow:ellipsis;overflow:hidden;-webkit-box-orient:vertical;-webkit-line-clamp:1;}.css-1dnyyvy .LinkCard.new .LinkCard-title.two-line{line-height:20px;display:-webkit-box;text-overflow:ellipsis;overflow:hidden;-webkit-box-orient:vertical;-webkit-line-clamp:2;}.css-1dnyyvy .LinkCard.new .LinkCard-title.loading{margin-bottom:8px;width:80%;}.css-1dnyyvy .LinkCard.new .LinkCard-title.loading.withTitle{margin-bottom:6px;}.css-1dnyyvy .LinkCard.new .LinkCard-title.loadingTitle{margin-bottom:5px;}.css-1dnyyvy .LinkCard.new .LinkCard-excerpt{display:-webkit-box;text-overflow:ellipsis;font-size:13px;line-height:18px;color:#999999;margin-bottom:4px;overflow:hidden;-webkit-box-orient:vertical;-webkit-line-clamp:1;}.css-1dnyyvy .LinkCard.new .LinkCard-excerpt .LinkCard-author{color:#444444;}.css-1dnyyvy .LinkCard.new .LinkCard-desc{display:-webkit-box;font-size:13px;height:18px;line-height:18px;color:#999999;word-break:break-all;text-overflow:ellipsis;overflow:hidden;-webkit-box-orient:vertical;-webkit-line-clamp:1;}.css-1dnyyvy .LinkCard.new .LinkCard-desc .LinkCard-tag,.css-1dnyyvy .LinkCard.new .LinkCard-desc .tag{display:inline-block;font-size:11px;margin-left:8px;padding:0 4px;border-radius:3px;background:rgba(211,211,211,0.3);}.css-1dnyyvy .LinkCard.new .LinkCard-desc.loading{width:40%;}.css-1dnyyvy .LinkCard.new .LinkCard-desc svg{margin-right:2px;}.css-1dnyyvy .LinkCard.new .LinkCard-image{-webkit-flex:0 0 auto;-ms-flex:0 0 auto;flex:0 0 auto;background-color:#EBEBEB;background-size:cover;background-position:center;position:relative;display:block;width:60px;height:60px;margin-left:20px;object-fit:cover;border-radius:inherit;overflow:hidden;}.css-1dnyyvy .LinkCard.new .LinkCard-image.LinkCard-image--default{display:-webkit-box;display:-webkit-flex;display:-ms-flexbox;display:flex;-webkit-align-items:center;-webkit-box-align:center;-ms-flex-align:center;align-items:center;-webkit-box-pack:center;-webkit-justify-content:center;-ms-flex-pack:center;justify-content:center;background-color:#EBEBEB;color:#D3D3D3;}.css-1dnyyvy .LinkCard.new .LinkCard-image.LinkCard-image--default svg{color:#999999;}.css-1dnyyvy .LinkCard.new .LinkCard-image img{width:100%;height:100%;object-fit:cover;}.css-1dnyyvy .LinkCard.new .LinkCard-image .LinkCard-image--video{display:-webkit-box;display:-webkit-flex;display:-ms-flexbox;display:flex;-webkit-align-items:center;-webkit-box-align:center;-ms-flex-align:center;align-items:center;-webkit-box-pack:center;-webkit-justify-content:center;-ms-flex-pack:center;justify-content:center;position:absolute;top:50%;left:50%;-webkit-transform:translateX(-50%) translateY(-50%);-ms-transform:translateX(-50%) translateY(-50%);transform:translateX(-50%) translateY(-50%);width:24px;height:24px;border-radius:12px;background:rgba(255,255,255,0.9);pointer-events:none;}.css-1dnyyvy .LinkCard.new .LinkCard-image .LinkCard-image--video svg{color:#444444;}.css-1dnyyvy .LinkCard.new .LinkCard-richText .text{color:#444444;}.css-1dnyyvy .LinkCard.new .LinkCard-richText .bold{font-weight:600;}.css-1dnyyvy .LinkCard.new .LinkCard-richText .tag{margin-left:4px;}.css-1dnyyvy .FileLinkCard{-webkit-align-items:center;-webkit-box-align:center;-ms-flex-align:center;align-items:center;background-color:rgba(246,246,246,0.88);border-radius:12px;box-sizing:border-box;display:-webkit-box;display:-webkit-flex;display:-ms-flexbox;display:flex;margin:1em auto;max-width:100%;overflow:hidden;padding:12px;position:relative;width:390px;}.css-1dnyyvy .FileLinkCard-icon{-webkit-flex-shrink:0;-ms-flex-negative:0;flex-shrink:0;height:30px;width:30px;}.css-1dnyyvy .FileLinkCard-info{margin-left:12px;}.css-1dnyyvy .FileLinkCard-name{color:#121212;font-size:15px;font-weight:500;line-height:21px;display:-webkit-box;text-overflow:ellipsis;overflow:hidden;-webkit-box-orient:vertical;-webkit-line-clamp:2;}.css-1dnyyvy .FileLinkCard-meta{color:#999999;font-size:12px;display:-webkit-box;display:-webkit-flex;display:-ms-flexbox;display:flex;line-height:14px;margin-top:5px;}.css-1dnyyvy .FileLinkCard-source{white-space:pre;}.css-1dnyyvy img[data-uncomfortable]{content:url(data:image/svg+xml,%3Csvg%20xmlns%3D%22http%3A%2F%2Fwww.w3.org%2F2000%2Fsvg%22%20viewBox%3D%220%200%20344.88888888888886%20194%22%3E%3CforeignObject%20width%3D%22344.88888888888886%22%20height%3D%22194%22%3E%0A%20%20%20%20%20%20%3Cdiv%20xmlns%3D%22http%3A%2F%2Fwww.w3.org%2F1999%2Fxhtml%22%20style%3D%22font-size%3A%2013px%3B%20font-family%3A%20-apple-system%2C%20BlinkMacSystemFont%2C%20Microsoft%20YaHei%2C%20sans-serif%3B%20color%3A%20%23fff%3B%20width%3A100%25%3B%20height%3A194px%3B%22%3E%0A%20%20%20%20%20%20%20%20%3Cdiv%20style%3D%22display%3A%20flex%3B%20flex-direction%3A%20column%3B%20align-items%3A%20center%3B%20justify-content%3A%20center%3B%20height%3A%20100%25%3B%22%3E%0A%20%20%20%20%20%20%20%20%20%20%3Csvg%20xmlns%3D%22http%3A%2F%2Fwww.w3.org%2F2000%2Fsvg%22%20width%3D%2218%22%20height%3D%2218%22%20viewBox%3D%220%200%2016%2016%22%20fill%3D%22currentColor%22%3E%3Cpath%20d%3D%22M8%203.65a7%207%200%2000-1.353.128.65.65%200%2011-.25-1.275A8.3%208.3%200%20018%202.35c2.387%200%204.172.954%205.357%202.125C14.511%205.615%2015.15%207.022%2015.15%208c0%20.621-.257%201.391-.699%202.134a7.076%207.076%200%2001-1.403%201.68l.495.46a.65.65%200%2011-.886.951l-.998-.929a.645.645%200%2001-.104-.097L9.73%2010.501a.647.647%200%2001-.29.301%203.15%203.15%200%2001-4.313-4.094.647.647%200%2001.234-.275L3.908%205.08a5.774%205.774%200%2000-1.283%201.522C2.282%207.198%202.15%207.707%202.15%208c0%20.522.41%201.616%201.407%202.6.965.954%202.43%201.75%204.443%201.75.468%200%20.905-.043%201.311-.12a.65.65%200%2001.243%201.277A8.322%208.322%200%20018%2013.65c-2.387%200-4.172-.954-5.357-2.125C1.49%2010.385.85%208.978.85%208c0-.598.238-1.333.648-2.046A7.054%207.054%200%20012.95%204.188l-.547-.509a.65.65%200%2011.886-.951l8.8%208.194a5.793%205.793%200%20001.244-1.453c.372-.624.516-1.163.516-1.469%200-.522-.41-1.616-1.407-2.6-.965-.954-2.43-1.75-4.443-1.75zM6.29%207.296a1.85%201.85%200%20002.534%202.36l-2.535-2.36zM8%204.85a.65.65%200%20100%201.3%201.85%201.85%200%20011.843%201.694.65.65%200%20101.296-.11A3.15%203.15%200%20008%204.85z%22%20fill-rule%3D%22evenodd%22%20clip-rule%3D%22evenodd%22%3E%3C%2Fpath%3E%3C%2Fsvg%3E%0A%20%20%20%20%20%20%20%20%20%20%3Cdiv%20style%3D%22margin%3A%20.6em%200%201.2em%22%3E%E8%AF%A5%E5%9B%BE%E7%89%87%E6%9C%89%E5%8F%AF%E8%83%BD%E4%BC%9A%E5%BC%95%E8%B5%B7%E4%B8%8D%E9%80%82%3C%2Fdiv%3E%0A%20%20%20%20%20%20%20%20%20%20%3Cbutton%20style%3D%22padding%3A%204px%201em%3B%20font-size%3A%201.1em%3B%20color%3A%20inherit%3B%20background%3A%20none%3B%20border%3A%201px%20solid%20rgba%28255%2C255%2C255%2C.5%29%3B%20border-radius%3A%209999px%3B%22%3E%E7%BB%A7%E7%BB%AD%E6%9F%A5%E7%9C%8B%3C%2Fbutton%3E%0A%20%20%20%20%20%20%20%20%3C%2Fdiv%3E%0A%20%20%20%20%20%20%3C%2Fdiv%3E%0A%20%20%20%20%3C%2FforeignObject%3E%3C%2Fsvg%3E);width:100%;height:194px;background:url(https://pic1.zhimg.com/v2-cf70d0759d787c70091857151c1cad4a.jpeg) no-repeat rgba(191,191,191,0.7);background-size:cover;cursor:pointer!important;}</style><style data-emotion-css="1g0fqss animation-1yvu044">.css-1g0fqss{word-break:break-word;line-height:1.6;}.css-1g0fqss > [data-first-child]{margin-top:0;}.css-1g0fqss > :last-child{margin-bottom:0;}.css-1g0fqss h1,.css-1g0fqss h2{clear:left;margin-top:calc((1.4em * 2) / 1.2);margin-bottom:calc(1.4em / 1.2);font-size:1.2em;line-height:1.5;font-weight:600;}.css-1g0fqss h3,.css-1g0fqss h4,.css-1g0fqss h5,.css-1g0fqss h6{clear:left;margin-top:calc((1.4em * 1.5) / 1.1);margin-bottom:calc(1.4em / 1.1);font-size:1.1em;line-height:1.5;font-weight:600;}.css-1g0fqss u{-webkit-text-decoration:none;text-decoration:none;border-bottom:1px solid #444444;}.css-1g0fqss b{font-weight:600;}.css-1g0fqss sup{font-size:0.8em;}.css-1g0fqss sup[data-draft-type='reference']{color:#175199;}.css-1g0fqss a:focus{outline:none;-webkit-transition:box-shadow 0.3s;transition:box-shadow 0.3s;}html[data-focus-visible] .css-1g0fqss a:focus{box-shadow:0 0 0 2px #FFFFFF,0 0 0 4px rgba(5,109,232,0.3);}.css-1g0fqss a.ztext-link,.css-1g0fqss a.internal,.css-1g0fqss a.external{-webkit-text-decoration:none;text-decoration:none;cursor:pointer;border-bottom:1px solid #808080;}.css-1g0fqss a.ztext-link:hover,.css-1g0fqss a.internal:hover,.css-1g0fqss a.external:hover{color:#175199;border-bottom:1px solid #175199;}.css-1g0fqss a.ztext-link > .ellipsis::after,.css-1g0fqss a.internal > .ellipsis::after,.css-1g0fqss a.external > .ellipsis::after{content:'...';}.css-1g0fqss a.ztext-link > .invisible,.css-1g0fqss a.internal > .invisible,.css-1g0fqss a.external > .invisible{font:0/0 a;color:transparent;text-shadow:none;background-color:transparent;}.css-1g0fqss a.ztext-link u,.css-1g0fqss a.internal u,.css-1g0fqss a.external u{border:none;}.css-1g0fqss a.member_mention{color:#175199;}.css-1g0fqss a.member_mention:hover{border-bottom:1px solid #175199;}.css-1g0fqss a.UserLink-link{color:#175199;}.css-1g0fqss a.UserLink-link:hover{border-bottom:1px solid #175199;}.css-1g0fqss p{margin:1.4em 0;}.css-1g0fqss p.ztext-empty-paragraph{margin:calc((2.8em- (1.4em * 2 + 1.6em)) / 2) 0;}.css-1g0fqss p.ztext-empty-paragraph + .ztext-empty-paragraph{margin:1.4em 0;}.css-1g0fqss hr{margin:4em auto;width:240px;max-width:100%;border:none;border-top:1px solid #D3D3D3;}.css-1g0fqss img[eeimg]{max-width:100%;vertical-align:middle;}.css-1g0fqss img[eeimg="1"]{margin:0 3px;max-width:calc(100% - 6px);display:inline-block;}.css-1g0fqss img[eeimg="2"]{margin:1.4em auto;display:block;}.css-1g0fqss blockquote{margin:1.4em 0;padding-left:1em;color:#646464;border-left:3px solid #D3D3D3;}.css-1g0fqss ol,.css-1g0fqss ul{margin:1.4em 0;padding:0;width:100%;}.css-1g0fqss ol ol,.css-1g0fqss ul ol,.css-1g0fqss ol ul,.css-1g0fqss ul ul{margin:0;}.css-1g0fqss ol li::before,.css-1g0fqss ul li::before{width:1em;}.css-1g0fqss ol > ol,.css-1g0fqss ul > ol,.css-1g0fqss ol > ul,.css-1g0fqss ul > ul{display:table-row;}.css-1g0fqss ol > ol::before,.css-1g0fqss ul > ol::before,.css-1g0fqss ol > ul::before,.css-1g0fqss ul > ul::before{display:table-cell;content:'';}.css-1g0fqss ul{display:table;}.css-1g0fqss ul>li{display:table-row;list-style:none;}.css-1g0fqss ul>li::before{display:table-cell;content:'•  ';white-space:pre;}.css-1g0fqss ol{display:table;counter-reset:ol;}.css-1g0fqss ol > li{display:table-row;list-style:none;}.css-1g0fqss ol > li::before{display:table-cell;text-align:right;counter-increment:ol;content:counter(ol) '. ';white-space:pre;}.css-1g0fqss ol ol{counter-reset:ol2;}.css-1g0fqss ol ol li::before{counter-increment:ol2;content:counter(ol2) '. ';}.css-1g0fqss ol ol ol{counter-reset:ol3;}.css-1g0fqss ol ol ol li::before{counter-increment:ol3;content:counter(ol3) '. ';}.css-1g0fqss ol ol ol ol{counter-reset:ol4;}.css-1g0fqss ol ol ol ol li::before{counter-increment:ol4;content:counter(ol4) '. ';}.css-1g0fqss figure{margin:1.4em 0;}.css-1g0fqss figure .content_image,.css-1g0fqss figure .origin_image{margin:0 auto;}.css-1g0fqss figure figcaption{margin-top:calc(0.6em / 0.9);padding:0 1em;font-size:0.9em;line-height:1.5;text-align:center;color:#999999;}.css-1g0fqss figure + figure{margin-top:calc(1.4em * 1.6);}.css-1g0fqss figure[data-size='small'],.css-1g0fqss figure:not([data-size]) > [data-size='small']{clear:both;}.css-1g0fqss figure[data-size='left'],.css-1g0fqss figure:not([data-size]) > [data-size='left']{float:left;margin:0 20px 20px 0;max-width:33%;}.css-1g0fqss figure[data-size='right'],.css-1g0fqss figure:not([data-size]) > [data-size='right']{float:right;margin:0 0 20px 20px;max-width:33%;}.css-1g0fqss figure[data-size='collapse']{margin-bottom:0;}.css-1g0fqss figure[data-size='collapse'] + figure{margin-top:0;}.css-1g0fqss .content_image,.css-1g0fqss .origin_image{display:block;max-width:100%;height:auto;margin:1.4em auto;}.css-1g0fqss .content_image[data-size='small'],.css-1g0fqss .origin_image[data-size='small']{max-width:40%;}.css-1g0fqss .content_image.zh-lightbox-thumb,.css-1g0fqss .origin_image.zh-lightbox-thumb{cursor:-webkit-zoom-in;cursor:-moz-zoom-in;cursor:zoom-in;}.css-1g0fqss code{margin:0 2px;padding:3px 4px;border-radius:3px;font-family:Menlo,Monaco,Consolas,'Andale Mono','lucida console','Courier New',monospace;font-size:0.9em;background-color:#F6F6F6;}.css-1g0fqss pre{margin:1.4em 0;padding:calc(0.8em / 0.9);font-size:0.9em;word-break:initial;word-wrap:initial;white-space:pre;overflow:auto;-webkit-overflow-scrolling:touch;background:#F6F6F6;border-radius:4px;}.css-1g0fqss pre code{margin:0;padding:0;font-size:inherit;border-radius:0;background-color:inherit;}.css-1g0fqss li pre{white-space:pre-wrap;}.css-1g0fqss table[data-draft-type='table']{border-collapse:collapse;font-size:15px;margin:1.4em auto;max-width:100%;table-layout:fixed;text-align:left;width:100%;}.css-1g0fqss table[data-draft-type='table'][data-size='small']{min-width:260px;width:40%;}.css-1g0fqss table[data-draft-type='table'][data-row-style='striped'] tr:nth-of-type(2n + 1){background:#F6F6F6;}.css-1g0fqss table[data-draft-type='table'] td,.css-1g0fqss table[data-draft-type='table'] th{border:1px solid #D3D3D3;line-height:24px;height:24px;padding:3px 12px;}.css-1g0fqss table[data-draft-type='table'] th{background:#EBEBEB;color:#121212;font-weight:500;}.css-1g0fqss .video-box,.css-1g0fqss .link-box{position:relative;display:-webkit-box;display:-webkit-flex;display:-ms-flexbox;display:flex;-webkit-box-pack:justify;-webkit-justify-content:space-between;-ms-flex-pack:justify;justify-content:space-between;margin:1.4em 0;overflow:auto;white-space:normal;cursor:pointer;border:solid 1px #EBEBEB;border-radius:4px;}.css-1g0fqss .lazy[data-lazy-status]{background-color:#F6F6F6;}.css-1g0fqss .lazy[data-lazy-status="ok"]{background-color:transparent;-webkit-animation:animation-1yvu044 0.5s ease-in;animation:animation-1yvu044 0.5s ease-in;}.css-1g0fqss .highlight{margin:1em 0;}.css-1g0fqss .highlight pre{margin:0;}.css-1g0fqss .highlight .hll{background-color:#FDFDFD;}.css-1g0fqss .highlight .c{font-style:italic;color:#999999;}.css-1g0fqss .highlight .err{color:#F1403C;}.css-1g0fqss .highlight .k{font-weight:600;}.css-1g0fqss .highlight .o{font-weight:600;}.css-1g0fqss .highlight .cm{font-style:italic;color:#999999;}.css-1g0fqss .highlight .cp{font-weight:600;color:#999999;}.css-1g0fqss .highlight .c1{font-style:italic;color:#999999;}.css-1g0fqss .highlight .cs{font-style:italic;font-weight:600;color:#999999;}.css-1g0fqss .highlight .gd{color:#FF3366;}.css-1g0fqss .highlight .ge{font-style:italic;}.css-1g0fqss .highlight .gr{color:#F1403C;}.css-1g0fqss .highlight .gh{color:#999999;}.css-1g0fqss .highlight .gi{color:#12b370;}.css-1g0fqss .highlight .go{color:#808080;}.css-1g0fqss .highlight .gp{color:#646464;}.css-1g0fqss .highlight .gs{font-weight:600;}.css-1g0fqss .highlight .gu{color:#999999;}.css-1g0fqss .highlight .gt{color:#F1403C;}.css-1g0fqss .highlight .kc{font-weight:600;}.css-1g0fqss .highlight .kd{font-weight:600;}.css-1g0fqss .highlight .kn{font-weight:600;}.css-1g0fqss .highlight .kp{font-weight:600;}.css-1g0fqss .highlight .kr{font-weight:600;}.css-1g0fqss .highlight .kt{font-weight:600;color:#175199;}.css-1g0fqss .highlight .m{color:#056DE8;}.css-1g0fqss .highlight .s{color:#F1403C;}.css-1g0fqss .highlight .na{color:#056DE8;}.css-1g0fqss .highlight .nb{color:#056DE8;}.css-1g0fqss .highlight .nc{font-weight:600;color:#175199;}.css-1g0fqss .highlight .no{color:#056DE8;}.css-1g0fqss .highlight .ni{color:#5555DD;}.css-1g0fqss .highlight .ne{font-weight:600;color:#F1403C;}.css-1g0fqss .highlight .nf{font-weight:600;color:#F1403C;}.css-1g0fqss .highlight .nn{color:#646464;}.css-1g0fqss .highlight .nt{color:#175199;}.css-1g0fqss .highlight .nv{color:#056DE8;}.css-1g0fqss .highlight .ow{font-weight:600;}.css-1g0fqss .highlight .w{color:#BFBFBF;}.css-1g0fqss .highlight .mf{color:#056DE8;}.css-1g0fqss .highlight .mh{color:#056DE8;}.css-1g0fqss .highlight .mi{color:#056DE8;}.css-1g0fqss .highlight .mo{color:#056DE8;}.css-1g0fqss .highlight .sb{color:#F1403C;}.css-1g0fqss .highlight .sc{color:#F1403C;}.css-1g0fqss .highlight .sd{color:#F1403C;}.css-1g0fqss .highlight .s2{color:#F1403C;}.css-1g0fqss .highlight .se{color:#F1403C;}.css-1g0fqss .highlight .sh{color:#F1403C;}.css-1g0fqss .highlight .si{color:#F1403C;}.css-1g0fqss .highlight .sx{color:#F1403C;}.css-1g0fqss .highlight .sr{color:#A5542F;}.css-1g0fqss .highlight .s1{color:#F1403C;}.css-1g0fqss .highlight .ss{color:#F1403C;}.css-1g0fqss .highlight .bp{color:#999999;}.css-1g0fqss .highlight .vc{color:#056DE8;}.css-1g0fqss .highlight .vg{color:#056DE8;}.css-1g0fqss .highlight .vi{color:#056DE8;}.css-1g0fqss .highlight .il{color:#056DE8;}.css-1g0fqss .highlight::-webkit-scrollbar{width:6px;height:6px;}.css-1g0fqss .highlight::-webkit-scrollbar-thumb:horizontal{background-color:rgba(18,18,18,0.5);border-radius:6px;}.css-1g0fqss .highlight::-webkit-scrollbar-thumb:horizontal:hover{background-color:rgba(18,18,18,0.6);}.css-1g0fqss .LinkCard.old{position:relative;display:block;margin:1em auto;width:390px;box-sizing:border-box;border-radius:12px;max-width:100%;overflow:hidden;}.css-1g0fqss .LinkCard.old,.css-1g0fqss .LinkCard.old:hover{-webkit-text-decoration:none;text-decoration:none;border:none !important;color:inherit !important;}.css-1g0fqss .LinkCard-ecommerceLoadingCard{position:relative;display:-webkit-box;display:-webkit-flex;display:-ms-flexbox;display:flex;-webkit-align-items:center;-webkit-box-align:center;-ms-flex-align:center;align-items:center;-webkit-box-pack:justify;-webkit-justify-content:space-between;-ms-flex-pack:justify;justify-content:space-between;padding:12px;border-radius:inherit;height:80px;box-sizing:border-box;background:rgba(246,246,246,0.88);color:#D3D3D3;}.css-1g0fqss .LinkCard-ecommerceLoadingCardAvatarWrapper{width:60px;height:60px;background:#EBEBEB;display:-webkit-box;display:-webkit-flex;display:-ms-flexbox;display:flex;-webkit-align-items:center;-webkit-box-align:center;-ms-flex-align:center;align-items:center;-webkit-box-pack:center;-webkit-justify-content:center;-ms-flex-pack:center;justify-content:center;border-radius:6px;margin-right:10px;}.css-1g0fqss .LinkCard-ecommerceLoadingCardNetwork{width:20px;height:20px;}.css-1g0fqss .LinkCard-ecommerceLoadingCardLoadingbar{height:60px;-webkit-flex:1;-ms-flex:1;flex:1;display:-webkit-box;display:-webkit-flex;display:-ms-flexbox;display:flex;-webkit-flex-direction:column;-ms-flex-direction:column;flex-direction:column;}.css-1g0fqss .LinkCard-ecommerceLoadingCardLoadingbar span{height:16px;display:inline-block;background:#EBEBEB;}.css-1g0fqss .LinkCard-ecommerceLoadingCardLoadingbar span:nth-of-type(1){width:60px;margin-bottom:4px;}.css-1g0fqss .LinkCard-ecommerceLoadingCardLoadingbar span:nth-of-type(2){width:127px;}.css-1g0fqss .LinkCard.new{position:relative;display:-webkit-box;display:-webkit-flex;display:-ms-flexbox;display:flex;box-sizing:border-box;-webkit-flex-direction:row;-ms-flex-direction:row;flex-direction:row;-webkit-align-items:center;-webkit-box-align:center;-ms-flex-align:center;align-items:center;width:390px;min-height:84px;border-radius:8px;max-width:100%;overflow:hidden;margin:16px auto;padding:12px 12px 9px 12px;background-color:#F6F6F6;}.css-1g0fqss .LinkCard.new,.css-1g0fqss .LinkCard.new:hover{-webkit-text-decoration:none;text-decoration:none;border:none !important;color:inherit !important;}.css-1g0fqss .LinkCard.new .LinkCard-contents{display:block;-webkit-flex:1 1 auto;-ms-flex:1 1 auto;flex:1 1 auto;position:relative;}.css-1g0fqss .LinkCard.new .LinkCard-contents .loading{height:14px;background:#EBEBEB;border-radius:7px;}.css-1g0fqss .LinkCard.new .LinkCard-contents.withTitle{margin-bottom:3px;}.css-1g0fqss .LinkCard.new .LinkCard-title{display:-webkit-box;font-size:15px;font-weight:500;line-height:1.4;margin-bottom:2px;color:#121212;text-overflow:ellipsis;overflow:hidden;-webkit-box-orient:vertical;-webkit-line-clamp:1;}.css-1g0fqss .LinkCard.new .LinkCard-title.two-line{line-height:20px;display:-webkit-box;text-overflow:ellipsis;overflow:hidden;-webkit-box-orient:vertical;-webkit-line-clamp:2;}.css-1g0fqss .LinkCard.new .LinkCard-title.loading{margin-bottom:8px;width:80%;}.css-1g0fqss .LinkCard.new .LinkCard-title.loading.withTitle{margin-bottom:6px;}.css-1g0fqss .LinkCard.new .LinkCard-title.loadingTitle{margin-bottom:5px;}.css-1g0fqss .LinkCard.new .LinkCard-excerpt{display:-webkit-box;text-overflow:ellipsis;font-size:13px;line-height:18px;color:#999999;margin-bottom:4px;overflow:hidden;-webkit-box-orient:vertical;-webkit-line-clamp:1;}.css-1g0fqss .LinkCard.new .LinkCard-excerpt .LinkCard-author{color:#444444;}.css-1g0fqss .LinkCard.new .LinkCard-desc{display:-webkit-box;font-size:13px;height:18px;line-height:18px;color:#999999;word-break:break-all;text-overflow:ellipsis;overflow:hidden;-webkit-box-orient:vertical;-webkit-line-clamp:1;}.css-1g0fqss .LinkCard.new .LinkCard-desc .LinkCard-tag,.css-1g0fqss .LinkCard.new .LinkCard-desc .tag{display:inline-block;font-size:11px;margin-left:8px;padding:0 4px;border-radius:3px;background:rgba(211,211,211,0.3);}.css-1g0fqss .LinkCard.new .LinkCard-desc.loading{width:40%;}.css-1g0fqss .LinkCard.new .LinkCard-desc svg{margin-right:2px;}.css-1g0fqss .LinkCard.new .LinkCard-image{-webkit-flex:0 0 auto;-ms-flex:0 0 auto;flex:0 0 auto;background-color:#EBEBEB;background-size:cover;background-position:center;position:relative;display:block;width:60px;height:60px;margin-left:20px;object-fit:cover;border-radius:inherit;overflow:hidden;}.css-1g0fqss .LinkCard.new .LinkCard-image.LinkCard-image--default{display:-webkit-box;display:-webkit-flex;display:-ms-flexbox;display:flex;-webkit-align-items:center;-webkit-box-align:center;-ms-flex-align:center;align-items:center;-webkit-box-pack:center;-webkit-justify-content:center;-ms-flex-pack:center;justify-content:center;background-color:#EBEBEB;color:#D3D3D3;}.css-1g0fqss .LinkCard.new .LinkCard-image.LinkCard-image--default svg{color:#999999;}.css-1g0fqss .LinkCard.new .LinkCard-image img{width:100%;height:100%;object-fit:cover;}.css-1g0fqss .LinkCard.new .LinkCard-image .LinkCard-image--video{display:-webkit-box;display:-webkit-flex;display:-ms-flexbox;display:flex;-webkit-align-items:center;-webkit-box-align:center;-ms-flex-align:center;align-items:center;-webkit-box-pack:center;-webkit-justify-content:center;-ms-flex-pack:center;justify-content:center;position:absolute;top:50%;left:50%;-webkit-transform:translateX(-50%) translateY(-50%);-ms-transform:translateX(-50%) translateY(-50%);transform:translateX(-50%) translateY(-50%);width:24px;height:24px;border-radius:12px;background:rgba(255,255,255,0.9);pointer-events:none;}.css-1g0fqss .LinkCard.new .LinkCard-image .LinkCard-image--video svg{color:#444444;}.css-1g0fqss .LinkCard.new .LinkCard-richText .text{color:#444444;}.css-1g0fqss .LinkCard.new .LinkCard-richText .bold{font-weight:600;}.css-1g0fqss .LinkCard.new .LinkCard-richText .tag{margin-left:4px;}.css-1g0fqss .FileLinkCard{-webkit-align-items:center;-webkit-box-align:center;-ms-flex-align:center;align-items:center;background-color:rgba(246,246,246,0.88);border-radius:12px;box-sizing:border-box;display:-webkit-box;display:-webkit-flex;display:-ms-flexbox;display:flex;margin:1em auto;max-width:100%;overflow:hidden;padding:12px;position:relative;width:390px;}.css-1g0fqss .FileLinkCard-icon{-webkit-flex-shrink:0;-ms-flex-negative:0;flex-shrink:0;height:30px;width:30px;}.css-1g0fqss .FileLinkCard-info{margin-left:12px;}.css-1g0fqss .FileLinkCard-name{color:#121212;font-size:15px;font-weight:500;line-height:21px;display:-webkit-box;text-overflow:ellipsis;overflow:hidden;-webkit-box-orient:vertical;-webkit-line-clamp:2;}.css-1g0fqss .FileLinkCard-meta{color:#999999;font-size:12px;display:-webkit-box;display:-webkit-flex;display:-ms-flexbox;display:flex;line-height:14px;margin-top:5px;}.css-1g0fqss .FileLinkCard-source{white-space:pre;}.css-1g0fqss img[data-uncomfortable]{content:url(data:image/svg+xml,%3Csvg%20xmlns%3D%22http%3A%2F%2Fwww.w3.org%2F2000%2Fsvg%22%20viewBox%3D%220%200%20344.88888888888886%20194%22%3E%3CforeignObject%20width%3D%22344.88888888888886%22%20height%3D%22194%22%3E%0A%20%20%20%20%20%20%3Cdiv%20xmlns%3D%22http%3A%2F%2Fwww.w3.org%2F1999%2Fxhtml%22%20style%3D%22font-size%3A%2013px%3B%20font-family%3A%20-apple-system%2C%20BlinkMacSystemFont%2C%20Microsoft%20YaHei%2C%20sans-serif%3B%20color%3A%20%23fff%3B%20width%3A100%25%3B%20height%3A194px%3B%22%3E%0A%20%20%20%20%20%20%20%20%3Cdiv%20style%3D%22display%3A%20flex%3B%20flex-direction%3A%20column%3B%20align-items%3A%20center%3B%20justify-content%3A%20center%3B%20height%3A%20100%25%3B%22%3E%0A%20%20%20%20%20%20%20%20%20%20%3Csvg%20xmlns%3D%22http%3A%2F%2Fwww.w3.org%2F2000%2Fsvg%22%20width%3D%2218%22%20height%3D%2218%22%20viewBox%3D%220%200%2016%2016%22%20fill%3D%22currentColor%22%3E%3Cpath%20d%3D%22M8%203.65a7%207%200%2000-1.353.128.65.65%200%2011-.25-1.275A8.3%208.3%200%20018%202.35c2.387%200%204.172.954%205.357%202.125C14.511%205.615%2015.15%207.022%2015.15%208c0%20.621-.257%201.391-.699%202.134a7.076%207.076%200%2001-1.403%201.68l.495.46a.65.65%200%2011-.886.951l-.998-.929a.645.645%200%2001-.104-.097L9.73%2010.501a.647.647%200%2001-.29.301%203.15%203.15%200%2001-4.313-4.094.647.647%200%2001.234-.275L3.908%205.08a5.774%205.774%200%2000-1.283%201.522C2.282%207.198%202.15%207.707%202.15%208c0%20.522.41%201.616%201.407%202.6.965.954%202.43%201.75%204.443%201.75.468%200%20.905-.043%201.311-.12a.65.65%200%2001.243%201.277A8.322%208.322%200%20018%2013.65c-2.387%200-4.172-.954-5.357-2.125C1.49%2010.385.85%208.978.85%208c0-.598.238-1.333.648-2.046A7.054%207.054%200%20012.95%204.188l-.547-.509a.65.65%200%2011.886-.951l8.8%208.194a5.793%205.793%200%20001.244-1.453c.372-.624.516-1.163.516-1.469%200-.522-.41-1.616-1.407-2.6-.965-.954-2.43-1.75-4.443-1.75zM6.29%207.296a1.85%201.85%200%20002.534%202.36l-2.535-2.36zM8%204.85a.65.65%200%20100%201.3%201.85%201.85%200%20011.843%201.694.65.65%200%20101.296-.11A3.15%203.15%200%20008%204.85z%22%20fill-rule%3D%22evenodd%22%20clip-rule%3D%22evenodd%22%3E%3C%2Fpath%3E%3C%2Fsvg%3E%0A%20%20%20%20%20%20%20%20%20%20%3Cdiv%20style%3D%22margin%3A%20.6em%200%201.2em%22%3E%E8%AF%A5%E5%9B%BE%E7%89%87%E6%9C%89%E5%8F%AF%E8%83%BD%E4%BC%9A%E5%BC%95%E8%B5%B7%E4%B8%8D%E9%80%82%3C%2Fdiv%3E%0A%20%20%20%20%20%20%20%20%20%20%3Cbutton%20style%3D%22padding%3A%204px%201em%3B%20font-size%3A%201.1em%3B%20color%3A%20inherit%3B%20background%3A%20none%3B%20border%3A%201px%20solid%20rgba%28255%2C255%2C255%2C.5%29%3B%20border-radius%3A%209999px%3B%22%3E%E7%BB%A7%E7%BB%AD%E6%9F%A5%E7%9C%8B%3C%2Fbutton%3E%0A%20%20%20%20%20%20%20%20%3C%2Fdiv%3E%0A%20%20%20%20%20%20%3C%2Fdiv%3E%0A%20%20%20%20%3C%2FforeignObject%3E%3C%2Fsvg%3E);width:100%;height:194px;background:url(https://pic1.zhimg.com/v2-cf70d0759d787c70091857151c1cad4a.jpeg) no-repeat rgba(191,191,191,0.7);background-size:cover;cursor:pointer!important;}@-webkit-keyframes animation-1yvu044{from{opacity:0;}to{opacity:1;}}@keyframes animation-1yvu044{from{opacity:0;}to{opacity:1;}}</style><style data-emotion-css="1s3a4zw">.css-1s3a4zw{position:relative;display:inline-block;height:30px;padding:0 12px;font-size:14px;line-height:30px;color:#056DE8;vertical-align:top;border-radius:100px;background:rgba(5,109,232,0.1);}.css-1s3a4zw:hover{background-color:rgba(5,109,232,0.15);}</style><style data-emotion-css="1xlfegr">.css-1xlfegr{background:transparent;box-shadow:none;}</style><style data-emotion-css="1gomreu">.css-1gomreu{position:relative;display:inline-block;}</style><style data-emotion-css="1any501">.css-1any501{box-sizing:border-box;margin:0;min-width:0;max-width:100%;height:auto;background-color:#FFFFFF;width:40px;height:40px;border-radius:50%;}</style><style data-emotion="css"></style><style>.MathJax_Preview ~ .math-holder {display: none}</style><style type="text/css">.MathJax_Hover_Frame {border-radius: .25em; -webkit-border-radius: .25em; -moz-border-radius: .25em; -khtml-border-radius: .25em; box-shadow: 0px 0px 15px #83A; -webkit-box-shadow: 0px 0px 15px #83A; -moz-box-shadow: 0px 0px 15px #83A; -khtml-box-shadow: 0px 0px 15px #83A; border: 1px solid #A6D ! important; display: inline-block; position: absolute}
.MathJax_Menu_Button .MathJax_Hover_Arrow {position: absolute; cursor: pointer; display: inline-block; border: 2px solid #AAA; border-radius: 4px; -webkit-border-radius: 4px; -moz-border-radius: 4px; -khtml-border-radius: 4px; font-family: 'Courier New',Courier; font-size: 9px; color: #F0F0F0}
.MathJax_Menu_Button .MathJax_Hover_Arrow span {display: block; background-color: #AAA; border: 1px solid; border-radius: 3px; line-height: 0; padding: 4px}
.MathJax_Hover_Arrow:hover {color: white!important; border: 2px solid #CCC!important}
.MathJax_Hover_Arrow:hover span {background-color: #CCC!important}
</style><style type="text/css">#MathJax_About {position: fixed; left: 50%; width: auto; text-align: center; border: 3px outset; padding: 1em 2em; background-color: #DDDDDD; color: black; cursor: default; font-family: message-box; font-size: 120%; font-style: normal; text-indent: 0; text-transform: none; line-height: normal; letter-spacing: normal; word-spacing: normal; word-wrap: normal; white-space: nowrap; float: none; z-index: 201; border-radius: 15px; -webkit-border-radius: 15px; -moz-border-radius: 15px; -khtml-border-radius: 15px; box-shadow: 0px 10px 20px #808080; -webkit-box-shadow: 0px 10px 20px #808080; -moz-box-shadow: 0px 10px 20px #808080; -khtml-box-shadow: 0px 10px 20px #808080; filter: progid:DXImageTransform.Microsoft.dropshadow(OffX=2, OffY=2, Color='gray', Positive='true')}
#MathJax_About.MathJax_MousePost {outline: none}
.MathJax_Menu {position: absolute; background-color: white; color: black; width: auto; padding: 2px; border: 1px solid #CCCCCC; margin: 0; cursor: default; font: menu; text-align: left; text-indent: 0; text-transform: none; line-height: normal; letter-spacing: normal; word-spacing: normal; word-wrap: normal; white-space: nowrap; float: none; z-index: 201; box-shadow: 0px 10px 20px #808080; -webkit-box-shadow: 0px 10px 20px #808080; -moz-box-shadow: 0px 10px 20px #808080; -khtml-box-shadow: 0px 10px 20px #808080; filter: progid:DXImageTransform.Microsoft.dropshadow(OffX=2, OffY=2, Color='gray', Positive='true')}
.MathJax_MenuItem {padding: 2px 2em; background: transparent}
.MathJax_MenuArrow {position: absolute; right: .5em; padding-top: .25em; color: #666666; font-size: .75em}
.MathJax_MenuActive .MathJax_MenuArrow {color: white}
.MathJax_MenuArrow.RTL {left: .5em; right: auto}
.MathJax_MenuCheck {position: absolute; left: .7em}
.MathJax_MenuCheck.RTL {right: .7em; left: auto}
.MathJax_MenuRadioCheck {position: absolute; left: 1em}
.MathJax_MenuRadioCheck.RTL {right: 1em; left: auto}
.MathJax_MenuLabel {padding: 2px 2em 4px 1.33em; font-style: italic}
.MathJax_MenuRule {border-top: 1px solid #CCCCCC; margin: 4px 1px 0px}
.MathJax_MenuDisabled {color: GrayText}
.MathJax_MenuActive {background-color: Highlight; color: HighlightText}
.MathJax_MenuDisabled:focus, .MathJax_MenuLabel:focus {background-color: #E8E8E8}
.MathJax_ContextMenu:focus {outline: none}
.MathJax_ContextMenu .MathJax_MenuItem:focus {outline: none}
#MathJax_AboutClose {top: .2em; right: .2em}
.MathJax_Menu .MathJax_MenuClose {top: -10px; left: -10px}
.MathJax_MenuClose {position: absolute; cursor: pointer; display: inline-block; border: 2px solid #AAA; border-radius: 18px; -webkit-border-radius: 18px; -moz-border-radius: 18px; -khtml-border-radius: 18px; font-family: 'Courier New',Courier; font-size: 24px; color: #F0F0F0}
.MathJax_MenuClose span {display: block; background-color: #AAA; border: 1.5px solid; border-radius: 18px; -webkit-border-radius: 18px; -moz-border-radius: 18px; -khtml-border-radius: 18px; line-height: 0; padding: 8px 0 6px}
.MathJax_MenuClose:hover {color: white!important; border: 2px solid #CCC!important}
.MathJax_MenuClose:hover span {background-color: #CCC!important}
.MathJax_MenuClose:hover:focus {outline: none}
</style><style type="text/css">.MathJax_Preview .MJXf-math {color: inherit!important}
</style><style type="text/css">.MJX_Assistive_MathML {position: absolute!important; top: 0; left: 0; clip: rect(1px, 1px, 1px, 1px); padding: 1px 0 0 0!important; border: 0!important; height: 1px!important; width: 1px!important; overflow: hidden!important; display: block!important; -webkit-touch-callout: none; -webkit-user-select: none; -khtml-user-select: none; -moz-user-select: none; -ms-user-select: none; user-select: none}
.MJX_Assistive_MathML.MJX_Assistive_MathML_Block {width: 100%!important}
</style><style type="text/css">#MathJax_Zoom {position: absolute; background-color: #F0F0F0; overflow: auto; display: block; z-index: 301; padding: .5em; border: 1px solid black; margin: 0; font-weight: normal; font-style: normal; text-align: left; text-indent: 0; text-transform: none; line-height: normal; letter-spacing: normal; word-spacing: normal; word-wrap: normal; white-space: nowrap; float: none; -webkit-box-sizing: content-box; -moz-box-sizing: content-box; box-sizing: content-box; box-shadow: 5px 5px 15px #AAAAAA; -webkit-box-shadow: 5px 5px 15px #AAAAAA; -moz-box-shadow: 5px 5px 15px #AAAAAA; -khtml-box-shadow: 5px 5px 15px #AAAAAA; filter: progid:DXImageTransform.Microsoft.dropshadow(OffX=2, OffY=2, Color='gray', Positive='true')}
#MathJax_ZoomOverlay {position: absolute; left: 0; top: 0; z-index: 300; display: inline-block; width: 100%; height: 100%; border: 0; padding: 0; margin: 0; background-color: white; opacity: 0; filter: alpha(opacity=0)}
#MathJax_ZoomFrame {position: relative; display: inline-block; height: 0; width: 0}
#MathJax_ZoomEventTrap {position: absolute; left: 0; top: 0; z-index: 302; display: inline-block; border: 0; padding: 0; margin: 0; background-color: white; opacity: 0; filter: alpha(opacity=0)}
</style><style type="text/css">.MathJax_Preview {color: #888; display: contents}
#MathJax_Message {position: fixed; left: 1em; bottom: 1.5em; background-color: #E6E6E6; border: 1px solid #959595; margin: 0px; padding: 2px 8px; z-index: 102; color: black; font-size: 80%; width: auto; white-space: nowrap}
#MathJax_MSIE_Frame {position: absolute; top: 0; left: 0; width: 0px; z-index: 101; border: 0px; margin: 0px; padding: 0px}
.MathJax_Error {color: #CC0000; font-style: italic}
.MathJax_SVG .noError {white-space: normal}
html[data-theme="dark"] .MathJax_SVG .noError {filter: invert(1)}
.ztext-math-scrollable .MathJax_SVG {max-width: 100%; overflow-x: auto; overflow-y: hidden}
.ztext-math-scrollable .MathJax_SVG svg {max-width: unset}
.MathJax_SVG svg {max-width: 100%}
div.MathJax_SVG_Display {display: none}
.MathJax_Preview ~ .math-holder {display: none}
</style><style type="text/css">.MJXp-script {font-size: .8em}
.MJXp-right {-webkit-transform-origin: right; -moz-transform-origin: right; -ms-transform-origin: right; -o-transform-origin: right; transform-origin: right}
.MJXp-bold {font-weight: bold}
.MJXp-italic {font-style: italic}
.MJXp-scr {font-family: MathJax_Script,'Times New Roman',Times,STIXGeneral,serif}
.MJXp-frak {font-family: MathJax_Fraktur,'Times New Roman',Times,STIXGeneral,serif}
.MJXp-sf {font-family: MathJax_SansSerif,'Times New Roman',Times,STIXGeneral,serif}
.MJXp-cal {font-family: MathJax_Caligraphic,'Times New Roman',Times,STIXGeneral,serif}
.MJXp-mono {font-family: MathJax_Typewriter,'Times New Roman',Times,STIXGeneral,serif}
.MJXp-largeop {font-size: 150%}
.MJXp-largeop.MJXp-int {vertical-align: -.2em}
.MJXp-math {display: inline-block; line-height: 1.2; text-indent: 0; font-family: 'Times New Roman',Times,STIXGeneral,serif; white-space: nowrap; border-collapse: collapse}
.MJXp-display {display: block; text-align: center; margin: 1em 0}
.MJXp-math span {display: inline-block}
.MJXp-box {display: block!important; text-align: center}
.MJXp-box:after {content: " "}
.MJXp-rule {display: block!important; margin-top: .1em}
.MJXp-char {display: block!important}
.MJXp-mo {margin: 0 .15em}
.MJXp-mfrac {margin: 0 .125em; vertical-align: .25em}
.MJXp-denom {display: inline-table!important; width: 100%}
.MJXp-denom > * {display: table-row!important}
.MJXp-surd {vertical-align: top}
.MJXp-surd > * {display: block!important}
.MJXp-script-box > *  {display: table!important; height: 50%}
.MJXp-script-box > * > * {display: table-cell!important; vertical-align: top}
.MJXp-script-box > *:last-child > * {vertical-align: bottom}
.MJXp-script-box > * > * > * {display: block!important}
.MJXp-mphantom {visibility: hidden}
.MJXp-munderover, .MJXp-munder {display: inline-table!important}
.MJXp-over {display: inline-block!important; text-align: center}
.MJXp-over > * {display: block!important}
.MJXp-munderover > *, .MJXp-munder > * {display: table-row!important}
.MJXp-mtable {vertical-align: .25em; margin: 0 .125em}
.MJXp-mtable > * {display: inline-table!important; vertical-align: middle}
.MJXp-mtr {display: table-row!important}
.MJXp-mtd {display: table-cell!important; text-align: center; padding: .5em 0 0 .5em}
.MJXp-mtr > .MJXp-mtd:first-child {padding-left: 0}
.MJXp-mtr:first-child > .MJXp-mtd {padding-top: 0}
.MJXp-mlabeledtr {display: table-row!important}
.MJXp-mlabeledtr > .MJXp-mtd:first-child {padding-left: 0}
.MJXp-mlabeledtr:first-child > .MJXp-mtd {padding-top: 0}
.MJXp-merror {background-color: #FFFF88; color: #CC0000; border: 1px solid #CC0000; padding: 1px 3px; font-style: normal; font-size: 90%}
.MJXp-scale0 {-webkit-transform: scaleX(.0); -moz-transform: scaleX(.0); -ms-transform: scaleX(.0); -o-transform: scaleX(.0); transform: scaleX(.0)}
.MJXp-scale1 {-webkit-transform: scaleX(.1); -moz-transform: scaleX(.1); -ms-transform: scaleX(.1); -o-transform: scaleX(.1); transform: scaleX(.1)}
.MJXp-scale2 {-webkit-transform: scaleX(.2); -moz-transform: scaleX(.2); -ms-transform: scaleX(.2); -o-transform: scaleX(.2); transform: scaleX(.2)}
.MJXp-scale3 {-webkit-transform: scaleX(.3); -moz-transform: scaleX(.3); -ms-transform: scaleX(.3); -o-transform: scaleX(.3); transform: scaleX(.3)}
.MJXp-scale4 {-webkit-transform: scaleX(.4); -moz-transform: scaleX(.4); -ms-transform: scaleX(.4); -o-transform: scaleX(.4); transform: scaleX(.4)}
.MJXp-scale5 {-webkit-transform: scaleX(.5); -moz-transform: scaleX(.5); -ms-transform: scaleX(.5); -o-transform: scaleX(.5); transform: scaleX(.5)}
.MJXp-scale6 {-webkit-transform: scaleX(.6); -moz-transform: scaleX(.6); -ms-transform: scaleX(.6); -o-transform: scaleX(.6); transform: scaleX(.6)}
.MJXp-scale7 {-webkit-transform: scaleX(.7); -moz-transform: scaleX(.7); -ms-transform: scaleX(.7); -o-transform: scaleX(.7); transform: scaleX(.7)}
.MJXp-scale8 {-webkit-transform: scaleX(.8); -moz-transform: scaleX(.8); -ms-transform: scaleX(.8); -o-transform: scaleX(.8); transform: scaleX(.8)}
.MJXp-scale9 {-webkit-transform: scaleX(.9); -moz-transform: scaleX(.9); -ms-transform: scaleX(.9); -o-transform: scaleX(.9); transform: scaleX(.9)}
.MathJax_PHTML .noError {vertical-align: ; font-size: 90%; text-align: left; color: black; padding: 1px 3px; border: 1px solid}
</style><style type="text/css">.MathJax_SVG_Display {text-align: center; margin: 1em 0em; position: relative; display: block!important; text-indent: 0; max-width: none; max-height: none; min-width: 0; min-height: 0; width: 100%}
.MathJax_SVG .MJX-monospace {font-family: monospace}
.MathJax_SVG .MJX-sans-serif {font-family: sans-serif}
#MathJax_SVG_Tooltip {background-color: InfoBackground; color: InfoText; border: 1px solid black; box-shadow: 2px 2px 5px #AAAAAA; -webkit-box-shadow: 2px 2px 5px #AAAAAA; -moz-box-shadow: 2px 2px 5px #AAAAAA; -khtml-box-shadow: 2px 2px 5px #AAAAAA; padding: 3px 4px; z-index: 401; position: absolute; left: 0; top: 0; width: auto; height: auto; display: none}
.MathJax_SVG {display: inline; font-style: normal; font-weight: normal; line-height: normal; font-size: 100%; font-size-adjust: none; text-indent: 0; text-align: left; text-transform: none; letter-spacing: normal; word-spacing: normal; word-wrap: normal; white-space: nowrap; float: none; direction: ltr; max-width: none; max-height: none; min-width: 0; min-height: 0; border: 0; padding: 0; margin: 0}
.MathJax_SVG * {transition: none; -webkit-transition: none; -moz-transition: none; -ms-transition: none; -o-transition: none}
.MathJax_SVG > div {display: inline-block}
.mjx-svg-href {fill: blue; stroke: blue}
.MathJax_SVG_Processing {visibility: hidden; position: absolute; top: 0; left: 0; width: 0; height: 0; overflow: hidden; display: block!important}
.MathJax_SVG_Processed {display: none!important}
.MathJax_SVG_test {font-style: normal; font-weight: normal; font-size: 100%; font-size-adjust: none; text-indent: 0; text-transform: none; letter-spacing: normal; word-spacing: normal; overflow: hidden; height: 1px}
.MathJax_SVG_test.mjx-test-display {display: table!important}
.MathJax_SVG_test.mjx-test-inline {display: inline!important; margin-right: -1px}
.MathJax_SVG_test.mjx-test-default {display: block!important; clear: both}
.MathJax_SVG_ex_box {display: inline-block!important; position: absolute; overflow: hidden; min-height: 0; max-height: none; padding: 0; border: 0; margin: 0; width: 1px; height: 60ex}
.mjx-test-inline .MathJax_SVG_left_box {display: inline-block; width: 0; float: left}
.mjx-test-inline .MathJax_SVG_right_box {display: inline-block; width: 0; float: right}
.mjx-test-display .MathJax_SVG_right_box {display: table-cell!important; width: 10000em!important; min-width: 0; max-width: none; padding: 0; border: 0; margin: 0}
.MathJax_SVG .noError {vertical-align: ; font-size: 90%; text-align: left; color: black; padding: 1px 3px; border: 1px solid}
</style></head><body class="WhiteBg-body PostIndex-body" data-rh="class"><div style="visibility: hidden; overflow: hidden; position: absolute; top: 0px; height: 1px; width: auto; padding: 0px; border: 0px; margin: 0px; text-align: left; text-indent: 0px; text-transform: none; line-height: normal; letter-spacing: normal; word-spacing: normal;"><div id="MathJax_SVG_Hidden"></div><svg><defs id="MathJax_SVG_glyphs"><path stroke-width="1" id="MJMATHI-63" d="M34 159Q34 268 120 355T306 442Q362 442 394 418T427 355Q427 326 408 306T360 285Q341 285 330 295T319 325T330 359T352 380T366 386H367Q367 388 361 392T340 400T306 404Q276 404 249 390Q228 381 206 359Q162 315 142 235T121 119Q121 73 147 50Q169 26 205 26H209Q321 26 394 111Q403 121 406 121Q410 121 419 112T429 98T420 83T391 55T346 25T282 0T202 -11Q127 -11 81 37T34 159Z"></path><path stroke-width="1" id="MJMATHI-74" d="M26 385Q19 392 19 395Q19 399 22 411T27 425Q29 430 36 430T87 431H140L159 511Q162 522 166 540T173 566T179 586T187 603T197 615T211 624T229 626Q247 625 254 615T261 596Q261 589 252 549T232 470L222 433Q222 431 272 431H323Q330 424 330 420Q330 398 317 385H210L174 240Q135 80 135 68Q135 26 162 26Q197 26 230 60T283 144Q285 150 288 151T303 153H307Q322 153 322 145Q322 142 319 133Q314 117 301 95T267 48T216 6T155 -11Q125 -11 98 4T59 56Q57 64 57 83V101L92 241Q127 382 128 383Q128 385 77 385H26Z"></path><path stroke-width="1" id="MJMATHI-78" d="M52 289Q59 331 106 386T222 442Q257 442 286 424T329 379Q371 442 430 442Q467 442 494 420T522 361Q522 332 508 314T481 292T458 288Q439 288 427 299T415 328Q415 374 465 391Q454 404 425 404Q412 404 406 402Q368 386 350 336Q290 115 290 78Q290 50 306 38T341 26Q378 26 414 59T463 140Q466 150 469 151T485 153H489Q504 153 504 145Q504 144 502 134Q486 77 440 33T333 -11Q263 -11 227 52Q186 -10 133 -10H127Q78 -10 57 16T35 71Q35 103 54 123T99 143Q142 143 142 101Q142 81 130 66T107 46T94 41L91 40Q91 39 97 36T113 29T132 26Q168 26 194 71Q203 87 217 139T245 247T261 313Q266 340 266 352Q266 380 251 392T217 404Q177 404 142 372T93 290Q91 281 88 280T72 278H58Q52 284 52 289Z"></path><path stroke-width="1" id="MJMAIN-2B" d="M56 237T56 250T70 270H369V420L370 570Q380 583 389 583Q402 583 409 568V270H707Q722 262 722 250T707 230H409V-68Q401 -82 391 -82H389H387Q375 -82 369 -68V230H70Q56 237 56 250Z"></path><path stroke-width="1" id="MJMATHI-6B" d="M121 647Q121 657 125 670T137 683Q138 683 209 688T282 694Q294 694 294 686Q294 679 244 477Q194 279 194 272Q213 282 223 291Q247 309 292 354T362 415Q402 442 438 442Q468 442 485 423T503 369Q503 344 496 327T477 302T456 291T438 288Q418 288 406 299T394 328Q394 353 410 369T442 390L458 393Q446 405 434 405H430Q398 402 367 380T294 316T228 255Q230 254 243 252T267 246T293 238T320 224T342 206T359 180T365 147Q365 130 360 106T354 66Q354 26 381 26Q429 26 459 145Q461 153 479 153H483Q499 153 499 144Q499 139 496 130Q455 -11 378 -11Q333 -11 305 15T277 90Q277 108 280 121T283 145Q283 167 269 183T234 206T200 217T182 220H180Q168 178 159 139T145 81T136 44T129 20T122 7T111 -2Q98 -11 83 -11Q66 -11 57 -1T48 16Q48 26 85 176T158 471L195 616Q196 629 188 632T149 637H144Q134 637 131 637T124 640T121 647Z"></path></defs></svg></div><div id="MathJax_Message" style="display: none;"></div><a id="ariaTipText" role="pagedescription" aria-label="欢迎进入 自监督学习: 人工智能的未来 - 知乎,盲人用户使用操作智能引导，请按快捷键Ctrl+Alt+R；阅读详细操作说明请按快捷键Ctrl+Alt+问号键。" aria-atomic="true" href="javascript:void(0)" class="skipAutoFix" style="width: 1px; height: 1px;"><img src="https://zhuanlan.zhihu.com/p/270547809" style="width:1px !important;height:1px !important;position:absolute;top:0;"></a><div id="root"><div class="App"><div class="LoadingBar  css-uzm3ri"></div><div><span style="position:absolute;top:-10000px;left:-10000px" role="log" aria-live="assertive"></span></div><main role="main" class="App-main"><div class="Post-content" data-zop-usertoken="{&quot;userToken&quot;:&quot;liu-zi-feng-da-da&quot;}" data-zop="{&quot;authorName&quot;:&quot;谷育龙Eric&quot;,&quot;itemId&quot;:270547809,&quot;title&quot;:&quot;自监督学习: 人工智能的未来&quot;,&quot;type&quot;:&quot;article&quot;}" data-za-detail-view-path-module="PostItem" data-za-extra-module="{&quot;card&quot;:{&quot;content&quot;:{&quot;type&quot;:&quot;Post&quot;,&quot;token&quot;:&quot;270547809&quot;}}}"><div class="ColumnPageHeader-Wrapper"><div><div class="Sticky ColumnPageHeader is-fixed css-1l12z7y" style="width: 1484.8px; top: 0px; left: 0px;"><div class="ColumnPageHeader-content"><a href="https://www.zhihu.com/" aria-label="知乎"><svg viewBox="0 0 64 30" fill="#056DE8" width="64" height="30" class="css-1hlrcxk"><path d="M29.05 4.582H16.733V25.94h3.018l.403 2.572 4.081-2.572h4.815V4.582zm-5.207 18.69l-2.396 1.509-.235-1.508h-1.724V7.233h6.78v16.04h-2.425zM14.46 14.191H9.982c0-.471.033-.954.039-1.458v-5.5h5.106V5.935a1.352 1.352 0 0 0-.404-.957 1.378 1.378 0 0 0-.968-.396H5.783c.028-.088.056-.177.084-.255.274-.82 1.153-3.326 1.153-3.326a4.262 4.262 0 0 0-2.413.698c-.57.4-.912.682-1.371 1.946-.532 1.453-.997 2.856-1.31 3.693C1.444 8.674.28 11.025.28 11.025a5.85 5.85 0 0 0 2.52-.61c1.119-.593 1.679-1.502 2.054-2.883l.09-.3h2.334v5.5c0 .5-.045.982-.073 1.46h-4.12c-.71 0-1.39.278-1.893.775a2.638 2.638 0 0 0-.783 1.874h6.527a17.717 17.717 0 0 1-.778 3.649 16.796 16.796 0 0 1-3.012 5.273A33.104 33.104 0 0 1 0 28.74s3.13 1.175 5.425-.954c1.388-1.292 2.631-3.814 3.23-5.727a28.09 28.09 0 0 0 1.12-5.229h5.967v-1.37a1.254 1.254 0 0 0-.373-.899 1.279 1.279 0 0 0-.909-.37z"></path><path d="M11.27 19.675l-2.312 1.491 5.038 7.458a6.905 6.905 0 0 0 .672-2.218 3.15 3.15 0 0 0-.28-2.168l-3.118-4.563zM51.449 15.195V5.842c4.181-.205 7.988-.405 9.438-.483l.851-.05c.387-.399.885-2.395.689-3.021-.073-.25-.213-.666-.638-.555a33.279 33.279 0 0 1-4.277.727c-2.766.321-3.97.404-7.804.682-6.718.487-12.709.72-12.709.72a2.518 2.518 0 0 0 .788 1.834 2.567 2.567 0 0 0 1.883.706c2.278-.095 5.598-.25 8.996-.41v9.203h-12.78c0 .703.281 1.377.783 1.874a2.69 2.69 0 0 0 1.892.777h10.105v7.075c0 .887-.464 1.192-1.231 1.214h-3.92a4.15 4.15 0 0 0 .837 1.544 4.2 4.2 0 0 0 1.403 1.067 6.215 6.215 0 0 0 2.71.277c1.36-.066 2.967-.826 2.967-3.57v-7.607h11.28c.342 0 .67-.135.91-.374.242-.239.378-.563.378-.902v-1.375H51.449z"></path><path d="M42.614 8.873a2.304 2.304 0 0 0-1.508-.926 2.334 2.334 0 0 0-1.727.405l-.376.272 4.255 5.85 2.24-1.62-2.884-3.98zM57.35 8.68l-3.125 4.097 2.24 1.663 4.517-5.927-.375-.277a2.32 2.32 0 0 0-1.722-.452 2.327 2.327 0 0 0-1.536.896z"></path></svg></a><i class="ColumnPageHeader-Line"></i><div class="ColumnPageHeader-Title"><div class="ColumnPageHeader-TitleName"><span class="ColumnPageHeader-TitleMeta">首发于</span><a class="ColumnLink ColumnPageHeader-TitleColumn" href="https://www.zhihu.com/column/c_1288235772122718208">互联网学社</a></div></div><div class="ColumnPageHeader-Button"><div class="Popover"><button title="更多" id="Popover1-toggle" aria-haspopup="true" aria-expanded="false" aria-owns="Popover1-content" type="button" class="Button ColumnPageHeader-MenuToggler Button--plain"><svg width="24" height="24" viewBox="0 0 24 24" class="Zi Zi--Dots" fill="currentColor"><path d="M5.34 12a1.665 1.665 0 1 1-3.33 0 1.665 1.665 0 0 1 3.33 0ZM13.665 12a1.665 1.665 0 1 1-3.33 0 1.665 1.665 0 0 1 3.33 0ZM21.99 12a1.665 1.665 0 1 1-3.33 0 1.665 1.665 0 0 1 3.33 0Z"></path></svg></button></div><button type="button" class="Button ColumnPageHeader-WriteButton Button--blue"><svg width="24" height="24" viewBox="0 0 24 24" class="Zi Zi--EditSurround" fill="currentColor"><path fill-rule="evenodd" d="M3.55 5.97a2.415 2.415 0 0 1 2.415-2.416h7.56a.75.75 0 0 1 0 1.5h-7.56a.915.915 0 0 0-.915.915v12.072c0 .505.41.915.915.915h12.074c.506 0 .915-.41.915-.915v-7.557a.75.75 0 0 1 1.5 0v7.557a2.415 2.415 0 0 1-2.415 2.415H5.965A2.415 2.415 0 0 1 3.55 18.04V5.969Z" clip-rule="evenodd"></path><path fill-rule="evenodd" d="M20.239 3.77a.75.75 0 0 1 0 1.06l-8.206 8.206a.75.75 0 0 1-1.06-1.06l8.205-8.206a.75.75 0 0 1 1.06 0Z" clip-rule="evenodd"></path></svg>写文章</button></div></div><div class="ColumnPageHeader-profile"><div class="Popover AppHeader-menu"><button id="Popover4-toggle" aria-haspopup="true" aria-expanded="false" aria-owns="Popover4-content" type="button" class="Button AppHeader-profileEntry FEfUrdfMIKpQDJDqkjte Button--plain fEPKGkUK5jyc4fUuT0QP"><img class="Avatar AppHeader-profileAvatar css-1tzrnvf" src="./自监督学习_ 人工智能的未来 - 知乎_files/v2-38755e385b05dcd213fa98d8bef04c46_l.jpg" srcset="https://picx.zhimg.com/v2-38755e385b05dcd213fa98d8bef04c46_l.jpg?source=32738c0c 2x" alt="点击打开刘梓枫大大的主页"></button></div><div class="Popover ddLajxN_Q0AuobBZjX9m AppHeaderProfileMenu-creatorHintPopover"><div class="AppHeaderProfileMenu-creatorHintToggler" id="Popover5-toggle" aria-haspopup="true" aria-expanded="false" aria-owns="Popover5-content"></div></div></div></div><div class="Sticky--holder" style="position: relative; inset: 0px; display: block; float: none; margin: 0px; height: 52px;"></div></div></div><div class="css-78p1r9"><div class="css-vn9cxj"><div class="css-1ld0bim"><img src="./自监督学习_ 人工智能的未来 - 知乎_files/v2-11303ae8c2a58596a23d75fe9cd6e1c4_720w.jpg" alt="自监督学习: 人工智能的未来" width="100%" height="100%" class="css-1phd9a0" srcset="https://picx.zhimg.com/v2-11303ae8c2a58596a23d75fe9cd6e1c4_200x0.jpg?source=172ae18b 200w,https://picx.zhimg.com/v2-11303ae8c2a58596a23d75fe9cd6e1c4_qhd.jpg?source=172ae18b 480w,https://picx.zhimg.com/v2-11303ae8c2a58596a23d75fe9cd6e1c4_720w.jpg?source=172ae18b 720w,https://picx.zhimg.com/v2-11303ae8c2a58596a23d75fe9cd6e1c4_1440w.jpg?source=172ae18b 1440w" loading="lazy"></div></div></div><article class="Post-Main Post-NormalMain" tabindex="-1"><header class="Post-Header"><h1 class="Post-Title">自监督学习: 人工智能的未来</h1><div class="Post-Author"><div class="AuthorInfo" itemprop="author" itemscope="" itemtype="http://schema.org/Person"><div class="AuthorInfo"><meta itemprop="name" content="谷育龙Eric"><meta itemprop="image" content="https://pica.zhimg.com/v2-5e488c2bdf6db9763d3defb3b20d6089_l.jpg?source=172ae18b"><meta itemprop="url" content="https://www.zhihu.com/people/gu-yu-long-eric"><meta itemprop="zhihu:followerCount"><span class="UserLink AuthorInfo-avatarWrapper"><div class="css-1gomreu"><a href="https://www.zhihu.com/people/gu-yu-long-eric" target="_blank" class="UserLink-link" data-za-detail-view-element_name="User"><img class="Avatar AuthorInfo-avatar css-1y9jkzv" src="./自监督学习_ 人工智能的未来 - 知乎_files/v2-5e488c2bdf6db9763d3defb3b20d6089_l.jpg" srcset="https://pica.zhimg.com/v2-5e488c2bdf6db9763d3defb3b20d6089_l.jpg?source=172ae18b 2x" alt="谷育龙Eric"></a></div></span><div class="AuthorInfo-content"><div class="AuthorInfo-head"><span class="UserLink AuthorInfo-name"><div class="css-1gomreu"><a href="https://www.zhihu.com/people/gu-yu-long-eric" target="_blank" class="UserLink-link" data-za-detail-view-element_name="User">谷育龙Eric</a></div><a href="https://www.zhihu.com/question/48510028" target="_blank" class="css-n99yhz" aria-label="清华大学 计算机系博士" data-tooltip="清华大学 计算机系博士"><span class="css-18biwo">​<svg viewBox="0 0 24 24" class="css-1ifz0go" width="18" height="18"><svg viewBox="0 0 24 24" x="-3" y="-3" fill="#FFFFFF" width="30" height="30"><path d="M3.56231227,13.8535307 C2.40051305,12.768677 2.41398885,11.0669203 3.59484487,9.99979213 L3.59222085,9.99654885 C4.26730143,9.45036719 4.79446755,8.21005186 4.7184197,7.34453784 L4.72305873,7.34412719 C4.66942824,5.75539997 5.8824188,4.56066914 7.47188965,4.64242381 L7.47229112,4.6386236 C8.33515314,4.72977993 9.58467253,4.22534048 10.1426329,3.55925173 L10.1462611,3.56228565 C11.2316055,2.40008701 12.9353108,2.41394456 14.0015072,3.59634088 L14.0047263,3.59374004 C14.5498229,4.26841874 15.7896857,4.79521622 16.6545744,4.71844347 L16.6549836,4.72304294 C18.245027,4.66894057 19.4396947,5.88213996 19.3575031,7.47241135 L19.3623099,7.47292747 C19.2704388,8.3358681 19.7742711,9.58421483 20.4407199,10.1424506 L20.437686,10.1460789 C21.5997217,11.2312209 21.5860695,12.9345218 20.4042441,14.0007396 L20.4072865,14.0045125 C19.7325967,14.5495925 19.2055209,15.7896954 19.2815865,16.6561959 L19.2770449,16.6565978 C19.3315454,18.2453037 18.1173775,19.4393568 16.5274188,19.3571512 L16.5269029,19.3619539 C15.6647098,19.270083 14.415408,19.7741709 13.8573671,20.4403558 L13.8537409,20.4373235 C12.76842,21.5995708 11.0650432,21.5864553 9.99899434,20.4039226 L9.99527367,20.406923 C9.45025436,19.7323399 8.21017638,19.2051872 7.34461983,19.2812352 L7.344304,19.2776405 C5.75448683,19.3312904 4.55977145,18.1170085 4.64254978,16.527117 L4.63769921,16.5265942 C4.72957031,15.6644394 4.22547659,14.4151814 3.55928015,13.8571569 L3.56231227,13.8535307 Z"></path></svg><path d="M2.63951518,13.3895441 C3.70763333,14.2842292 4.44777637,16.1226061 4.30075305,17.5023312 L4.32211542,17.3063047 C4.17509209,18.6910561 5.17786655,19.7063729 6.5613937,19.5844846 L6.364106,19.6008202 C7.75140298,19.4789319 9.57474349,20.2554985 10.4468305,21.3349009 L10.3224262,21.1803415 C11.1982831,22.2647703 12.6257916,22.2723098 13.5167278,21.2079863 L13.3898102,21.3600325 C14.2845162,20.2919393 16.1229361,19.5518136 17.5026934,19.6988334 L17.3054057,19.6774716 C18.6914461,19.8244915 19.7067866,18.8217404 19.5836389,17.4395022 L19.6012314,17.6367853 C19.4793403,16.2482641 20.255925,14.4249662 21.3353526,13.5528995 L21.1807897,13.677301 C22.2639871,12.8014646 22.2727834,11.3739894 21.2084351,10.483074 L21.3604848,10.6099886 C20.2923667,9.71530351 19.5522236,7.87818322 19.6992469,6.49720154 L19.6778846,6.69448464 C19.8249079,5.30847665 18.8221335,4.2944164 17.4386063,4.41630468 L17.635894,4.39871256 C16.248597,4.52185742 14.4252565,3.74529084 13.5531695,2.66588842 L13.6775738,2.81919121 C12.8017169,1.73601905 11.3742084,1.72722299 10.4832722,2.79154644 L10.6101898,2.63950024 C9.71548377,3.70759343 7.87706394,4.44771919 6.49730661,4.30195588 L6.69459432,4.32206116 C5.30855394,4.17504128 4.29447,5.17904888 4.41636114,6.56128713 L4.3987686,6.36400404 C4.52065973,7.75126861 3.74407501,9.57456653 2.66464737,10.4478898 L2.81921035,10.3222318 C1.73601288,11.1993248 1.72721662,12.6255433 2.79156494,13.5164587 L2.63951518,13.3895441 Z" fill="#0066FF"></path><svg class="Zi Zi--Check" fill="#fff" x="6" y="6" viewBox="0 0 24 24" width="12" height="12"><path d="M10.229 17.516c-.318.327-.75.484-1.199.484-.453 0-.884-.16-1.202-.488l-4.335-4.47a1.77 1.77 0 0 1 .007-2.459 1.663 1.663 0 0 1 2.397.01l3.137 3.246 9.072-9.329a1.662 1.662 0 0 1 2.397 0c.663.681.663 1.786 0 2.466L10.23 17.516z" fill-rule="evenodd"></path></svg></svg></span></a><span rel="noopener noreferrer" class="css-2dtzk2">​<img src="./自监督学习_ 人工智能的未来 - 知乎_files/v2-4812630bc27d642f7cafcd6cdeca3d7a.jpg" alt="" class="css-1m3x3v9"></span></span></div><div class="AuthorInfo-detail"><div class="AuthorInfo-badge"><div class="ztext AuthorInfo-badgeText css-0">清华大学 计算机系博士</div></div></div></div></div></div><button type="button" class="Button FollowButton Button--primary Button--blue"><span style="display:inline-flex;align-items:center">​<svg width="1.2em" height="1.2em" viewBox="0 0 24 24" class="Zi Zi--Plus FollowButton-icon" fill="currentColor"><path fill-rule="evenodd" d="M13.25 3.25a1.25 1.25 0 1 0-2.5 0v7.5h-7.5a1.25 1.25 0 1 0 0 2.5h7.5v7.5a1.25 1.25 0 1 0 2.5 0v-7.5h7.5a1.25 1.25 0 0 0 0-2.5h-7.5v-7.5Z" clip-rule="evenodd"></path></svg></span>关注他</button></div><div class="LabelContainer-wrapper"></div><div role="button" tabindex="0"><span class="Voters"><button type="button" class="Button Button--plain">58 人<!-- -->赞同了该文章</button></span></div></header><div class="Post-RichTextContainer"><div class="css-1yuhvjn"><div class="css-376mun"><div class="RichText ztext Post-RichText css-1g0fqss" options="[object Object]"><p data-first-child="" data-pid="No2mR7k8">原创：知乎专栏<a href="https://www.zhihu.com/column/c_1288235772122718208" class="internal">搜索推荐广告排序艺术</a>，转载请联系作者。</p><p data-pid="nMdro9nh">导读：什么是自监督学习？为什么自监督学习是AI的未来？自监督学习如何实现？本文将回顾下自监督学习的前世今生，介绍它在CV、NLP、Graph、RecSys、RL等领域已经取得的令人惊叹的效果！</p><hr><h2>1. 什么是自监督学习？</h2><p data-pid="IteaOGre">实际的场景里通常有海量的无监督数据，而有监督数据却很少。那么能否利用这些海量的无监督数据，用来改进监督学习任务的效果呢？</p><p data-pid="A-lpXem_">▲自监督学习（Self-supervised Learning）作为Unsupervised Learning的一个重要分支，给出了很好地解决方案。它的目标是更好地利用无监督数据，提升后续监督学习任务的效果。</p><p data-pid="sgZz8pm9">其基本思想是：Predicting everything from everything else。</p><p data-pid="xxZnPx4X">具体方法是首先定义一个Pretext task (辅助任务)，即从无监督的数据中，通过巧妙地设计自动构造出有监督（伪标签）数据，学习一个预训练模型。构造有监督（伪标签）数据的方法可以是：假装输入中的一部分不存在，然后基于其余的部分用模型预测缺失的这部分。如果学习的预训练模型能准确预测缺失部分的数据，说明它的表示学习能力很强，能够学习到输入中的高级语义信息、泛化能力比较强。而深度学习的精髓正在于强大的表示学习能力。</p><p data-pid="wnsp_epY">然后可以将预训练模型，通过简单的Finetune，应用到下游的多个应用场景，能比只使用监督数据训练的模型有更好的效果。</p><p data-pid="8Q3OpKz6">▲通常来说有标签数据越少的场景，自监督学习能带来的提升越大。事实上，在一些论文的实验结果里，在大量无标签数据上自监督学习的模型，不需要finetune，能取得比使用标签数据学得的监督模型更好的效果……</p><p data-pid="ZoLIHRyU">对于有大量标签数据的场景，自监督学习也能进一步提升模型的泛化能力和效果。</p><p data-pid="Ak7WNd7q">▲下图展示了在CV领域自监督学习的标准流程：</p><p data-pid="ESdnMRDV">在自监督学习中，最重要的问题是：如何定义Pretext任务、如何从Pretext任务学习预训练模型。</p><figure data-size="normal"><noscript><img src="https://pic4.zhimg.com/v2-40069111d6becaf29815576a6cb607bf_b.jpg" data-size="normal" data-rawwidth="986" data-rawheight="1502" class="origin_image zh-lightbox-thumb" width="986" data-original="https://pic4.zhimg.com/v2-40069111d6becaf29815576a6cb607bf_r.jpg"/></noscript><div><img src="data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;986&#39; height=&#39;1502&#39;&gt;&lt;/svg&gt;" data-size="normal" data-rawwidth="986" data-rawheight="1502" class="origin_image zh-lightbox-thumb lazy" width="986" data-original="https://pic4.zhimg.com/v2-40069111d6becaf29815576a6cb607bf_r.jpg" data-actualsrc="https://pic4.zhimg.com/v2-40069111d6becaf29815576a6cb607bf_b.jpg"></div><figcaption>Self-supervised Visual Feature Learning with Deep Neural Networks: A Survey</figcaption></figure><hr><h2>2. 为什么自监督学习是AI的未来？</h2><p data-pid="cLcOyN35">Yann Lecun在AAAI 2020的演讲中，指出目前深度学习遇到的挑战：</p><p data-pid="1E7qQDPy">▲监督学习：深度模型有海量参数，需要大量的label数据，标注成本高、扩展性差，难以应用到无标记或标记数据少的场景。</p><p data-pid="0YTAdJFC">▲强化学习：agent需要和环境大量的交互尝试，很多实际场景（例如互联网搜索推荐、无人驾驶）中交互成本大、代价高，很难应用。</p><p data-pid="TofRecPs">而人类和动物学习快速的原因：最重要的是观察世界，而不是靠大量的监督、强化学习。</p><figure data-size="normal"><noscript><img src="https://pic3.zhimg.com/v2-b357fb6b1189f5bcd0ebc9f7b3b0adda_b.jpg" data-size="normal" data-rawwidth="2034" data-rawheight="1144" class="origin_image zh-lightbox-thumb" width="2034" data-original="https://pic3.zhimg.com/v2-b357fb6b1189f5bcd0ebc9f7b3b0adda_r.jpg"/></noscript><div><img src="data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;2034&#39; height=&#39;1144&#39;&gt;&lt;/svg&gt;" data-size="normal" data-rawwidth="2034" data-rawheight="1144" class="origin_image zh-lightbox-thumb lazy" width="2034" data-original="https://pic3.zhimg.com/v2-b357fb6b1189f5bcd0ebc9f7b3b0adda_r.jpg" data-actualsrc="https://pic3.zhimg.com/v2-b357fb6b1189f5bcd0ebc9f7b3b0adda_b.jpg"></div><figcaption>Yann LeCun</figcaption></figure><p data-pid="Ru4_LURK">智能的精髓在于预测：我们通过观察世界、理解世界、尝试预测未来，并根据实际结果的反馈信息，来不断调整自己的世界模型，变得越来越有智能。简单来说，无论人还是机器，预测的准确度越高，说明智能越强。</p><p data-pid="xxPME0zL">自监督学习的思想就是通过构造任务来提升预训练模型预测能力，即Predicting everything from everything else。具体方法是假装输入中的一部分不存在，然后基于其余的部分用模型预测这个部分，从而学习得到一个能很好地建模输入语义信息的表示学习模型。</p><figure data-size="normal"><noscript><img src="https://pic1.zhimg.com/v2-a09225e56ba51f8a5c1bd012f7013be8_b.jpg" data-size="normal" data-rawwidth="1440" data-rawheight="822" class="origin_image zh-lightbox-thumb" width="1440" data-original="https://pic1.zhimg.com/v2-a09225e56ba51f8a5c1bd012f7013be8_r.jpg"/></noscript><div><img src="data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1440&#39; height=&#39;822&#39;&gt;&lt;/svg&gt;" data-size="normal" data-rawwidth="1440" data-rawheight="822" class="origin_image zh-lightbox-thumb lazy" width="1440" data-original="https://pic1.zhimg.com/v2-a09225e56ba51f8a5c1bd012f7013be8_r.jpg" data-actualsrc="https://pic1.zhimg.com/v2-a09225e56ba51f8a5c1bd012f7013be8_b.jpg"></div><figcaption>Yann LeCun</figcaption></figure><p data-pid="NXv-EFre">例如，在NLP中，自监督学习Word2Vec、BERT、GPT、GPT2、GPT3等模型，可以很好地应用到语言模型、机器翻译、对话系统等多个任务中。</p><p data-pid="w-9yiZBr">在CV中，自监督学习SimCLR等模型，可以应用到图像分类、补全等任务。</p><figure data-size="normal"><noscript><img src="https://pic4.zhimg.com/v2-56599f893490139a121a3aeada412877_b.jpg" data-size="normal" data-rawwidth="2006" data-rawheight="1146" class="origin_image zh-lightbox-thumb" width="2006" data-original="https://pic4.zhimg.com/v2-56599f893490139a121a3aeada412877_r.jpg"/></noscript><div><img src="data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;2006&#39; height=&#39;1146&#39;&gt;&lt;/svg&gt;" data-size="normal" data-rawwidth="2006" data-rawheight="1146" class="origin_image zh-lightbox-thumb lazy" width="2006" data-original="https://pic4.zhimg.com/v2-56599f893490139a121a3aeada412877_r.jpg" data-actualsrc="https://pic4.zhimg.com/v2-56599f893490139a121a3aeada412877_b.jpg"></div><figcaption>Yann LeCun</figcaption></figure><p data-pid="0XiY1DT_">关于机器学习的作用，LeCun做了一个形象的比喻，如下图所示：强化学习像蛋糕上的樱桃，监督学习像蛋糕上的糖霜，而自监督学习是蛋糕本身。</p><figure data-size="normal"><noscript><img src="https://pic4.zhimg.com/v2-23b7d4df2c260ff77d79ec03833b85ab_b.jpg" data-size="normal" data-rawwidth="720" data-rawheight="414" class="origin_image zh-lightbox-thumb" width="720" data-original="https://pic4.zhimg.com/v2-23b7d4df2c260ff77d79ec03833b85ab_r.jpg"/></noscript><div><img src="data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;720&#39; height=&#39;414&#39;&gt;&lt;/svg&gt;" data-size="normal" data-rawwidth="720" data-rawheight="414" class="origin_image zh-lightbox-thumb lazy" width="720" data-original="https://pic4.zhimg.com/v2-23b7d4df2c260ff77d79ec03833b85ab_r.jpg" data-actualsrc="https://pic4.zhimg.com/v2-23b7d4df2c260ff77d79ec03833b85ab_b.jpg"></div><figcaption>Yann LeCun</figcaption></figure><p data-pid="MDNPbaHH">通过这个比喻，可以很好的理解自监督学习在人工智能中的重要基石作用。</p><p data-pid="Z_-X0Vjp">数据时代，很有前景的人工智能的实现方式是：在底层，首先基于海量的无标签数据，利用自监督学习，学习得到一个强大的通用表示模型。在上层，基于监督学习实现具体任务的目标、基于强化学习实现智能控制。自监督学习的作用在于，可以增强监督学习和强化学习模型的性能、泛化能力、鲁棒性等。</p><p data-pid="oYvpxr5p">事实上，在CV、NLP等人工智能领域，自监督学习已经开始发挥着至关重要的作用。</p><hr><h2>3. 自监督学习如何实现？</h2><p data-pid="1sQLH_c8">自监督学习的关键在于如何设计Pretext任务、损失函数，来优化预训练模型参数。</p><p data-pid="eruolaIY">下面我们通过介绍自监督学习在CV、NLP、Graph、Recsys等领域的经典工作，了解自监督学习的具体实现方式。</p><h2>3.1 Computer Vision:</h2><p data-pid="KZcoFBPl">CV领域中，自监督学习的Pretext任务可以是预测图片相对位置信息、旋转角度、视频中帧的顺序等。</p><p data-pid="5raaaPii"><b>[1] 2015 (ICCV) Unsupervised Learning of Visual Representations Using Videos</b></p><p data-pid="H8B-xzsv">ICCV 2015这篇论文的思想是：视频场景，对物体跟踪，对于一个包含物体o的帧X，包含物体o的另外一个帧 <span class="ztext-math" data-eeimg="1" data-tex="X_i^+"><span></span><span><script type="math/tex;mode=inline">X_i^+</script><span class="tex2jax_ignore math-holder">X_i^+</span></span></span> 应该比一个随机的帧 <span class="ztext-math" data-eeimg="1" data-tex="X_i^-"><span></span><span><script type="math/tex;mode=inline">X_i^-</script><span class="tex2jax_ignore math-holder">X_i^-</span></span></span> 和X的相似度更高。</p><figure data-size="normal"><noscript><img src="https://pic2.zhimg.com/v2-64c10a3743a84f95d97e56c80c048825_b.jpg" data-caption="" data-size="normal" data-rawwidth="1300" data-rawheight="1154" class="origin_image zh-lightbox-thumb" width="1300" data-original="https://pic2.zhimg.com/v2-64c10a3743a84f95d97e56c80c048825_r.jpg"/></noscript><div><img src="data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1300&#39; height=&#39;1154&#39;&gt;&lt;/svg&gt;" data-caption="" data-size="normal" data-rawwidth="1300" data-rawheight="1154" class="origin_image zh-lightbox-thumb lazy" width="1300" data-original="https://pic2.zhimg.com/v2-64c10a3743a84f95d97e56c80c048825_r.jpg" data-actualsrc="https://pic2.zhimg.com/v2-64c10a3743a84f95d97e56c80c048825_b.jpg"></div></figure><p data-pid="1eQMvn7W">这篇论文里使用CNN作为Encoder, 对于图片X，f代表CNN，f(X)为X新的表示向量，定义两个图片的距离为如下：</p><figure data-size="normal"><noscript><img src="https://pic2.zhimg.com/v2-4c4bdbab0ac9baf233fc6f6c868b3d9d_b.jpg" data-caption="" data-size="normal" data-rawwidth="602" data-rawheight="134" class="origin_image zh-lightbox-thumb" width="602" data-original="https://pic2.zhimg.com/v2-4c4bdbab0ac9baf233fc6f6c868b3d9d_r.jpg"/></noscript><div><img src="data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;602&#39; height=&#39;134&#39;&gt;&lt;/svg&gt;" data-caption="" data-size="normal" data-rawwidth="602" data-rawheight="134" class="origin_image zh-lightbox-thumb lazy" width="602" data-original="https://pic2.zhimg.com/v2-4c4bdbab0ac9baf233fc6f6c868b3d9d_r.jpg" data-actualsrc="https://pic2.zhimg.com/v2-4c4bdbab0ac9baf233fc6f6c868b3d9d_b.jpg"></div></figure><p data-pid="FSJAuJqx">然后就可以基于hinge loss来学习CNN的参数了：</p><figure data-size="normal"><noscript><img src="https://pic4.zhimg.com/v2-663548a78f4651ffef5ef8d2c581c11b_b.jpg" data-caption="" data-size="normal" data-rawwidth="958" data-rawheight="132" class="origin_image zh-lightbox-thumb" width="958" data-original="https://pic4.zhimg.com/v2-663548a78f4651ffef5ef8d2c581c11b_r.jpg"/></noscript><div><img src="data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;958&#39; height=&#39;132&#39;&gt;&lt;/svg&gt;" data-caption="" data-size="normal" data-rawwidth="958" data-rawheight="132" class="origin_image zh-lightbox-thumb lazy" width="958" data-original="https://pic4.zhimg.com/v2-663548a78f4651ffef5ef8d2c581c11b_r.jpg" data-actualsrc="https://pic4.zhimg.com/v2-663548a78f4651ffef5ef8d2c581c11b_b.jpg"></div></figure><figure data-size="normal"><noscript><img src="https://pic4.zhimg.com/v2-aad32041da3bc3bf37909da832a77f7f_b.jpg" data-caption="" data-size="normal" data-rawwidth="1092" data-rawheight="748" class="origin_image zh-lightbox-thumb" width="1092" data-original="https://pic4.zhimg.com/v2-aad32041da3bc3bf37909da832a77f7f_r.jpg"/></noscript><div><img src="data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1092&#39; height=&#39;748&#39;&gt;&lt;/svg&gt;" data-caption="" data-size="normal" data-rawwidth="1092" data-rawheight="748" class="origin_image zh-lightbox-thumb lazy" width="1092" data-original="https://pic4.zhimg.com/v2-aad32041da3bc3bf37909da832a77f7f_r.jpg" data-actualsrc="https://pic4.zhimg.com/v2-aad32041da3bc3bf37909da832a77f7f_b.jpg"></div></figure><p data-pid="oIvp1bJc">学习完CNN参数，就可以将它作为表示学习器（特征抽取器），应用到后续的其他CV任务了。这篇论文里，作者对比了基于100k无标签的视频无监督预训练CNN，和基于ImageNet千万级的监督数据预训练CNN，在后续的CV任务中取得了很接近的效果。</p><p data-pid="TkFqar7T"><b>[2] 2015 (ICCV) Unsupervised Visual Representation Learning by Context Prediction  </b></p><p data-pid="YFmTnrCx">ICCV 2015这篇论文里，构造训练数据的方法是：随机从图片采样一个patch，然后从它的邻居里随机采样一个patch，监督标签对应是邻居的位置信息。作者认为，准确地预测两个patch的位置关系，需要模型学会物体整体和部分的关系。</p><figure data-size="normal"><noscript><img src="https://pic3.zhimg.com/v2-c7c466638b869d8dc0e6cb71b646fc7e_b.jpg" data-caption="" data-size="normal" data-rawwidth="834" data-rawheight="688" class="origin_image zh-lightbox-thumb" width="834" data-original="https://pic3.zhimg.com/v2-c7c466638b869d8dc0e6cb71b646fc7e_r.jpg"/></noscript><div><img src="data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;834&#39; height=&#39;688&#39;&gt;&lt;/svg&gt;" data-caption="" data-size="normal" data-rawwidth="834" data-rawheight="688" class="origin_image zh-lightbox-thumb lazy" width="834" data-original="https://pic3.zhimg.com/v2-c7c466638b869d8dc0e6cb71b646fc7e_r.jpg" data-actualsrc="https://pic3.zhimg.com/v2-c7c466638b869d8dc0e6cb71b646fc7e_b.jpg"></div></figure><p data-pid="77mSgjRs"><b>[3] 2016 (ECCV) Unsupervised learning of visual representations by solving jigsaw puzzles</b></p><p data-pid="W5h4PEiB">EECV 2016这篇论文里，自监督的方法是：学习解决Jigsaw Puzzles（拼图）问题。随机打乱图片位置，学习恢复拼图，即生成原有图片的顺序。</p><figure data-size="normal"><noscript><img src="https://pic3.zhimg.com/v2-abf7f22c1cc68fdb02c5759d851b08ea_b.jpg" data-caption="" data-size="normal" data-rawwidth="2106" data-rawheight="742" class="origin_image zh-lightbox-thumb" width="2106" data-original="https://pic3.zhimg.com/v2-abf7f22c1cc68fdb02c5759d851b08ea_r.jpg"/></noscript><div><img src="data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;2106&#39; height=&#39;742&#39;&gt;&lt;/svg&gt;" data-caption="" data-size="normal" data-rawwidth="2106" data-rawheight="742" class="origin_image zh-lightbox-thumb lazy" width="2106" data-original="https://pic3.zhimg.com/v2-abf7f22c1cc68fdb02c5759d851b08ea_r.jpg" data-actualsrc="https://pic3.zhimg.com/v2-abf7f22c1cc68fdb02c5759d851b08ea_b.jpg"></div></figure><p class="ztext-empty-paragraph"><br></p><p data-pid="NS_fYMZt"><b>[4] 2016 (CVPR) Deepak Pathak et al. Context Encoders: Feature Learning by Inpainting</b></p><p data-pid="-TvKhRWi">CVPR 2016这篇论文里，自监督的方法是：学习恢复图片缺失部分。</p><figure data-size="normal"><noscript><img src="https://pic1.zhimg.com/v2-ddf4e4c71bb113c22ce9ff2e35b9b9b4_b.jpg" data-caption="" data-size="normal" data-rawwidth="950" data-rawheight="1068" class="origin_image zh-lightbox-thumb" width="950" data-original="https://pic1.zhimg.com/v2-ddf4e4c71bb113c22ce9ff2e35b9b9b4_r.jpg"/></noscript><div><img src="data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;950&#39; height=&#39;1068&#39;&gt;&lt;/svg&gt;" data-caption="" data-size="normal" data-rawwidth="950" data-rawheight="1068" class="origin_image zh-lightbox-thumb lazy" width="950" data-original="https://pic1.zhimg.com/v2-ddf4e4c71bb113c22ce9ff2e35b9b9b4_r.jpg" data-actualsrc="https://pic1.zhimg.com/v2-ddf4e4c71bb113c22ce9ff2e35b9b9b4_b.jpg"></div></figure><p data-pid="pZiUC5RH"><b>[5] 2016 (ECCV) A Colorful image colorization</b></p><p data-pid="MY9MPLkd">ECCV 2016这篇论文里，自监督的方法是：由黑白图片生成彩色图片。</p><figure data-size="normal"><noscript><img src="https://pic3.zhimg.com/v2-2b09014fac74adb83b5f433dc091e0e2_b.jpg" data-caption="" data-size="normal" data-rawwidth="2136" data-rawheight="922" class="origin_image zh-lightbox-thumb" width="2136" data-original="https://pic3.zhimg.com/v2-2b09014fac74adb83b5f433dc091e0e2_r.jpg"/></noscript><div><img src="data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;2136&#39; height=&#39;922&#39;&gt;&lt;/svg&gt;" data-caption="" data-size="normal" data-rawwidth="2136" data-rawheight="922" class="origin_image zh-lightbox-thumb lazy" width="2136" data-original="https://pic3.zhimg.com/v2-2b09014fac74adb83b5f433dc091e0e2_r.jpg" data-actualsrc="https://pic3.zhimg.com/v2-2b09014fac74adb83b5f433dc091e0e2_b.jpg"></div></figure><p data-pid="WdLNNc8H"><b>[6] 2017 (CVPR) Split-Brain Autoencoders: Unsupervised Learning by Cross-Channel Prediction</b></p><p data-pid="24Bq98H5">CVPR 2017这篇论文里，学习使用输入的一个channel取预测另一个channel。</p><figure data-size="normal"><noscript><img src="https://pic3.zhimg.com/v2-abff632b6a22e977050018ed611b1b12_b.jpg" data-caption="" data-size="normal" data-rawwidth="722" data-rawheight="516" class="origin_image zh-lightbox-thumb" width="722" data-original="https://pic3.zhimg.com/v2-abff632b6a22e977050018ed611b1b12_r.jpg"/></noscript><div><img src="data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;722&#39; height=&#39;516&#39;&gt;&lt;/svg&gt;" data-caption="" data-size="normal" data-rawwidth="722" data-rawheight="516" class="origin_image zh-lightbox-thumb lazy" width="722" data-original="https://pic3.zhimg.com/v2-abff632b6a22e977050018ed611b1b12_r.jpg" data-actualsrc="https://pic3.zhimg.com/v2-abff632b6a22e977050018ed611b1b12_b.jpg"></div></figure><figure data-size="normal"><noscript><img src="https://pic2.zhimg.com/v2-b5bab5b6cc331e6470f9b2b78a12f365_b.jpg" data-caption="" data-size="normal" data-rawwidth="1904" data-rawheight="570" class="origin_image zh-lightbox-thumb" width="1904" data-original="https://pic2.zhimg.com/v2-b5bab5b6cc331e6470f9b2b78a12f365_r.jpg"/></noscript><div><img src="data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1904&#39; height=&#39;570&#39;&gt;&lt;/svg&gt;" data-caption="" data-size="normal" data-rawwidth="1904" data-rawheight="570" class="origin_image zh-lightbox-thumb lazy" width="1904" data-original="https://pic2.zhimg.com/v2-b5bab5b6cc331e6470f9b2b78a12f365_r.jpg" data-actualsrc="https://pic2.zhimg.com/v2-b5bab5b6cc331e6470f9b2b78a12f365_b.jpg"></div></figure><p data-pid="wM61Z5PR"><b>[7] 2018 (ICLR) Unsupervised Representation Learning by Predicting Image Rotations</b></p><p data-pid="SWCUd5DX">2018 ICLR这篇论文里，采取的自监督的方法是：预测图片的旋转角度。</p><figure data-size="normal"><noscript><img src="https://pic4.zhimg.com/v2-2dc667812617528ce1950f8b9f38bb3b_b.jpg" data-caption="" data-size="normal" data-rawwidth="1434" data-rawheight="914" class="origin_image zh-lightbox-thumb" width="1434" data-original="https://pic4.zhimg.com/v2-2dc667812617528ce1950f8b9f38bb3b_r.jpg"/></noscript><div><img src="data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1434&#39; height=&#39;914&#39;&gt;&lt;/svg&gt;" data-caption="" data-size="normal" data-rawwidth="1434" data-rawheight="914" class="origin_image zh-lightbox-thumb lazy" width="1434" data-original="https://pic4.zhimg.com/v2-2dc667812617528ce1950f8b9f38bb3b_r.jpg" data-actualsrc="https://pic4.zhimg.com/v2-2dc667812617528ce1950f8b9f38bb3b_b.jpg"></div></figure><p data-pid="ZmGz1G-n"><b>[8] 2017 (ICCV) Unsupervised Representation Learning by Sorting Sequences</b></p><p data-pid="Q7ePcz3K">ICCV 2017 这篇论文里，采取的自监督的方法是：随机打乱视频帧，然后学习对它们排序。</p><figure data-size="normal"><noscript><img src="https://pic3.zhimg.com/v2-1d83a09ddf08a58728cc3ee724d63f1a_b.jpg" data-caption="" data-size="normal" data-rawwidth="1132" data-rawheight="686" class="origin_image zh-lightbox-thumb" width="1132" data-original="https://pic3.zhimg.com/v2-1d83a09ddf08a58728cc3ee724d63f1a_r.jpg"/></noscript><div><img src="data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1132&#39; height=&#39;686&#39;&gt;&lt;/svg&gt;" data-caption="" data-size="normal" data-rawwidth="1132" data-rawheight="686" class="origin_image zh-lightbox-thumb lazy" width="1132" data-original="https://pic3.zhimg.com/v2-1d83a09ddf08a58728cc3ee724d63f1a_r.jpg" data-actualsrc="https://pic3.zhimg.com/v2-1d83a09ddf08a58728cc3ee724d63f1a_b.jpg"></div></figure><p data-pid="Vwm8hpfz"><b>[9] 2018 (Google) (ICRA) Time-Contrastive Networks: Self-Supervised Learning from Video</b></p><p data-pid="N71M5V8v">ICRA 2018这篇论文里，采取的自监督的方法是：从视频里取一帧，然后选临近的帧作为正例，随机的离得远的帧作为负例，来学习DNN网络，然后结合强化学习，应用到机器人控制。</p><figure data-size="normal"><noscript><img src="https://pic4.zhimg.com/v2-bb5bf678605cbde87c04d840ca81ed9b_b.jpg" data-caption="" data-size="normal" data-rawwidth="1204" data-rawheight="976" class="origin_image zh-lightbox-thumb" width="1204" data-original="https://pic4.zhimg.com/v2-bb5bf678605cbde87c04d840ca81ed9b_r.jpg"/></noscript><div><img src="data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1204&#39; height=&#39;976&#39;&gt;&lt;/svg&gt;" data-caption="" data-size="normal" data-rawwidth="1204" data-rawheight="976" class="origin_image zh-lightbox-thumb lazy" width="1204" data-original="https://pic4.zhimg.com/v2-bb5bf678605cbde87c04d840ca81ed9b_r.jpg" data-actualsrc="https://pic4.zhimg.com/v2-bb5bf678605cbde87c04d840ca81ed9b_b.jpg"></div></figure><figure data-size="normal"><noscript><img src="https://pic4.zhimg.com/v2-75b6f0bf1e724c89210e9febd8fa9f43_b.jpg" data-caption="" data-size="normal" data-rawwidth="1224" data-rawheight="836" class="origin_image zh-lightbox-thumb" width="1224" data-original="https://pic4.zhimg.com/v2-75b6f0bf1e724c89210e9febd8fa9f43_r.jpg"/></noscript><div><img src="data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1224&#39; height=&#39;836&#39;&gt;&lt;/svg&gt;" data-caption="" data-size="normal" data-rawwidth="1224" data-rawheight="836" class="origin_image zh-lightbox-thumb lazy" width="1224" data-original="https://pic4.zhimg.com/v2-75b6f0bf1e724c89210e9febd8fa9f43_r.jpg" data-actualsrc="https://pic4.zhimg.com/v2-75b6f0bf1e724c89210e9febd8fa9f43_b.jpg"></div></figure><p data-pid="JC6ArrR2"><b>[10] 2018 (DeepMind) (Arxiv) (CPC) Representation Learning with Contrastive Predictive Coding</b></p><p data-pid="zXKWSNzo">2018年DeepMind提出CPC，基本的思想是context信息能用来预测target的原因是，context的high-level表示是自回归依赖的。所以可以对输入x通过编码器encode到high-level表示，然后在high-level表示层学习出 <span class="ztext-math" data-eeimg="1" data-tex="c_t"><span></span><span><script type="math/tex;mode=inline">c_t</script><span class="tex2jax_ignore math-holder">c_t</span></span></span> ，用来进行预测。</p><figure data-size="normal"><noscript><img src="https://pic4.zhimg.com/v2-9fdf0416f48c779add2c77cbf3de10cf_b.jpg" data-caption="" data-size="normal" data-rawwidth="1380" data-rawheight="572" class="origin_image zh-lightbox-thumb" width="1380" data-original="https://pic4.zhimg.com/v2-9fdf0416f48c779add2c77cbf3de10cf_r.jpg"/></noscript><div><img src="data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1380&#39; height=&#39;572&#39;&gt;&lt;/svg&gt;" data-caption="" data-size="normal" data-rawwidth="1380" data-rawheight="572" class="origin_image zh-lightbox-thumb lazy" width="1380" data-original="https://pic4.zhimg.com/v2-9fdf0416f48c779add2c77cbf3de10cf_r.jpg" data-actualsrc="https://pic4.zhimg.com/v2-9fdf0416f48c779add2c77cbf3de10cf_b.jpg"></div></figure><p data-pid="NkuGDl5k">优化的目标是x和c的Mutual Information最大：</p><figure data-size="normal"><noscript><img src="https://pic3.zhimg.com/v2-cabb9f9d86acdb37f47f9cf4184c66ae_b.jpg" data-caption="" data-size="normal" data-rawwidth="462" data-rawheight="118" class="origin_image zh-lightbox-thumb" width="462" data-original="https://pic3.zhimg.com/v2-cabb9f9d86acdb37f47f9cf4184c66ae_r.jpg"/></noscript><div><img src="./自监督学习_ 人工智能的未来 - 知乎_files/v2-cabb9f9d86acdb37f47f9cf4184c66ae_720w.webp" data-caption="" data-size="normal" data-rawwidth="462" data-rawheight="118" class="origin_image zh-lightbox-thumb lazy" width="462" data-original="https://pic3.zhimg.com/v2-cabb9f9d86acdb37f47f9cf4184c66ae_r.jpg" data-actualsrc="https://pic3.zhimg.com/v2-cabb9f9d86acdb37f47f9cf4184c66ae_b.jpg" height="118" data-lazy-status="ok"></div></figure><p data-pid="8uEY9jCC">直接通过生成模型用 <span class="ztext-math" data-eeimg="1" data-tex="c_t"><span></span><span><span class="MathJax_Preview" style="color: inherit;"></span><span class="MathJax_SVG" id="MathJax-Element-1-Frame" tabindex="0" style="font-size: 100%; display: inline-block; position: relative;" data-mathml="&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;&gt;&lt;msub&gt;&lt;mi&gt;c&lt;/mi&gt;&lt;mi&gt;t&lt;/mi&gt;&lt;/msub&gt;&lt;/math&gt;" role="presentation"><svg xmlns:xlink="http://www.w3.org/1999/xlink" width="1.833ex" height="1.747ex" viewBox="0 -500.7 789.1 752.1" role="img" focusable="false" aria-hidden="true" style="vertical-align: -0.584ex;"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="matrix(1 0 0 -1 0 0)"><use href="#MJMATHI-63" x="0" y="0"></use><use transform="scale(0.707)" href="#MJMATHI-74" x="613" y="-213"></use></g></svg><span class="MJX_Assistive_MathML" role="presentation"><math xmlns="http://www.w3.org/1998/Math/MathML"><msub><mi>c</mi><mi>t</mi></msub></math></span></span><script type="math/tex;mode=inline" id="MathJax-Element-1">c_t</script><span class="tex2jax_ignore math-holder">c_t</span></span></span> 预测未来的 <span class="ztext-math" data-eeimg="1" data-tex="x_{t+k}"><span></span><span><span class="MathJax_Preview" style="color: inherit;"></span><span class="MathJax_SVG" id="MathJax-Element-2-Frame" tabindex="0" style="font-size: 100%; display: inline-block; position: relative;" data-mathml="&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;&gt;&lt;msub&gt;&lt;mi&gt;x&lt;/mi&gt;&lt;mrow class=&quot;MJX-TeXAtom-ORD&quot;&gt;&lt;mi&gt;t&lt;/mi&gt;&lt;mo&gt;+&lt;/mo&gt;&lt;mi&gt;k&lt;/mi&gt;&lt;/mrow&gt;&lt;/msub&gt;&lt;/math&gt;" role="presentation"><svg xmlns:xlink="http://www.w3.org/1999/xlink" width="4.291ex" height="1.863ex" viewBox="0 -500.7 1847.4 801.9" role="img" focusable="false" aria-hidden="true" style="vertical-align: -0.7ex;"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="matrix(1 0 0 -1 0 0)"><use href="#MJMATHI-78" x="0" y="0"></use><g transform="translate(572,-150)"><use transform="scale(0.707)" href="#MJMATHI-74" x="0" y="0"></use><use transform="scale(0.707)" href="#MJMAIN-2B" x="361" y="0"></use><use transform="scale(0.707)" href="#MJMATHI-6B" x="1140" y="0"></use></g></g></svg><span class="MJX_Assistive_MathML" role="presentation"><math xmlns="http://www.w3.org/1998/Math/MathML"><msub><mi>x</mi><mrow class="MJX-TeXAtom-ORD"><mi>t</mi><mo>+</mo><mi>k</mi></mrow></msub></math></span></span><script type="math/tex;mode=inline" id="MathJax-Element-2">x_{t+k}</script><span class="tex2jax_ignore math-holder">x_{t+k}</span></span></span> 比较困难， 所以作者定义了概率密度来保持 <span class="ztext-math" data-eeimg="1" data-tex="x_{t+k}"><span></span><span><span class="MathJax_Preview" style="color: inherit;"></span><span class="MathJax_SVG" id="MathJax-Element-4-Frame" tabindex="0" style="font-size: 100%; display: inline-block; position: relative;" data-mathml="&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;&gt;&lt;msub&gt;&lt;mi&gt;x&lt;/mi&gt;&lt;mrow class=&quot;MJX-TeXAtom-ORD&quot;&gt;&lt;mi&gt;t&lt;/mi&gt;&lt;mo&gt;+&lt;/mo&gt;&lt;mi&gt;k&lt;/mi&gt;&lt;/mrow&gt;&lt;/msub&gt;&lt;/math&gt;" role="presentation"><svg xmlns:xlink="http://www.w3.org/1999/xlink" width="4.291ex" height="1.863ex" viewBox="0 -500.7 1847.4 801.9" role="img" focusable="false" aria-hidden="true" style="vertical-align: -0.7ex;"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="matrix(1 0 0 -1 0 0)"><use href="#MJMATHI-78" x="0" y="0"></use><g transform="translate(572,-150)"><use transform="scale(0.707)" href="#MJMATHI-74" x="0" y="0"></use><use transform="scale(0.707)" href="#MJMAIN-2B" x="361" y="0"></use><use transform="scale(0.707)" href="#MJMATHI-6B" x="1140" y="0"></use></g></g></svg><span class="MJX_Assistive_MathML" role="presentation"><math xmlns="http://www.w3.org/1998/Math/MathML"><msub><mi>x</mi><mrow class="MJX-TeXAtom-ORD"><mi>t</mi><mo>+</mo><mi>k</mi></mrow></msub></math></span></span><script type="math/tex;mode=inline" id="MathJax-Element-4">x_{t+k}</script><span class="tex2jax_ignore math-holder">x_{t+k}</span></span></span> 和 <span class="ztext-math" data-eeimg="1" data-tex="c_t"><span></span><span><span class="MathJax_Preview" style="color: inherit;"></span><span class="MathJax_SVG" id="MathJax-Element-3-Frame" tabindex="0" style="font-size: 100%; display: inline-block; position: relative;" data-mathml="&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;&gt;&lt;msub&gt;&lt;mi&gt;c&lt;/mi&gt;&lt;mi&gt;t&lt;/mi&gt;&lt;/msub&gt;&lt;/math&gt;" role="presentation"><svg xmlns:xlink="http://www.w3.org/1999/xlink" width="1.833ex" height="1.747ex" viewBox="0 -500.7 789.1 752.1" role="img" focusable="false" aria-hidden="true" style="vertical-align: -0.584ex;"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="matrix(1 0 0 -1 0 0)"><use href="#MJMATHI-63" x="0" y="0"></use><use transform="scale(0.707)" href="#MJMATHI-74" x="613" y="-213"></use></g></svg><span class="MJX_Assistive_MathML" role="presentation"><math xmlns="http://www.w3.org/1998/Math/MathML"><msub><mi>c</mi><mi>t</mi></msub></math></span></span><script type="math/tex;mode=inline" id="MathJax-Element-3">c_t</script><span class="tex2jax_ignore math-holder">c_t</span></span></span> 的互信息：</p><figure data-size="normal"><noscript><img src="https://pic3.zhimg.com/v2-5b48479e4efa988eecd1bf873600fb0e_b.jpg" data-caption="" data-size="normal" data-rawwidth="420" data-rawheight="128" class="content_image" width="420"/></noscript><div><img src="./自监督学习_ 人工智能的未来 - 知乎_files/v2-5b48479e4efa988eecd1bf873600fb0e_720w.webp" data-caption="" data-size="normal" data-rawwidth="420" data-rawheight="128" class="content_image lazy" width="420" data-actualsrc="https://pic3.zhimg.com/v2-5b48479e4efa988eecd1bf873600fb0e_b.jpg" height="128" data-lazy-status="ok"></div></figure><p data-pid="UwiDLtxk">作者采用的公式是：</p><figure data-size="normal"><noscript><img src="https://pic4.zhimg.com/v2-3cc478a1f3cbc7b72115a02f48a912df_b.jpg" data-caption="" data-size="normal" data-rawwidth="516" data-rawheight="106" class="origin_image zh-lightbox-thumb" width="516" data-original="https://pic4.zhimg.com/v2-3cc478a1f3cbc7b72115a02f48a912df_r.jpg"/></noscript><div><img src="./自监督学习_ 人工智能的未来 - 知乎_files/v2-3cc478a1f3cbc7b72115a02f48a912df_720w.webp" data-caption="" data-size="normal" data-rawwidth="516" data-rawheight="106" class="origin_image zh-lightbox-thumb lazy" width="516" data-original="https://pic4.zhimg.com/v2-3cc478a1f3cbc7b72115a02f48a912df_r.jpg" data-actualsrc="https://pic4.zhimg.com/v2-3cc478a1f3cbc7b72115a02f48a912df_b.jpg" height="106" data-lazy-status="ok"></div></figure><p data-pid="3GOP7JgK">作者使用InfoNCE loss来优化，使Mutual Information最大：</p><figure data-size="normal"><noscript><img src="https://pic1.zhimg.com/v2-57b52a5645eb083714a303e4e8dfad6c_b.jpg" data-caption="" data-size="normal" data-rawwidth="570" data-rawheight="134" class="origin_image zh-lightbox-thumb" width="570" data-original="https://pic1.zhimg.com/v2-57b52a5645eb083714a303e4e8dfad6c_r.jpg"/></noscript><div><img src="./自监督学习_ 人工智能的未来 - 知乎_files/v2-57b52a5645eb083714a303e4e8dfad6c_720w.webp" data-caption="" data-size="normal" data-rawwidth="570" data-rawheight="134" class="origin_image zh-lightbox-thumb lazy" width="570" data-original="https://pic1.zhimg.com/v2-57b52a5645eb083714a303e4e8dfad6c_r.jpg" data-actualsrc="https://pic1.zhimg.com/v2-57b52a5645eb083714a303e4e8dfad6c_b.jpg" height="134" data-lazy-status="ok"></div></figure><p data-pid="dMTk1-GC">其中X为N个随机sample，包含1个正例和N-1个负例。</p><p data-pid="egka5e9K"><b>[11] 2019 (ICLR) [DIM] Learning deep representations by mutual information estimation and maximization</b></p><figure data-size="normal"><noscript><img src="https://pic1.zhimg.com/v2-f80bc359f9c6aa8b5602c21b5e2a9850_b.jpg" data-caption="" data-size="normal" data-rawwidth="1378" data-rawheight="830" class="origin_image zh-lightbox-thumb" width="1378" data-original="https://pic1.zhimg.com/v2-f80bc359f9c6aa8b5602c21b5e2a9850_r.jpg"/></noscript><div><img src="./自监督学习_ 人工智能的未来 - 知乎_files/v2-f80bc359f9c6aa8b5602c21b5e2a9850_720w.webp" data-caption="" data-size="normal" data-rawwidth="1378" data-rawheight="830" class="origin_image zh-lightbox-thumb lazy" width="1378" data-original="https://pic1.zhimg.com/v2-f80bc359f9c6aa8b5602c21b5e2a9850_r.jpg" data-actualsrc="https://pic1.zhimg.com/v2-f80bc359f9c6aa8b5602c21b5e2a9850_b.jpg" height="830" data-lazy-status="ok"></div></figure><p data-pid="fjv6U_sq"><b>[12] 2020 (Hinton) (ICML) [SimCLR] A Simple Framework for Contrastive Learning of Visual Representations</b></p><p data-pid="h6skrCQ4">Hinton等人，在这篇论文里提出SimCLR:</p><p data-pid="VFdxxa_7">对一个输入x，首先通过两次独立的数据增强生成两个样本，然后分别通过编码器，再分别通过投影:</p><figure data-size="normal"><noscript><img src="https://pic1.zhimg.com/v2-90ecb20682b34725d86ad3e32a5745b8_b.jpg" data-caption="" data-size="normal" data-rawwidth="710" data-rawheight="688" class="origin_image zh-lightbox-thumb" width="710" data-original="https://pic1.zhimg.com/v2-90ecb20682b34725d86ad3e32a5745b8_r.jpg"/></noscript><div><img src="./自监督学习_ 人工智能的未来 - 知乎_files/v2-90ecb20682b34725d86ad3e32a5745b8_720w.webp" data-caption="" data-size="normal" data-rawwidth="710" data-rawheight="688" class="origin_image zh-lightbox-thumb lazy" width="710" data-original="https://pic1.zhimg.com/v2-90ecb20682b34725d86ad3e32a5745b8_r.jpg" data-actualsrc="https://pic1.zhimg.com/v2-90ecb20682b34725d86ad3e32a5745b8_b.jpg" height="688" data-lazy-status="ok"></div></figure><p data-pid="1BF7aWzJ">最后在投影空间定义对比loss，其中负样本从batch中的其他图片：</p><figure data-size="normal"><noscript><img src="https://pic4.zhimg.com/v2-0ba303520713a355d4f660b0b1b2f67f_b.jpg" data-caption="" data-size="normal" data-rawwidth="566" data-rawheight="92" class="origin_image zh-lightbox-thumb" width="566" data-original="https://pic4.zhimg.com/v2-0ba303520713a355d4f660b0b1b2f67f_r.jpg"/></noscript><div><img src="data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;566&#39; height=&#39;92&#39;&gt;&lt;/svg&gt;" data-caption="" data-size="normal" data-rawwidth="566" data-rawheight="92" class="origin_image zh-lightbox-thumb lazy" width="566" data-original="https://pic4.zhimg.com/v2-0ba303520713a355d4f660b0b1b2f67f_r.jpg" data-actualsrc="https://pic4.zhimg.com/v2-0ba303520713a355d4f660b0b1b2f67f_b.jpg"></div></figure><p class="ztext-empty-paragraph"><br></p><p data-pid="p1G2lbzD"><b>[13] 2020 (Hinton) (Arxiv) Big Self-Supervised Models are Strong Semi-Supervised Learners</b></p><p data-pid="gHZi-U0z">Hinton等人在这篇论文里，首先在大量无监督数据上自监督学习task-agnostic的encoder，然后基于少量的label数据finetune，再通过蒸馏技术，利用无监督的标签或有标签数据，学习得到更强的task-specific的encoder，对面向具体任务的迁移学习很有指导意义。</p><figure data-size="normal"><noscript><img src="https://pic3.zhimg.com/v2-3bee785243d01ba30a4b6d7080d78af6_b.jpg" data-caption="" data-size="normal" data-rawwidth="1374" data-rawheight="734" class="origin_image zh-lightbox-thumb" width="1374" data-original="https://pic3.zhimg.com/v2-3bee785243d01ba30a4b6d7080d78af6_r.jpg"/></noscript><div><img src="data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1374&#39; height=&#39;734&#39;&gt;&lt;/svg&gt;" data-caption="" data-size="normal" data-rawwidth="1374" data-rawheight="734" class="origin_image zh-lightbox-thumb lazy" width="1374" data-original="https://pic3.zhimg.com/v2-3bee785243d01ba30a4b6d7080d78af6_r.jpg" data-actualsrc="https://pic3.zhimg.com/v2-3bee785243d01ba30a4b6d7080d78af6_b.jpg"></div></figure><p data-pid="3YGoFZbE">Distillation：</p><p data-pid="T9mEKHL2">无标签数据：</p><figure data-size="normal"><noscript><img src="https://pic1.zhimg.com/v2-94f03de5a4cad1d61b17b833ec4bd74c_b.jpg" data-caption="" data-size="normal" data-rawwidth="940" data-rawheight="168" class="origin_image zh-lightbox-thumb" width="940" data-original="https://pic1.zhimg.com/v2-94f03de5a4cad1d61b17b833ec4bd74c_r.jpg"/></noscript><div><img src="data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;940&#39; height=&#39;168&#39;&gt;&lt;/svg&gt;" data-caption="" data-size="normal" data-rawwidth="940" data-rawheight="168" class="origin_image zh-lightbox-thumb lazy" width="940" data-original="https://pic1.zhimg.com/v2-94f03de5a4cad1d61b17b833ec4bd74c_r.jpg" data-actualsrc="https://pic1.zhimg.com/v2-94f03de5a4cad1d61b17b833ec4bd74c_b.jpg"></div></figure><p data-pid="QiBTbGgS">无标签数据+标签数据：</p><figure data-size="normal"><noscript><img src="https://pic3.zhimg.com/v2-dae93d4aae43b79da822931b1ca511d6_b.jpg" data-caption="" data-size="normal" data-rawwidth="1610" data-rawheight="168" class="origin_image zh-lightbox-thumb" width="1610" data-original="https://pic3.zhimg.com/v2-dae93d4aae43b79da822931b1ca511d6_r.jpg"/></noscript><div><img src="data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1610&#39; height=&#39;168&#39;&gt;&lt;/svg&gt;" data-caption="" data-size="normal" data-rawwidth="1610" data-rawheight="168" class="origin_image zh-lightbox-thumb lazy" width="1610" data-original="https://pic3.zhimg.com/v2-dae93d4aae43b79da822931b1ca511d6_r.jpg" data-actualsrc="https://pic3.zhimg.com/v2-dae93d4aae43b79da822931b1ca511d6_b.jpg"></div></figure><p data-pid="CUddVT9s"><b>[14] 2020 (Google) (Arxiv) Supervised Contrastive Learning</b></p><p data-pid="bDFhif0k">一般自监督学习，对每个anchor，一般是一个positive, 多个negative。这篇论文里，提出在学习时，使用多个positive和多个negative一起学习。</p><figure data-size="normal"><noscript><img src="https://pic3.zhimg.com/v2-498e318c08bd29174814496e48c169e6_b.jpg" data-caption="" data-size="normal" data-rawwidth="1330" data-rawheight="616" class="origin_image zh-lightbox-thumb" width="1330" data-original="https://pic3.zhimg.com/v2-498e318c08bd29174814496e48c169e6_r.jpg"/></noscript><div><img src="data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1330&#39; height=&#39;616&#39;&gt;&lt;/svg&gt;" data-caption="" data-size="normal" data-rawwidth="1330" data-rawheight="616" class="origin_image zh-lightbox-thumb lazy" width="1330" data-original="https://pic3.zhimg.com/v2-498e318c08bd29174814496e48c169e6_r.jpg" data-actualsrc="https://pic3.zhimg.com/v2-498e318c08bd29174814496e48c169e6_b.jpg"></div></figure><p class="ztext-empty-paragraph"><br></p><p data-pid="r2LShtuI"><b>[15] 2020 (CVPR) [MoCo] Momentum Contrast for Unsupervised Visual Representation Learning</b></p><figure data-size="normal"><noscript><img src="https://pic4.zhimg.com/v2-1e6ee69cf2feea5652c9c9021f565907_b.jpg" data-caption="" data-size="normal" data-rawwidth="1866" data-rawheight="650" class="origin_image zh-lightbox-thumb" width="1866" data-original="https://pic4.zhimg.com/v2-1e6ee69cf2feea5652c9c9021f565907_r.jpg"/></noscript><div><img src="data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1866&#39; height=&#39;650&#39;&gt;&lt;/svg&gt;" data-caption="" data-size="normal" data-rawwidth="1866" data-rawheight="650" class="origin_image zh-lightbox-thumb lazy" width="1866" data-original="https://pic4.zhimg.com/v2-1e6ee69cf2feea5652c9c9021f565907_r.jpg" data-actualsrc="https://pic4.zhimg.com/v2-1e6ee69cf2feea5652c9c9021f565907_b.jpg"></div></figure><p data-pid="MDbQVv0m"><b>[16] 2020 (DeepMind) (Arxiv) [BYOL] Bootstrap Your Own Latent: A New Approach to Self-Supervised Learning</b></p><p data-pid="qlJilgIA">一般自监督学习，需要负样本。这篇论文里，DeepMing的作者提出了BYOL，不需要负样本，也能自监督学习。</p><figure data-size="normal"><noscript><img src="https://pic1.zhimg.com/v2-9902c574b5d354e697f13b7190a8ecc0_b.jpg" data-caption="" data-size="normal" data-rawwidth="1742" data-rawheight="660" class="origin_image zh-lightbox-thumb" width="1742" data-original="https://pic1.zhimg.com/v2-9902c574b5d354e697f13b7190a8ecc0_r.jpg"/></noscript><div><img src="data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1742&#39; height=&#39;660&#39;&gt;&lt;/svg&gt;" data-caption="" data-size="normal" data-rawwidth="1742" data-rawheight="660" class="origin_image zh-lightbox-thumb lazy" width="1742" data-original="https://pic1.zhimg.com/v2-9902c574b5d354e697f13b7190a8ecc0_r.jpg" data-actualsrc="https://pic1.zhimg.com/v2-9902c574b5d354e697f13b7190a8ecc0_b.jpg"></div></figure><p data-pid="bBi0RlPW"><b>2020 (Arxiv) [SimSiam] Exploring Simple Siamese Representation Learning</b></p><figure data-size="normal"><noscript><img src="https://pic4.zhimg.com/v2-b673b20abd42ba8e831c1158fffa6bbf_b.jpg" data-caption="" data-size="normal" data-rawwidth="1030" data-rawheight="748" class="origin_image zh-lightbox-thumb" width="1030" data-original="https://pic4.zhimg.com/v2-b673b20abd42ba8e831c1158fffa6bbf_r.jpg"/></noscript><div><img src="data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1030&#39; height=&#39;748&#39;&gt;&lt;/svg&gt;" data-caption="" data-size="normal" data-rawwidth="1030" data-rawheight="748" class="origin_image zh-lightbox-thumb lazy" width="1030" data-original="https://pic4.zhimg.com/v2-b673b20abd42ba8e831c1158fffa6bbf_r.jpg" data-actualsrc="https://pic4.zhimg.com/v2-b673b20abd42ba8e831c1158fffa6bbf_b.jpg"></div></figure><p data-pid="UOck5lx1"><b>2021 (Arxiv) [BarlowTwins] Barlow Twins: Self-Supervised Learning via Redundancy Reduction</b></p><figure data-size="normal"><noscript><img src="https://pic3.zhimg.com/v2-62eda1cb54b747ed1bdb818a68a836ea_b.jpg" data-caption="" data-size="normal" data-rawwidth="1310" data-rawheight="1222" class="origin_image zh-lightbox-thumb" width="1310" data-original="https://pic3.zhimg.com/v2-62eda1cb54b747ed1bdb818a68a836ea_r.jpg"/></noscript><div><img src="data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1310&#39; height=&#39;1222&#39;&gt;&lt;/svg&gt;" data-caption="" data-size="normal" data-rawwidth="1310" data-rawheight="1222" class="origin_image zh-lightbox-thumb lazy" width="1310" data-original="https://pic3.zhimg.com/v2-62eda1cb54b747ed1bdb818a68a836ea_r.jpg" data-actualsrc="https://pic3.zhimg.com/v2-62eda1cb54b747ed1bdb818a68a836ea_b.jpg"></div></figure><h2>3.2 Natural Language Processing：</h2><p data-pid="DfLIg2E6">NLP领域，包括多个任务：POS tagging, Named-entity recognition, Language Modeling, Word sense disambiguation, Summarization, Sentiment analysis, Text Classification, Relation Extraction, Question Answering, Machine Translation等。这些任务的标注成本很高，因此训练数据规模一般比较小，而无标签的文本数据却是海量的，因此通过自监督学习学习预训练模型，然后应用到下游任务，既是自然、也是必须的，已经成为state-of-the-art的NLP技术。</p><p data-pid="I5TjvLNq">▲NLP领域的自监督学习方法包括经典的Word2vec、ELMo、BERT、GPT、GPT3等。</p><p data-pid="pN9faQCm">预训练模型，应用到下游任务的方式包括：</p><p data-pid="pjFqiIxG">（1）Feature-based: 将训练好的embedding或模型输出，添加到下游任务的输入特征中。</p><p data-pid="-h8jwmpn">          例如Word2vec, ELMo</p><p data-pid="9hpCIUa9">  (2) Finetune: 预训练的模型基础上，添加输出层，用下游监督数据finetune模型参数。</p><p data-pid="MCYhK5zK">          例如BERT，GPT</p><p data-pid="A3lrp-kB"><b>[17] 2013 (Google) (NIPS) [word2vec] Distributed representations of words and phrases and their compositionality</b>     [<a href="https://link.zhihu.com/?target=http%3A//jalammar.github.io/illustrated-word2vec/" class=" wrap external" target="_blank" rel="nofollow noreferrer">Word2vec</a>]</p><p data-pid="40kiF71g">word2vec: Skip-gram模型，预测附近的单词。</p><figure data-size="normal"><noscript><img src="https://pic1.zhimg.com/v2-e0569daec46370cd2fe921410f20057c_b.jpg" data-caption="" data-size="normal" data-rawwidth="576" data-rawheight="682" class="origin_image zh-lightbox-thumb" width="576" data-original="https://pic1.zhimg.com/v2-e0569daec46370cd2fe921410f20057c_r.jpg"/></noscript><div><img src="data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;576&#39; height=&#39;682&#39;&gt;&lt;/svg&gt;" data-caption="" data-size="normal" data-rawwidth="576" data-rawheight="682" class="origin_image zh-lightbox-thumb lazy" width="576" data-original="https://pic1.zhimg.com/v2-e0569daec46370cd2fe921410f20057c_r.jpg" data-actualsrc="https://pic1.zhimg.com/v2-e0569daec46370cd2fe921410f20057c_b.jpg"></div></figure><figure data-size="normal"><noscript><img src="https://pic2.zhimg.com/v2-4ac9f1d8ed2d1254cb82f7b7917fecbd_b.jpg" data-caption="" data-size="normal" data-rawwidth="636" data-rawheight="184" class="origin_image zh-lightbox-thumb" width="636" data-original="https://pic2.zhimg.com/v2-4ac9f1d8ed2d1254cb82f7b7917fecbd_r.jpg"/></noscript><div><img src="data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;636&#39; height=&#39;184&#39;&gt;&lt;/svg&gt;" data-caption="" data-size="normal" data-rawwidth="636" data-rawheight="184" class="origin_image zh-lightbox-thumb lazy" width="636" data-original="https://pic2.zhimg.com/v2-4ac9f1d8ed2d1254cb82f7b7917fecbd_r.jpg" data-actualsrc="https://pic2.zhimg.com/v2-4ac9f1d8ed2d1254cb82f7b7917fecbd_b.jpg"></div></figure><p class="ztext-empty-paragraph"><br></p><p data-pid="HOdeihru">NEG loss:</p><figure data-size="normal"><noscript><img src="https://pic2.zhimg.com/v2-657feae54fd7a69f02f636fb77243fb5_b.jpg" data-caption="" data-size="normal" data-rawwidth="880" data-rawheight="148" class="origin_image zh-lightbox-thumb" width="880" data-original="https://pic2.zhimg.com/v2-657feae54fd7a69f02f636fb77243fb5_r.jpg"/></noscript><div><img src="data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;880&#39; height=&#39;148&#39;&gt;&lt;/svg&gt;" data-caption="" data-size="normal" data-rawwidth="880" data-rawheight="148" class="origin_image zh-lightbox-thumb lazy" width="880" data-original="https://pic2.zhimg.com/v2-657feae54fd7a69f02f636fb77243fb5_r.jpg" data-actualsrc="https://pic2.zhimg.com/v2-657feae54fd7a69f02f636fb77243fb5_b.jpg"></div></figure><p data-pid="2YfWCjFV"><b>[18] 2017 (Google) (NIPS) [Transformer] Attention is All you Need </b>   [<a href="https://link.zhihu.com/?target=http%3A//jalammar.github.io/illustrated-transformer/" class=" wrap external" target="_blank" rel="nofollow noreferrer">Transformer</a>]</p><p data-pid="3pLktBh_">Transformer作为目前最强大的序列建模模型，是BERT等预训练模型的基本单元。</p><p data-pid="0Nzc9iVJ"><b>[19] 2018 (ACL) (ELMo) Deep contextualized word representations</b></p><p data-pid="y9SoUXxH">word2vec的skip-gram模型中用一个单词预测附近的一个单词，ELMo模型的思想是使用整个句子学习更好的word embedding，来更好地建模单词的语法、语义、多义问题。</p><p data-pid="7w_XqIM3">▲ELMo (Embeddings from Language Models) 模型中，作者使用Deep Bi-LSTM来建模一个句子，使用language model来学习模型参数：</p><figure data-size="normal"><noscript><img src="https://pic4.zhimg.com/v2-09f64000144e9a2ae1ee443cdf65cfd7_b.jpg" data-caption="" data-size="normal" data-rawwidth="756" data-rawheight="224" class="origin_image zh-lightbox-thumb" width="756" data-original="https://pic4.zhimg.com/v2-09f64000144e9a2ae1ee443cdf65cfd7_r.jpg"/></noscript><div><img src="data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;756&#39; height=&#39;224&#39;&gt;&lt;/svg&gt;" data-caption="" data-size="normal" data-rawwidth="756" data-rawheight="224" class="origin_image zh-lightbox-thumb lazy" width="756" data-original="https://pic4.zhimg.com/v2-09f64000144e9a2ae1ee443cdf65cfd7_r.jpg" data-actualsrc="https://pic4.zhimg.com/v2-09f64000144e9a2ae1ee443cdf65cfd7_b.jpg"></div></figure><p data-pid="tbiEowIw">▲把模型中多层的LSTM隐藏向量结合起来作为word的表示：</p><figure data-size="normal"><noscript><img src="https://pic2.zhimg.com/v2-6aa652be7ab12fa4a031f1726162eb45_b.jpg" data-caption="" data-size="normal" data-rawwidth="790" data-rawheight="286" class="origin_image zh-lightbox-thumb" width="790" data-original="https://pic2.zhimg.com/v2-6aa652be7ab12fa4a031f1726162eb45_r.jpg"/></noscript><div><img src="data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;790&#39; height=&#39;286&#39;&gt;&lt;/svg&gt;" data-caption="" data-size="normal" data-rawwidth="790" data-rawheight="286" class="origin_image zh-lightbox-thumb lazy" width="790" data-original="https://pic2.zhimg.com/v2-6aa652be7ab12fa4a031f1726162eb45_r.jpg" data-actualsrc="https://pic2.zhimg.com/v2-6aa652be7ab12fa4a031f1726162eb45_b.jpg"></div></figure><p data-pid="NdiNyf43">作者指出word的多层表示中，底层的表示一般包括语法（适合Pos tagging任务）、上层的表示更好地建模context信息（适合问答任务）。</p><p data-pid="eGN3tUOZ">▲ELMo中将预训练模型应用到下游任务的方式是feature-based，即将模型输出作为下游任务的特征。通过把这些不同的表示通过权重结合起来，添加到下游任务输入中，来提高下游模型指标：</p><figure data-size="normal"><noscript><img src="https://pic4.zhimg.com/v2-b3d99e5bcda9a995cb63d59d2c22eeb7_b.jpg" data-caption="" data-size="normal" data-rawwidth="790" data-rawheight="150" class="origin_image zh-lightbox-thumb" width="790" data-original="https://pic4.zhimg.com/v2-b3d99e5bcda9a995cb63d59d2c22eeb7_r.jpg"/></noscript><div><img src="data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;790&#39; height=&#39;150&#39;&gt;&lt;/svg&gt;" data-caption="" data-size="normal" data-rawwidth="790" data-rawheight="150" class="origin_image zh-lightbox-thumb lazy" width="790" data-original="https://pic4.zhimg.com/v2-b3d99e5bcda9a995cb63d59d2c22eeb7_r.jpg" data-actualsrc="https://pic4.zhimg.com/v2-b3d99e5bcda9a995cb63d59d2c22eeb7_b.jpg"></div></figure><p data-pid="ICy9Om-r"><b>[20] 2018 (Google) (NAACL) [BERT] Bert: Pre-training of deep bidirectional transformers for language understanding. </b> [<a href="https://link.zhihu.com/?target=http%3A//jalammar.github.io/a-visual-guide-to-using-bert-for-the-first-time/" class=" wrap external" target="_blank" rel="nofollow noreferrer">BERT example</a>][<a href="https://link.zhihu.com/?target=http%3A//jalammar.github.io/illustrated-bert/" class=" wrap external" target="_blank" rel="nofollow noreferrer">BERT</a>]</p><p data-pid="mk8mQyfq">ELMo和GPT模型，预训练模型时采用的标准language model是单向的，对需要sentence-level或context-level信息的任务来说是不好的，BERT使用双向的Transformer来解决这个问题。</p><figure data-size="normal"><noscript><img src="https://pic4.zhimg.com/v2-ae7b4f30628e63b82b5fa1eb87b736a3_b.jpg" data-size="normal" data-rawwidth="1814" data-rawheight="768" class="origin_image zh-lightbox-thumb" width="1814" data-original="https://pic4.zhimg.com/v2-ae7b4f30628e63b82b5fa1eb87b736a3_r.jpg"/></noscript><div><img src="data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1814&#39; height=&#39;768&#39;&gt;&lt;/svg&gt;" data-size="normal" data-rawwidth="1814" data-rawheight="768" class="origin_image zh-lightbox-thumb lazy" width="1814" data-original="https://pic4.zhimg.com/v2-ae7b4f30628e63b82b5fa1eb87b736a3_r.jpg" data-actualsrc="https://pic4.zhimg.com/v2-ae7b4f30628e63b82b5fa1eb87b736a3_b.jpg"></div><figcaption>BERT, GPT, ELMo</figcaption></figure><p data-pid="QP_Fu5OA">BERT （Bidirectional Encoder Representations from Transformers）:</p><p data-pid="wU9BenWh"><b>▲阶段1 预训练模型:</b></p><p data-pid="OcWG28v4">(1) 提出Masked Language Model (MLM)预训练目标，来消除标准language model是单向的限制。MLM通过随机mask掉输入的一部分单词，然后使用句子中的其他context单词（包括左边的和右边的）来预测被mask的部分。从而BERT能够实现深度双向表示学习。</p><p data-pid="oa7NdPse">具体来说，构造训练数据时，随机选取输入句子中15%的token位置，对于第i个选取的token， 80%的概率替换为[MASK]，10%的概率替换为随机token， 10%的概率不变。然后用BERT模型输出层在第i个token位置的向量，预测原始的第i个token，使用cross entropy loss优化。</p><p data-pid="25WdggAE">(2) 提出Next Sentence Prediction预训练目标: 预测两个句子是否在源数据中是相邻的。</p><p data-pid="YJA_-cx5">QA等任务，基于两个句子的关系理解来完，但language model无法建模这些信息。BERT中提出的NSP任务，可以很好的解决这个问题。在构造训练数据时，对于A和B，50%的概率B是A后的下一个句子，50%的概率B是随机的一个句子。使用C来预测这个标签是否真的是下一个句子，使用cross entropy loss优化。</p><p data-pid="lRNJO43x">BERT使用BooksCorpus和Wikipedia来预训练。</p><p data-pid="u58wNdFp">▲<b>阶段2 Finetune：</b></p><p data-pid="82hzX-yK">BERT通过使用下游任务监督数据finetune所有参数，或将BERT在输入上的结果作为后续模型的输入，在下游多个任务取得很好地效果，已经成为NLP的主流技术。</p><figure data-size="normal"><noscript><img src="https://pic3.zhimg.com/v2-d4b69cd199e1cd172d8c0c0083a3725e_b.jpg" data-size="normal" data-rawwidth="1844" data-rawheight="1010" class="origin_image zh-lightbox-thumb" width="1844" data-original="https://pic3.zhimg.com/v2-d4b69cd199e1cd172d8c0c0083a3725e_r.jpg"/></noscript><div><img src="data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1844&#39; height=&#39;1010&#39;&gt;&lt;/svg&gt;" data-size="normal" data-rawwidth="1844" data-rawheight="1010" class="origin_image zh-lightbox-thumb lazy" width="1844" data-original="https://pic3.zhimg.com/v2-d4b69cd199e1cd172d8c0c0083a3725e_r.jpg" data-actualsrc="https://pic3.zhimg.com/v2-d4b69cd199e1cd172d8c0c0083a3725e_b.jpg"></div><figcaption>BERT</figcaption></figure><p data-pid="ZjkN-TcB">▲<b>具体任务Finetune的设计：</b></p><figure data-size="normal"><noscript><img src="https://pic3.zhimg.com/v2-f556aaa650e30145eb032342fdabbece_b.jpg" data-caption="" data-size="normal" data-rawwidth="1528" data-rawheight="1624" class="origin_image zh-lightbox-thumb" width="1528" data-original="https://pic3.zhimg.com/v2-f556aaa650e30145eb032342fdabbece_r.jpg"/></noscript><div><img src="data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1528&#39; height=&#39;1624&#39;&gt;&lt;/svg&gt;" data-caption="" data-size="normal" data-rawwidth="1528" data-rawheight="1624" class="origin_image zh-lightbox-thumb lazy" width="1528" data-original="https://pic3.zhimg.com/v2-f556aaa650e30145eb032342fdabbece_r.jpg" data-actualsrc="https://pic3.zhimg.com/v2-f556aaa650e30145eb032342fdabbece_b.jpg"></div></figure><p data-pid="OW5TMx2v">▲<b>BERT模型参数：</b></p><p data-pid="ej0d4FF9">BERT(base)包括12层，hidden size 768，self-attention head为12，参数量为1.1亿，与GPT参数相当。BERT(large)包括24层，hidden size 1024，self-attention head为16，参数量为3.4亿。</p><p data-pid="VWfumLr6">▲<b>BERT输入：</b></p><p data-pid="RM7jtSZ2">为了使BERT预训练后能应用到各种各样的任务中，BERT的输入是一个sentence，可以对应原始的一个句子或句子对。一个&lt;Question, Answer&gt;，可以通过[SEP]连接成一个sentence。</p><p data-pid="EtFNIhn_">其中第一个token是[CLS] ，用于汇总整个句子的表示，可以用于句子分类等任务。BERT采用WordPiece embedding, vocabulary size是30000。</p><p data-pid="0AfnvLGK">输入序列中每一个单元的表示包括：token embedding, 指示在句子A还是B的segment embedding, Position embedding。</p><figure data-size="normal"><noscript><img src="https://pic4.zhimg.com/v2-fa5c5a4582990fc88bb29794d7222d27_b.jpg" data-caption="" data-size="normal" data-rawwidth="1470" data-rawheight="508" class="origin_image zh-lightbox-thumb" width="1470" data-original="https://pic4.zhimg.com/v2-fa5c5a4582990fc88bb29794d7222d27_r.jpg"/></noscript><div><img src="data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1470&#39; height=&#39;508&#39;&gt;&lt;/svg&gt;" data-caption="" data-size="normal" data-rawwidth="1470" data-rawheight="508" class="origin_image zh-lightbox-thumb lazy" width="1470" data-original="https://pic4.zhimg.com/v2-fa5c5a4582990fc88bb29794d7222d27_r.jpg" data-actualsrc="https://pic4.zhimg.com/v2-fa5c5a4582990fc88bb29794d7222d27_b.jpg"></div></figure><p data-pid="QcRmJQK6"><b>[21] 2018 (OpenAI) (Arxiv) (GPT) Improving Language Understanding by Generative Pre-Training</b></p><p data-pid="ja-2dPPX">GPT模型的思想：基于大量无标签数据学习预训练模型，然后通过finetune应用到后续的多个任务。</p><p data-pid="tNNdBOLU">▲<b>阶段1：Unsupervised pre-training</b></p><p data-pid="OJW5v1UV">使用language modeling目标来优化：</p><figure data-size="normal"><noscript><img src="https://pic4.zhimg.com/v2-7cc105df426f41aa28d9e6f8344227fb_b.jpg" data-caption="" data-size="normal" data-rawwidth="1750" data-rawheight="340" class="origin_image zh-lightbox-thumb" width="1750" data-original="https://pic4.zhimg.com/v2-7cc105df426f41aa28d9e6f8344227fb_r.jpg"/></noscript><div><img src="data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1750&#39; height=&#39;340&#39;&gt;&lt;/svg&gt;" data-caption="" data-size="normal" data-rawwidth="1750" data-rawheight="340" class="origin_image zh-lightbox-thumb lazy" width="1750" data-original="https://pic4.zhimg.com/v2-7cc105df426f41aa28d9e6f8344227fb_r.jpg" data-actualsrc="https://pic4.zhimg.com/v2-7cc105df426f41aa28d9e6f8344227fb_b.jpg"></div></figure><p data-pid="iaUiiuwL">对应的模型结构，作者选取的是multi-layer Transformer Decoder （比Transformer少一半参数）：将输入转化为embedding，经过Masked Multi-head Self-Attention，然后使用softmax层即可预测输出概率分布。</p><figure data-size="normal"><noscript><img src="https://pic1.zhimg.com/v2-7b7ad4b3a2f9910602e3ffae324060e4_b.jpg" data-caption="" data-size="normal" data-rawwidth="1784" data-rawheight="308" class="origin_image zh-lightbox-thumb" width="1784" data-original="https://pic1.zhimg.com/v2-7b7ad4b3a2f9910602e3ffae324060e4_r.jpg"/></noscript><div><img src="data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1784&#39; height=&#39;308&#39;&gt;&lt;/svg&gt;" data-caption="" data-size="normal" data-rawwidth="1784" data-rawheight="308" class="origin_image zh-lightbox-thumb lazy" width="1784" data-original="https://pic1.zhimg.com/v2-7b7ad4b3a2f9910602e3ffae324060e4_r.jpg" data-actualsrc="https://pic1.zhimg.com/v2-7b7ad4b3a2f9910602e3ffae324060e4_b.jpg"></div></figure><p data-pid="4oyN-2zZ">▲<b>阶段2：Supervised fine-tuning</b></p><p data-pid="5AnGzzvj">阶段1学习得到的预训练模型，后面连接一个Linear层，即可实现下游任务的预测。</p><p data-pid="u3fW_vvg">对于下游任务，按下图的方式，构造模型的输入。例如对于文本分类问题：前后加入特殊字符后变成一个序列，经过预训练模型Transformer，Linear层即可实现预测。</p><figure data-size="normal"><noscript><img src="https://pic4.zhimg.com/v2-f6b64178ab4eb125fb6353ff25415c53_b.jpg" data-caption="" data-size="normal" data-rawwidth="1814" data-rawheight="542" class="origin_image zh-lightbox-thumb" width="1814" data-original="https://pic4.zhimg.com/v2-f6b64178ab4eb125fb6353ff25415c53_r.jpg"/></noscript><div><img src="data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1814&#39; height=&#39;542&#39;&gt;&lt;/svg&gt;" data-caption="" data-size="normal" data-rawwidth="1814" data-rawheight="542" class="origin_image zh-lightbox-thumb lazy" width="1814" data-original="https://pic4.zhimg.com/v2-f6b64178ab4eb125fb6353ff25415c53_r.jpg" data-actualsrc="https://pic4.zhimg.com/v2-f6b64178ab4eb125fb6353ff25415c53_b.jpg"></div></figure><figure data-size="normal"><noscript><img src="https://pic1.zhimg.com/v2-f40ca20b84969e13a2f2908ae5c26a98_b.jpg" data-caption="" data-size="normal" data-rawwidth="1848" data-rawheight="916" class="origin_image zh-lightbox-thumb" width="1848" data-original="https://pic1.zhimg.com/v2-f40ca20b84969e13a2f2908ae5c26a98_r.jpg"/></noscript><div><img src="data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1848&#39; height=&#39;916&#39;&gt;&lt;/svg&gt;" data-caption="" data-size="normal" data-rawwidth="1848" data-rawheight="916" class="origin_image zh-lightbox-thumb lazy" width="1848" data-original="https://pic1.zhimg.com/v2-f40ca20b84969e13a2f2908ae5c26a98_r.jpg" data-actualsrc="https://pic1.zhimg.com/v2-f40ca20b84969e13a2f2908ae5c26a98_b.jpg"></div></figure><p data-pid="OoW9c2oG">▲<b>论文实验结论：</b></p><p data-pid="sjkX0D1Q">（1）预训练能大幅提升在多个下游任务的指标。</p><p data-pid="-yIsQDFf">（2）预训练模型中的embedding、序列模型各层参数，都有助于提升下游任务。</p><p data-pid="FEqJNB-K">（3）Zero-shot Knowledge Transfer: 仅使用预训练模型，不用下游任务数据finetune, 预训练的越多，下游效果越好。</p><p data-pid="3GNQnL8Q">（4）在Finetune阶段也添加Language Modeling的辅助任务，有助于提升模型指标。简单理解是，在下游任务的数据上继续pre-training。</p><p data-pid="MZcLcbjL"><b>[22] 2019 (OpenAI) (Arxiv) (GPT-2) Language Models are Unsupervised Multitask Learners</b> [<a href="https://link.zhihu.com/?target=http%3A//jalammar.github.io/illustrated-gpt2/" class=" wrap external" target="_blank" rel="nofollow noreferrer">GPT2</a>]</p><p data-pid="lJMjzcdL">作者提出的15亿参数的Transformer预训练模型GPT-2，具有很强的Zero-shot Knowledge Transfer能力：在多个任务上，即使不用下游数据finetune，也取得了比用下游数据监督学习的模型更好的表现。</p><p data-pid="gXYSgz9U">GPT-2中采用了Byte Pair Encoding (BPE) 来取代character和word embedding。参数量如下：</p><figure data-size="normal"><noscript><img src="https://pic1.zhimg.com/v2-e6e32ab4101cc9c4af8c5aa814e6f9b4_b.jpg" data-caption="" data-size="normal" data-rawwidth="846" data-rawheight="336" class="origin_image zh-lightbox-thumb" width="846" data-original="https://pic1.zhimg.com/v2-e6e32ab4101cc9c4af8c5aa814e6f9b4_r.jpg"/></noscript><div><img src="data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;846&#39; height=&#39;336&#39;&gt;&lt;/svg&gt;" data-caption="" data-size="normal" data-rawwidth="846" data-rawheight="336" class="origin_image zh-lightbox-thumb lazy" width="846" data-original="https://pic1.zhimg.com/v2-e6e32ab4101cc9c4af8c5aa814e6f9b4_r.jpg" data-actualsrc="https://pic1.zhimg.com/v2-e6e32ab4101cc9c4af8c5aa814e6f9b4_b.jpg"></div></figure><p data-pid="jOqLyise">第一行参数量1.17亿，和GPT相当。第二行参数量3.45亿，和BERT相当。最大的参数量为15.42亿，为GPT-2。</p><p data-pid="yst4f6oM"><b>[23] 2020 (OpenAI) (Arixv) (GPT-3) Language Models are Few-Shot Learners </b>[<a href="https://link.zhihu.com/?target=http%3A//jalammar.github.io/how-gpt3-works-visualizations-animations/" class=" wrap external" target="_blank" rel="nofollow noreferrer">GPT3</a>]</p><p data-pid="ArneHU0h">GPT-3采取的模型和GPT-2相同，但参数量有1750亿。</p><p data-pid="XyicKtKX">关于参数量和下游任务的性能的分析图如下：</p><p data-pid="DwtsjOy_">可以看出：参数量越大，在Zero-shot, One-shot, Few-shot的情况下指标都不断提升。</p><figure data-size="normal"><noscript><img src="https://pic3.zhimg.com/v2-0f061d17b37e74389f4f1e2b8f591c72_b.jpg" data-caption="" data-size="normal" data-rawwidth="1564" data-rawheight="928" class="origin_image zh-lightbox-thumb" width="1564" data-original="https://pic3.zhimg.com/v2-0f061d17b37e74389f4f1e2b8f591c72_r.jpg"/></noscript><div><img src="data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1564&#39; height=&#39;928&#39;&gt;&lt;/svg&gt;" data-caption="" data-size="normal" data-rawwidth="1564" data-rawheight="928" class="origin_image zh-lightbox-thumb lazy" width="1564" data-original="https://pic3.zhimg.com/v2-0f061d17b37e74389f4f1e2b8f591c72_r.jpg" data-actualsrc="https://pic3.zhimg.com/v2-0f061d17b37e74389f4f1e2b8f591c72_b.jpg"></div></figure><p data-pid="F73BUg9g"><b>[24] 2019 (ACL) Self-Supervised Dialogue Learning</b></p><p data-pid="JK-JxbQV">这篇论文介绍，自监督学习如何应用到对话中。</p><p data-pid="ldr9dBCI"><b>[25] 2020 (ACL) Don’t Stop Pretraining: Adapt Language Models to Domains and Tasks</b></p><p data-pid="jrc7xGuR">ACL这篇论文，指出在通用语料上预训练后，继续在具体的任务语料上预训练，可进一步提升任务上的效果。</p><figure data-size="normal"><noscript><img src="https://pic2.zhimg.com/v2-49e616404c8907eca4565b00ead9ba55_b.jpg" data-caption="" data-size="normal" data-rawwidth="930" data-rawheight="564" class="origin_image zh-lightbox-thumb" width="930" data-original="https://pic2.zhimg.com/v2-49e616404c8907eca4565b00ead9ba55_r.jpg"/></noscript><div><img src="data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;930&#39; height=&#39;564&#39;&gt;&lt;/svg&gt;" data-caption="" data-size="normal" data-rawwidth="930" data-rawheight="564" class="origin_image zh-lightbox-thumb lazy" width="930" data-original="https://pic2.zhimg.com/v2-49e616404c8907eca4565b00ead9ba55_r.jpg" data-actualsrc="https://pic2.zhimg.com/v2-49e616404c8907eca4565b00ead9ba55_b.jpg"></div></figure><p data-pid="Ih_qJZTP"><b>2019 (EMNLP) Sentence-BERT - Sentence Embeddings using Siamese BERT-Networks</b></p><p data-pid="lNTXUZIv"><b>2019 (MT-DNN) Multi-Task Deep Neural Networks for Natural Language Understanding</b></p><figure data-size="normal"><noscript><img src="https://pic4.zhimg.com/v2-2ce3f752c5ad641b67af8c434cb8f51b_b.jpg" data-caption="" data-size="normal" data-rawwidth="1732" data-rawheight="1246" class="origin_image zh-lightbox-thumb" width="1732" data-original="https://pic4.zhimg.com/v2-2ce3f752c5ad641b67af8c434cb8f51b_r.jpg"/></noscript><div><img src="data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1732&#39; height=&#39;1246&#39;&gt;&lt;/svg&gt;" data-caption="" data-size="normal" data-rawwidth="1732" data-rawheight="1246" class="origin_image zh-lightbox-thumb lazy" width="1732" data-original="https://pic4.zhimg.com/v2-2ce3f752c5ad641b67af8c434cb8f51b_r.jpg" data-actualsrc="https://pic4.zhimg.com/v2-2ce3f752c5ad641b67af8c434cb8f51b_b.jpg"></div></figure><p data-pid="BchBtMzk"><b>2020 (Alibaba) (Structbert) Structbert - Incorporating language structures into pre-training for deep language understanding</b></p><p data-pid="BAPJ9qn2"><b>Arxiv 21 SimCSE - Simple Contrastive Learning of Sentence Embeddings</b></p><figure data-size="normal"><noscript><img src="https://pic2.zhimg.com/v2-7d1f30da13c96b0a4457dd7f19f84a85_b.jpg" data-caption="" data-size="normal" data-rawwidth="2060" data-rawheight="950" class="origin_image zh-lightbox-thumb" width="2060" data-original="https://pic2.zhimg.com/v2-7d1f30da13c96b0a4457dd7f19f84a85_r.jpg"/></noscript><div><img src="data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;2060&#39; height=&#39;950&#39;&gt;&lt;/svg&gt;" data-caption="" data-size="normal" data-rawwidth="2060" data-rawheight="950" class="origin_image zh-lightbox-thumb lazy" width="2060" data-original="https://pic2.zhimg.com/v2-7d1f30da13c96b0a4457dd7f19f84a85_r.jpg" data-actualsrc="https://pic2.zhimg.com/v2-7d1f30da13c96b0a4457dd7f19f84a85_b.jpg"></div></figure><h2>3.3 Graph Learning:</h2><p data-pid="UK2aDfzN">自监督学习同样可以用在图表示学习（例如GNN）中，通过预测节点属性、mask掉节点、边等方法，来提升图表示学习的泛化能力、鲁棒性。</p><p data-pid="ERHs3aCM"><b>[26] 2020 (KDD) GPT-GNN: Generative Pre-Training of Graph Neural Networks</b></p><p data-pid="1qv7QVMB">GNN对于具体任务，需要大量的标签。这篇论文里提出首先通过Self-supervised learning，学习预测图中的节点属性、边。然后可以在下游任务中通过少量label finetune。</p><figure data-size="normal"><noscript><img src="https://pic4.zhimg.com/v2-54e615c441c33df5e086cd2ac5b91e07_b.jpg" data-caption="" data-size="normal" data-rawwidth="1718" data-rawheight="784" class="origin_image zh-lightbox-thumb" width="1718" data-original="https://pic4.zhimg.com/v2-54e615c441c33df5e086cd2ac5b91e07_r.jpg"/></noscript><div><img src="data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1718&#39; height=&#39;784&#39;&gt;&lt;/svg&gt;" data-caption="" data-size="normal" data-rawwidth="1718" data-rawheight="784" class="origin_image zh-lightbox-thumb lazy" width="1718" data-original="https://pic4.zhimg.com/v2-54e615c441c33df5e086cd2ac5b91e07_r.jpg" data-actualsrc="https://pic4.zhimg.com/v2-54e615c441c33df5e086cd2ac5b91e07_b.jpg"></div></figure><p data-pid="dw11OzoZ"><b>[27] 2020 (Arxiv) Self-supervised Learning on Graphs: Deep Insights and New Direction</b></p><p data-pid="eLEEGSYB"><b>[28] 2020 (WWW) Graph Representation Learning via Graphical Mutual Information Maximization</b></p><h2>3.4 Recommender Systems:</h2><p data-pid="qVJN8eve">推荐系统中通常item都是亿级别，DNN模型中对应的embedding参数是百亿千亿级别、query/item-item交互的数据很稀疏，因此即使几百亿的训练数据，仍然显得有些不足。</p><p data-pid="_G3-BE9d">通过无监督学习，能更好地利用全网用户的行为，学习更强大的召回、排序模型。</p><p data-pid="DWUFXMuV"><b>[29] 2019 (Alibaba) (CIKM)    BERT4Rec: Sequential Recommendation with Bidirectional Encoder Representations from Transformer </b></p><p data-pid="OlJxa390"><b>[30] 2020 (Google) (Arxiv) Self-supervised Learning for Large-scale Item Recommendations</b></p><p data-pid="dTzy6P0h">实际推荐系统中query-item或者item-item交互数据稀疏，而且长尾item表示难以学好。</p><p data-pid="mjPL_E7R">Google这篇论文将SimCLR思想用来改进召回模型（Two-tower)，该工作对于召回模型优化、长尾item表示学习、纯曝光数据利用等方向具有很好的指导作用。</p><figure data-size="normal"><noscript><img src="https://pic1.zhimg.com/v2-d6ea6a308e7ba992c21ca9bb8d620fe4_b.jpg" data-caption="" data-size="normal" data-rawwidth="764" data-rawheight="426" class="origin_image zh-lightbox-thumb" width="764" data-original="https://pic1.zhimg.com/v2-d6ea6a308e7ba992c21ca9bb8d620fe4_r.jpg"/></noscript><div><img src="data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;764&#39; height=&#39;426&#39;&gt;&lt;/svg&gt;" data-caption="" data-size="normal" data-rawwidth="764" data-rawheight="426" class="origin_image zh-lightbox-thumb lazy" width="764" data-original="https://pic1.zhimg.com/v2-d6ea6a308e7ba992c21ca9bb8d620fe4_r.jpg" data-actualsrc="https://pic1.zhimg.com/v2-d6ea6a308e7ba992c21ca9bb8d620fe4_b.jpg"></div></figure><p data-pid="ft3p0f-J">提出了两种数据增强模式：Feature Mask，Feature Dropout。具体做法，类似SIMCLR，对于输入特征，分成多组特征，将一些特征mask或dropout掉。</p><p data-pid="xMymOMVl">self-supervised loss: 对于输入x，做两种不同的变换，然后经过encoder，使得它们的相似度比别的输入x'经过变换、encoder后的表示高。</p><figure data-size="normal"><noscript><img src="https://pic3.zhimg.com/v2-1b699b05bc5afce16771f7f857973056_b.jpg" data-caption="" data-size="normal" data-rawwidth="762" data-rawheight="584" class="origin_image zh-lightbox-thumb" width="762" data-original="https://pic3.zhimg.com/v2-1b699b05bc5afce16771f7f857973056_r.jpg"/></noscript><div><img src="data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;762&#39; height=&#39;584&#39;&gt;&lt;/svg&gt;" data-caption="" data-size="normal" data-rawwidth="762" data-rawheight="584" class="origin_image zh-lightbox-thumb lazy" width="762" data-original="https://pic3.zhimg.com/v2-1b699b05bc5afce16771f7f857973056_r.jpg" data-actualsrc="https://pic3.zhimg.com/v2-1b699b05bc5afce16771f7f857973056_b.jpg"></div></figure><p data-pid="jZcIXyik">Feature Mask方法如下：</p><figure data-size="normal"><noscript><img src="https://pic3.zhimg.com/v2-3a26807bdea8ca9152d1e1f8eee93432_b.jpg" data-caption="" data-size="normal" data-rawwidth="1988" data-rawheight="608" class="origin_image zh-lightbox-thumb" width="1988" data-original="https://pic3.zhimg.com/v2-3a26807bdea8ca9152d1e1f8eee93432_r.jpg"/></noscript><div><img src="data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1988&#39; height=&#39;608&#39;&gt;&lt;/svg&gt;" data-caption="" data-size="normal" data-rawwidth="1988" data-rawheight="608" class="origin_image zh-lightbox-thumb lazy" width="1988" data-original="https://pic3.zhimg.com/v2-3a26807bdea8ca9152d1e1f8eee93432_r.jpg" data-actualsrc="https://pic3.zhimg.com/v2-3a26807bdea8ca9152d1e1f8eee93432_b.jpg"></div></figure><p data-pid="I_8OHNW7">Feature Dropout方法如下：</p><figure data-size="normal"><noscript><img src="https://pic2.zhimg.com/v2-84fb65ad790872179dd385e3e9023fb1_b.jpg" data-caption="" data-size="normal" data-rawwidth="2018" data-rawheight="596" class="origin_image zh-lightbox-thumb" width="2018" data-original="https://pic2.zhimg.com/v2-84fb65ad790872179dd385e3e9023fb1_r.jpg"/></noscript><div><img src="data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;2018&#39; height=&#39;596&#39;&gt;&lt;/svg&gt;" data-caption="" data-size="normal" data-rawwidth="2018" data-rawheight="596" class="origin_image zh-lightbox-thumb lazy" width="2018" data-original="https://pic2.zhimg.com/v2-84fb65ad790872179dd385e3e9023fb1_r.jpg" data-actualsrc="https://pic2.zhimg.com/v2-84fb65ad790872179dd385e3e9023fb1_b.jpg"></div></figure><p data-pid="fEQrEZv8">然后和监督式任务相结合：</p><figure data-size="normal"><noscript><img src="https://pic1.zhimg.com/v2-e851e725cd27bbc3c0d1581c856269ec_b.jpg" data-caption="" data-size="normal" data-rawwidth="786" data-rawheight="122" class="origin_image zh-lightbox-thumb" width="786" data-original="https://pic1.zhimg.com/v2-e851e725cd27bbc3c0d1581c856269ec_r.jpg"/></noscript><div><img src="data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;786&#39; height=&#39;122&#39;&gt;&lt;/svg&gt;" data-caption="" data-size="normal" data-rawwidth="786" data-rawheight="122" class="origin_image zh-lightbox-thumb lazy" width="786" data-original="https://pic1.zhimg.com/v2-e851e725cd27bbc3c0d1581c856269ec_r.jpg" data-actualsrc="https://pic1.zhimg.com/v2-e851e725cd27bbc3c0d1581c856269ec_b.jpg"></div></figure><figure data-size="normal"><noscript><img src="https://pic3.zhimg.com/v2-c6e4383e0065e827dd08467a5bbe0e9a_b.jpg" data-caption="" data-size="normal" data-rawwidth="750" data-rawheight="152" class="origin_image zh-lightbox-thumb" width="750" data-original="https://pic3.zhimg.com/v2-c6e4383e0065e827dd08467a5bbe0e9a_r.jpg"/></noscript><div><img src="data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;750&#39; height=&#39;152&#39;&gt;&lt;/svg&gt;" data-caption="" data-size="normal" data-rawwidth="750" data-rawheight="152" class="origin_image zh-lightbox-thumb lazy" width="750" data-original="https://pic3.zhimg.com/v2-c6e4383e0065e827dd08467a5bbe0e9a_r.jpg" data-actualsrc="https://pic3.zhimg.com/v2-c6e4383e0065e827dd08467a5bbe0e9a_b.jpg"></div></figure><figure data-size="normal"><noscript><img src="https://pic3.zhimg.com/v2-52f992604033964778e982cd882411ea_b.jpg" data-caption="" data-size="normal" data-rawwidth="336" data-rawheight="104" class="content_image" width="336"/></noscript><div><img src="data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;336&#39; height=&#39;104&#39;&gt;&lt;/svg&gt;" data-caption="" data-size="normal" data-rawwidth="336" data-rawheight="104" class="content_image lazy" width="336" data-actualsrc="https://pic3.zhimg.com/v2-52f992604033964778e982cd882411ea_b.jpg"></div></figure><p data-pid="x8zvNtQb"><b>[31] 2020 (Alibaba) (KDD) Disentangled Self-Supervision in Sequential Recommenders</b></p><p data-pid="Ud8sq4Gc">推荐系统一般采取seq2item这种方式来训练，存在对长尾item不能很好学习、短视的缺点。这篇论文里，作者提出seq2seq的思路，根据历史行为序列预测未来的行为序列，基于自监督学习辅助监督学习任务，在candidate generation任务上取得比较好的效果。</p><figure data-size="normal"><noscript><img src="https://pic3.zhimg.com/v2-116f4c2318bf6f4a5af43a99dc3421f6_b.jpg" data-caption="" data-size="normal" data-rawwidth="1842" data-rawheight="770" class="origin_image zh-lightbox-thumb" width="1842" data-original="https://pic3.zhimg.com/v2-116f4c2318bf6f4a5af43a99dc3421f6_r.jpg"/></noscript><div><img src="data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1842&#39; height=&#39;770&#39;&gt;&lt;/svg&gt;" data-caption="" data-size="normal" data-rawwidth="1842" data-rawheight="770" class="origin_image zh-lightbox-thumb lazy" width="1842" data-original="https://pic3.zhimg.com/v2-116f4c2318bf6f4a5af43a99dc3421f6_r.jpg" data-actualsrc="https://pic3.zhimg.com/v2-116f4c2318bf6f4a5af43a99dc3421f6_b.jpg"></div></figure><p data-pid="6FO-wEG9"><b>[32] 2020 (CIKM) [S3Rec] S3-Rec: Self-Supervised Learning for Sequential Recommendation with Mutual Information Maximization</b></p><p data-pid="bIm_hXSv">这篇论文基于Mutual Information Maximization的自监督任务，来辅助推荐模型训练。</p><p data-pid="9KA3qycA">作者提出了四个自监督辅助任务：基于item-属性、基于序列-item、序列-item属性、序列-小序列，分别计算MIM的InfoNCE loss:</p><p data-pid="11w1Ss3M">例如对于item-属性辅助任务，loss函数为：</p><figure data-size="normal"><noscript><img src="https://pic3.zhimg.com/v2-96cf31e68115bc338a29d69ed5689f92_b.jpg" data-caption="" data-size="normal" data-rawwidth="922" data-rawheight="104" class="origin_image zh-lightbox-thumb" width="922" data-original="https://pic3.zhimg.com/v2-96cf31e68115bc338a29d69ed5689f92_r.jpg"/></noscript><div><img src="data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;922&#39; height=&#39;104&#39;&gt;&lt;/svg&gt;" data-caption="" data-size="normal" data-rawwidth="922" data-rawheight="104" class="origin_image zh-lightbox-thumb lazy" width="922" data-original="https://pic3.zhimg.com/v2-96cf31e68115bc338a29d69ed5689f92_r.jpg" data-actualsrc="https://pic3.zhimg.com/v2-96cf31e68115bc338a29d69ed5689f92_b.jpg"></div></figure><figure data-size="normal"><noscript><img src="https://pic2.zhimg.com/v2-bbc9ec87959f502cb589d53b671096bd_b.jpg" data-caption="" data-size="normal" data-rawwidth="468" data-rawheight="84" class="origin_image zh-lightbox-thumb" width="468" data-original="https://pic2.zhimg.com/v2-bbc9ec87959f502cb589d53b671096bd_r.jpg"/></noscript><div><img src="data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;468&#39; height=&#39;84&#39;&gt;&lt;/svg&gt;" data-caption="" data-size="normal" data-rawwidth="468" data-rawheight="84" class="origin_image zh-lightbox-thumb lazy" width="468" data-original="https://pic2.zhimg.com/v2-bbc9ec87959f502cb589d53b671096bd_r.jpg" data-actualsrc="https://pic2.zhimg.com/v2-bbc9ec87959f502cb589d53b671096bd_b.jpg"></div></figure><figure data-size="normal"><noscript><img src="https://pic3.zhimg.com/v2-8f394b57214959086563116ff56affb2_b.jpg" data-caption="" data-size="normal" data-rawwidth="2548" data-rawheight="910" class="origin_image zh-lightbox-thumb" width="2548" data-original="https://pic3.zhimg.com/v2-8f394b57214959086563116ff56affb2_r.jpg"/></noscript><div><img src="data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;2548&#39; height=&#39;910&#39;&gt;&lt;/svg&gt;" data-caption="" data-size="normal" data-rawwidth="2548" data-rawheight="910" class="origin_image zh-lightbox-thumb lazy" width="2548" data-original="https://pic3.zhimg.com/v2-8f394b57214959086563116ff56affb2_r.jpg" data-actualsrc="https://pic3.zhimg.com/v2-8f394b57214959086563116ff56affb2_b.jpg"></div></figure><p data-pid="dea_Cbat">自监督训练后，再基于监督训练来finetune推荐模型。</p><p data-pid="gohSy_zU"><b>[33] 2020 (Arxiv) Self-supervised Graph Learning for Recommendation</b></p><p data-pid="AVLE7AZg">这篇论文里，作者提出在GNN中使用自监督辅助任务来改进模型学习。</p><p data-pid="k3HFYDfL">提出了四种方法：embedding masking, embedding dropout, node dropout, edge dropout。</p><p data-pid="x0bKPWFL"><b>[34] 2020 (SIGIR) Self-Supervised Reinforcement Learning for Recommender Systems</b></p><p data-pid="akugs4DT">这篇论文里，作者将自监督学习和RL结合，用于推荐。</p><p data-pid="abA4kQBJ"><b>2020 (Alibaba) (Arxiv) Contrastive Learning for Debiased Candidate Generation in Large-Scale Recommender Systems</b></p><p data-pid="WiFsd6W7"><b>WSDM 2021 PROP - Pre-training with Representative Words Prediction for Ad-hoc Retrieval</b> </p><p data-pid="G6BQSriF"><b>2020 (Arxiv) USERBERT - SELF-SUPERVISED USER REPRESENTATION LEARNING</b></p><p data-pid="Yo5OG-v7"><b>2019 (Alibaba) (AAAI) Deep interest evolution network for click-through rate prediction</b></p><p data-pid="j8QoITnr"><b>2020 (Alibaba) (AAAI) Deep Match to Rank Model for Personalized Click-Through Rate Prediction</b></p><p data-pid="dzSbIS1F"><b>2020 (EMNLP) PTUM - Pre-training User Model from Unlabeled User Behaviors via Self-supervision</b></p><h2>3.5 Machine Learning Perspective: </h2><p data-pid="3X4Pxoco">Self-supervised learning与许多机器学习方法有着丰富的联系，例如与Semi-supervised learning、Pre-training、Self-training，都是希望利用更多的有标签或无标签数据，来提升某个任务下的性能。</p><p data-pid="qzS3cP0f">具体关注的重点在于：</p><p data-pid="l6BFhcsR">Semi-supervised learning: 同时利用无标签和有标签数据进行学习。</p><p data-pid="U9TvSayp">Self-supervised learning: 使用无监督数据，通过定义pretext task来学习。</p><p data-pid="nDbHQn8e">Pre-training: 预训练。一般先在大量数据上学习，再在下游任务finetune。</p><p data-pid="1N9KXfaQ">Self-training: 自训练。用学习的模型，在无标记数据上标注生成伪标签，然后借助这些伪标签数据，改进监督模型效果。</p><p data-pid="5XaEmLkN">下面我们介绍一些与自监督学习关系比较密切的机器学习方法相关的论文。</p><ul><li data-pid="Kk7YmIXM"><b>Self-supervised Learning:</b></li></ul><p data-pid="rUxRfSSE"><b>[35] 2020 (ICLR) A Mutual Information Maximization Perspective of Language Representation Learning</b></p><ul><li data-pid="M9Iy9mYj"><b>Task Related Self-Supervised Learning:</b></li></ul><p data-pid="lr7qbQ4J"><b>[36] 2019 (ICCV) Boosting Few-Shot Visual Learning with Self-Supervision</b></p><p data-pid="1MtgPiu7"><b>[37] 2019 (Arxiv) Rethinking Data Augmentation: Self-Supervision and Self-Distillation</b></p><ul><li data-pid="hAeEB7J_"><b>Self-supervised and Semi-supervised Learning:</b></li></ul><p data-pid="ZLSJ9YEp"><b>[38] 2013 (ICML Workshop) Pseudo-Label - The Simple and Efficient Semi-Supervised Learning Method for Deep Neural Networks</b></p><p data-pid="MZrO1Tb7">使用模型在无标签数据生成伪label，选取最大的预测结果作为分类结果。</p><figure data-size="normal"><noscript><img src="https://pic1.zhimg.com/v2-2c988e417d852cef0379f96296acf8f0_b.jpg" data-caption="" data-size="normal" data-rawwidth="782" data-rawheight="288" class="origin_image zh-lightbox-thumb" width="782" data-original="https://pic1.zhimg.com/v2-2c988e417d852cef0379f96296acf8f0_r.jpg"/></noscript><div><img src="data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;782&#39; height=&#39;288&#39;&gt;&lt;/svg&gt;" data-caption="" data-size="normal" data-rawwidth="782" data-rawheight="288" class="origin_image zh-lightbox-thumb lazy" width="782" data-original="https://pic1.zhimg.com/v2-2c988e417d852cef0379f96296acf8f0_r.jpg" data-actualsrc="https://pic1.zhimg.com/v2-2c988e417d852cef0379f96296acf8f0_b.jpg"></div></figure><p data-pid="SlXrzAZI">然后基于有标签和伪标签数据联合训练：</p><figure data-size="normal"><noscript><img src="https://pic1.zhimg.com/v2-647441df5cb7ecd606b19689e2c27660_b.jpg" data-caption="" data-size="normal" data-rawwidth="766" data-rawheight="116" class="origin_image zh-lightbox-thumb" width="766" data-original="https://pic1.zhimg.com/v2-647441df5cb7ecd606b19689e2c27660_r.jpg"/></noscript><div><img src="data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;766&#39; height=&#39;116&#39;&gt;&lt;/svg&gt;" data-caption="" data-size="normal" data-rawwidth="766" data-rawheight="116" class="origin_image zh-lightbox-thumb lazy" width="766" data-original="https://pic1.zhimg.com/v2-647441df5cb7ecd606b19689e2c27660_r.jpg" data-actualsrc="https://pic1.zhimg.com/v2-647441df5cb7ecd606b19689e2c27660_b.jpg"></div></figure><p data-pid="Yw60cQVB">逐渐增大伪标签的权重：</p><figure data-size="normal"><noscript><img src="https://pic1.zhimg.com/v2-a13d7f683dd6d75e8ef181fd462b28c0_b.jpg" data-caption="" data-size="normal" data-rawwidth="768" data-rawheight="176" class="origin_image zh-lightbox-thumb" width="768" data-original="https://pic1.zhimg.com/v2-a13d7f683dd6d75e8ef181fd462b28c0_r.jpg"/></noscript><div><img src="data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;768&#39; height=&#39;176&#39;&gt;&lt;/svg&gt;" data-caption="" data-size="normal" data-rawwidth="768" data-rawheight="176" class="origin_image zh-lightbox-thumb lazy" width="768" data-original="https://pic1.zhimg.com/v2-a13d7f683dd6d75e8ef181fd462b28c0_r.jpg" data-actualsrc="https://pic1.zhimg.com/v2-a13d7f683dd6d75e8ef181fd462b28c0_b.jpg"></div></figure><p data-pid="0kvsrQ5s"><b>[39] 2019 (Google) (ICCV) S​4L: Self-Supervised Semi-Supervised Learning</b></p><p data-pid="FdVlPMcZ">self-supervised learning: 同时利用无标签和有标签数据进行学习。</p><p data-pid="h4RIiW9d">Self-supervised learning: 使用无监督数据，通过定义pretext task来学习。</p><p data-pid="_lPy7jt6">Google这篇论文里，指出self-supervised有助于semi-supervised learning学习。</p><figure data-size="normal"><noscript><img src="https://pic2.zhimg.com/v2-2bb2655406d589c7dfd7b3ac014f740d_b.jpg" data-caption="" data-size="normal" data-rawwidth="1354" data-rawheight="1014" class="origin_image zh-lightbox-thumb" width="1354" data-original="https://pic2.zhimg.com/v2-2bb2655406d589c7dfd7b3ac014f740d_r.jpg"/></noscript><div><img src="data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1354&#39; height=&#39;1014&#39;&gt;&lt;/svg&gt;" data-caption="" data-size="normal" data-rawwidth="1354" data-rawheight="1014" class="origin_image zh-lightbox-thumb lazy" width="1354" data-original="https://pic2.zhimg.com/v2-2bb2655406d589c7dfd7b3ac014f740d_r.jpg" data-actualsrc="https://pic2.zhimg.com/v2-2bb2655406d589c7dfd7b3ac014f740d_b.jpg"></div></figure><ul><li data-pid="9Hq-otCG"><b>Semi-supervised :</b></li></ul><p data-pid="LWtl5QRU"><b>[40] 2019 (Facebook) (Arxiv) Billion-scale semi-supervised learning for image classification</b></p><p data-pid="5zMJXQbv">Facebook在这篇论文里，介绍如何利用有标签数据和无标签数据，共同学习，一个图像分类模型。首先基于有标签数据训练teacher模型，然后在无标签数据打分后，选出每个类别top K构成新的有标签数据，用于训练student model, 然后再用有标签数据对student model finetune。</p><figure data-size="normal"><noscript><img src="https://pic3.zhimg.com/v2-f8977b98b5b3cb0307721d04f70e8fea_b.jpg" data-caption="" data-size="normal" data-rawwidth="1376" data-rawheight="1316" class="origin_image zh-lightbox-thumb" width="1376" data-original="https://pic3.zhimg.com/v2-f8977b98b5b3cb0307721d04f70e8fea_r.jpg"/></noscript><div><img src="data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1376&#39; height=&#39;1316&#39;&gt;&lt;/svg&gt;" data-caption="" data-size="normal" data-rawwidth="1376" data-rawheight="1316" class="origin_image zh-lightbox-thumb lazy" width="1376" data-original="https://pic3.zhimg.com/v2-f8977b98b5b3cb0307721d04f70e8fea_r.jpg" data-actualsrc="https://pic3.zhimg.com/v2-f8977b98b5b3cb0307721d04f70e8fea_b.jpg"></div></figure><ul><li data-pid="XIhrXg5Q"><b>Self training:</b></li></ul><p data-pid="802Wuhvi"><b>[41] 2020 (Google) (NIPS) NIPS 2020 Rethinking the Value of Labels for Improving Class-Imbalanced Learning</b></p><p data-pid="qidepb4I"><b>[42] 2020 (Google) (CVPR) Self-Training With Noisy Student Improves ImageNet Classification</b></p><p data-pid="vv115S7D">这篇论文里，作者先用label数据训练teacher模型，然后在无标签数据上生成伪label，基于标签数据和伪label，训练noisy student模型，再把这个noisy student当作teacher模型，继续生成伪label，注意teacher model不添加noise，这样伪label尽可能准确。多次迭代，使得noisy的student模型与更强大的无noise的teacher模型一致，从而performance越来越好。</p><p data-pid="b_Y8DVOl">noise: (1) 输入: 数据增强   （2）模型：dropout，stochastic depth。</p><p data-pid="i2V1PgFy">trick: （1）data filtering：过滤掉teacher模型置信度低的  （2） balancing: 每个类别，无标签数据规模接近。 </p><p class="ztext-empty-paragraph"><br></p><figure data-size="normal"><noscript><img src="https://pic1.zhimg.com/v2-9b543fe71df7d0ca42c4d9f632b3b1e0_b.jpg" data-caption="" data-size="normal" data-rawwidth="1252" data-rawheight="770" class="origin_image zh-lightbox-thumb" width="1252" data-original="https://pic1.zhimg.com/v2-9b543fe71df7d0ca42c4d9f632b3b1e0_r.jpg"/></noscript><div><img src="data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1252&#39; height=&#39;770&#39;&gt;&lt;/svg&gt;" data-caption="" data-size="normal" data-rawwidth="1252" data-rawheight="770" class="origin_image zh-lightbox-thumb lazy" width="1252" data-original="https://pic1.zhimg.com/v2-9b543fe71df7d0ca42c4d9f632b3b1e0_r.jpg" data-actualsrc="https://pic1.zhimg.com/v2-9b543fe71df7d0ca42c4d9f632b3b1e0_b.jpg"></div></figure><p data-pid="SWMsYWVZ"><b>[43] 2020 (Google) (NIPS) Rethinking Pre-training and Self-training</b></p><ul><li data-pid="YQU5TVrV"><b>Multi-task Self-supervised Learning:</b></li></ul><p data-pid="pabgpNhS"><b>[44] 2017 (ICCV) Multi-task Self-Supervised Visual Learning</b></p><p data-pid="1p2MWDeL">多个任务联合self-supervised learning:</p><figure data-size="normal"><noscript><img src="https://pic4.zhimg.com/v2-c05ae8983334c39440910063959c7c83_b.jpg" data-caption="" data-size="normal" data-rawwidth="1840" data-rawheight="684" class="origin_image zh-lightbox-thumb" width="1840" data-original="https://pic4.zhimg.com/v2-c05ae8983334c39440910063959c7c83_r.jpg"/></noscript><div><img src="data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1840&#39; height=&#39;684&#39;&gt;&lt;/svg&gt;" data-caption="" data-size="normal" data-rawwidth="1840" data-rawheight="684" class="origin_image zh-lightbox-thumb lazy" width="1840" data-original="https://pic4.zhimg.com/v2-c05ae8983334c39440910063959c7c83_r.jpg" data-actualsrc="https://pic4.zhimg.com/v2-c05ae8983334c39440910063959c7c83_b.jpg"></div></figure><p data-pid="ntlJ5dwD"><b>[45] 2018 (CVPR) Cross-domain Self-supervised Multi-task Feature Learning Using Synthetic Imagery</b></p><p data-pid="cwTHTduQ">利用合成图片多个任务联合self-supervised learning，并使用adversarial learning接近实际图片。</p><figure data-size="normal"><noscript><img src="https://pic4.zhimg.com/v2-bee06eba8fbfced8d483062e246fb223_b.jpg" data-caption="" data-size="normal" data-rawwidth="1252" data-rawheight="696" class="origin_image zh-lightbox-thumb" width="1252" data-original="https://pic4.zhimg.com/v2-bee06eba8fbfced8d483062e246fb223_r.jpg"/></noscript><div><img src="data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1252&#39; height=&#39;696&#39;&gt;&lt;/svg&gt;" data-caption="" data-size="normal" data-rawwidth="1252" data-rawheight="696" class="origin_image zh-lightbox-thumb lazy" width="1252" data-original="https://pic4.zhimg.com/v2-bee06eba8fbfced8d483062e246fb223_r.jpg" data-actualsrc="https://pic4.zhimg.com/v2-bee06eba8fbfced8d483062e246fb223_b.jpg"></div></figure><figure data-size="normal"><noscript><img src="https://pic2.zhimg.com/v2-d71e46b6ed8b238a1e4091d610401a51_b.jpg" data-caption="" data-size="normal" data-rawwidth="1338" data-rawheight="670" class="origin_image zh-lightbox-thumb" width="1338" data-original="https://pic2.zhimg.com/v2-d71e46b6ed8b238a1e4091d610401a51_r.jpg"/></noscript><div><img src="data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1338&#39; height=&#39;670&#39;&gt;&lt;/svg&gt;" data-caption="" data-size="normal" data-rawwidth="1338" data-rawheight="670" class="origin_image zh-lightbox-thumb lazy" width="1338" data-original="https://pic2.zhimg.com/v2-d71e46b6ed8b238a1e4091d610401a51_r.jpg" data-actualsrc="https://pic2.zhimg.com/v2-d71e46b6ed8b238a1e4091d610401a51_b.jpg"></div></figure><p data-pid="9IrEDc2p"><b>Reinforcement Learning:</b></p><p data-pid="1G6HdGKB"><b>[46] 2019 (NIPS) Unsupervised State Representation Learning in Atari</b></p><p data-pid="f_oAJfEd">Bengio等人在这篇论文里，将Self-supervised learning和Reinforcement learning结合，从输入学习更好的状态编码器，改进泛化性能，从而在reward数据有限时，也能改进RL效果。</p><h2><br>3.6 Survey:</h2><p data-pid="YaWR96s7">自监督学习的综述论文:</p><p data-pid="fNjiK4qQ"><b>[47] 2020 (Arxiv) Self-supervised Learning: Generative or Contrastive.</b></p><p data-pid="eDEYckRx"><b>[48] 2020 (T-PAMI) Self-supervised Visual Feature Learning with Deep Neural Networks: A Survey</b></p><h2>3.7 More Materials：</h2><p data-pid="S7vw46Ni">Github: </p><p data-pid="TmBvUBYa">Facebook: <a href="https://link.zhihu.com/?target=https%3A//github.com/facebookresearch/fair_self_supervision_benchmark" class=" external" target="_blank" rel="nofollow noreferrer"><span class="invisible">https://</span><span class="visible">github.com/facebookrese</span><span class="invisible">arch/fair_self_supervision_benchmark</span><span class="ellipsis"></span></a><br><br>blogs:</p><p data-pid="JKmvUz1D"><a href="https://link.zhihu.com/?target=https%3A//lilianweng.github.io/lil-log/2019/11/10/self-supervised-learning.html" class=" external" target="_blank" rel="nofollow noreferrer"><span class="invisible">https://</span><span class="visible">lilianweng.github.io/li</span><span class="invisible">l-log/2019/11/10/self-supervised-learning.html</span><span class="ellipsis"></span></a></p><p data-pid="T75EkvFh">Talks:</p><p data-pid="ZbZIEZXi">Self-Supervised Learning. Yan Lecun. AAAI 2020. <a href="https://link.zhihu.com/?target=https%3A//drive.google.com/file/d/1r-mDL4IX_hzZLDBKp8_e8VZqD7fOzBkF/view%3Fusp%3Dsharing" class=" wrap external" target="_blank" rel="nofollow noreferrer">[pdf]</a></p><p data-pid="am_c9_3F">Graph Embeddings, Content Understanding, &amp; Self-Supervised Learning. Yann LeCun. (NYU &amp; FAIR) <a href="https://link.zhihu.com/?target=https%3A//drive.google.com/file/d/12pDCno02FJPDEBk4iGuuaj8b2rr48Hh0/view" class=" wrap external" target="_blank" rel="nofollow noreferrer">[pdf]</a><a href="https://link.zhihu.com/?target=https%3A//www.youtube.com/watch%3Fv%3DUGPT64wo7lU" class=" wrap external" target="_blank" rel="nofollow noreferrer">[video]</a></p><p data-pid="a-xeNWAl">Self-supervised learning: could machines learn like humans? Yann LeCun @EPFL. <a href="https://link.zhihu.com/?target=https%3A//www.youtube.com/watch%3Fv%3D7I0Qt7GALVk" class=" wrap external" target="_blank" rel="nofollow noreferrer">[video]</a></p><hr><h2>后记：</h2><p data-pid="GGc3HhlA">Self-supervised Learning在CV、NLP、Graph、RL、RecSys等领域已经取得了很awesome的效果。如何更好的挖掘无标签数据中的知识？如何和有监督数据更好地结合学习？仍然都是开放的问题，通过已有的研究工作，我们有充足的理由相信: </p><p data-pid="ndYxEwLC">Self-supervised Learning is the future of AI！</p><h2>本文介绍的论文集合：</h2><p data-pid="0cEFgTiz"><a href="https://link.zhihu.com/?target=https%3A//github.com/guyulongcs/Awesome-Self-supervised-Learning-papers" class=" external" target="_blank" rel="nofollow noreferrer"><span class="invisible">https://</span><span class="visible">github.com/guyulongcs/A</span><span class="invisible">wesome-Self-supervised-Learning-papers</span><span class="ellipsis"></span></a></p><hr><h2>关于搜索推荐广告排序艺术：</h2><p data-pid="DSflUMrk">搜索推荐广告排序艺术，聚焦人工智能、深度学习在互联网搜索、推荐、广告中的应用，这里有最前沿的学术论文、最新的互联网公司技术分享，欢迎对互联网、搜索、推荐、广告、排序算法、深度学习、AI感兴趣的朋友们，关注微信公众号/知乎专栏：搜索推荐广告排序艺术。</p><p data-pid="BIHfgqu5">微信公众号：搜索推荐广告排序艺术</p><p data-pid="uAykUPFg">知乎专栏：</p><div class="RichText-LinkCardContainer"><a target="_blank" href="https://www.zhihu.com/column/c_1288235772122718208" data-draft-node="block" data-draft-type="link-card" data-text="搜索推荐广告排序艺术" class="LinkCard new css-1wr1m8" data-image="https://pic2.zhimg.com/v2-b6546a28845e426a25b971ce7fcab919_ipico.jpg" data-image-width="200" data-image-height="200"><span class="LinkCard-contents"><span class="LinkCard-title loading" data-text="true"></span><span class="LinkCard-desc loading"></span></span><span class="LinkCard-image LinkCard-image--default"></span></a></div><p></p></div></div></div></div><div role="button" tabindex="0" class="ContentItem-time">编辑于 2021-05-11 10:49</div><span></span><div class="Reward"><div><div class="Reward-tagline">「真诚赞赏，手留余香」</div><button class="Reward-rewardBtn">赞赏</button></div><div class="Reward-countZero">还没有人赞赏，快来当第一个赞赏的人吧！</div></div><div class="Post-topicsAndReviewer"><div class="TopicList Post-Topics"><div class="Tag Topic css-1s3a4zw" data-za-detail-view-path-module="TopicItem" data-za-extra-module="{&quot;card&quot;:{&quot;content&quot;:{&quot;type&quot;:&quot;Topic&quot;,&quot;token&quot;:&quot;19813032&quot;}}}"><span class="Tag-content"><a class="TopicLink" href="https://www.zhihu.com/topic/19813032" target="_blank"><div class="css-1gomreu">深度学习（Deep Learning）</div></a></span></div><div class="Tag Topic css-1s3a4zw" data-za-detail-view-path-module="TopicItem" data-za-extra-module="{&quot;card&quot;:{&quot;content&quot;:{&quot;type&quot;:&quot;Topic&quot;,&quot;token&quot;:&quot;20088404&quot;}}}"><span class="Tag-content"><a class="TopicLink" href="https://www.zhihu.com/topic/20088404" target="_blank"><div class="css-1gomreu">半监督学习</div></a></span></div><div class="Tag Topic css-1s3a4zw" data-za-detail-view-path-module="TopicItem" data-za-extra-module="{&quot;card&quot;:{&quot;content&quot;:{&quot;type&quot;:&quot;Topic&quot;,&quot;token&quot;:&quot;19590194&quot;}}}"><span class="Tag-content"><a class="TopicLink" href="https://www.zhihu.com/topic/19590194" target="_blank"><div class="css-1gomreu">无监督学习</div></a></span></div></div></div><div><div class="Sticky RichContent-actions is-fixed is-bottom" style="width: 690px; bottom: 0px; left: 397.4px;"><div class="ContentItem-actions" data-za-detail-view-path-module="BottomBar" data-za-extra-module="{&quot;card&quot;:{&quot;content&quot;:{&quot;type&quot;:&quot;Post&quot;,&quot;id&quot;:&quot;270547809&quot;}}}"><span><button aria-label="赞同 58 " aria-live="polite" type="button" class="Button VoteButton VoteButton--up"><span style="display:inline-flex;align-items:center">​<svg width="10" height="10" viewBox="0 0 24 24" class="Zi Zi--TriangleUp VoteButton-TriangleUp" fill="currentColor"><path fill-rule="evenodd" d="M13.792 3.681c-.781-1.406-2.803-1.406-3.584 0l-7.79 14.023c-.76 1.367.228 3.046 1.791 3.046h15.582c1.563 0 2.55-1.68 1.791-3.046l-7.79-14.023Z" clip-rule="evenodd"></path></svg></span>赞同 58</button><button aria-label="反对" aria-live="polite" type="button" class="Button VoteButton VoteButton--down"><span style="display:inline-flex;align-items:center">​<svg width="10" height="10" viewBox="0 0 24 24" class="Zi Zi--TriangleDown" fill="currentColor"><path fill-rule="evenodd" d="M13.792 20.319c-.781 1.406-2.803 1.406-3.584 0L2.418 6.296c-.76-1.367.228-3.046 1.791-3.046h15.582c1.563 0 2.55 1.68 1.791 3.046l-7.79 14.023Z" clip-rule="evenodd"></path></svg></span></button></span><button type="button" class="Button BottomActions-CommentBtn Button--plain Button--withIcon Button--withLabel"><span style="display:inline-flex;align-items:center">​<svg width="1.2em" height="1.2em" viewBox="0 0 24 24" class="Zi Zi--Comment Button-zi" fill="currentColor"><path fill-rule="evenodd" d="M12 2.75a9.25 9.25 0 1 0 4.737 17.197l2.643.817a1 1 0 0 0 1.25-1.25l-.8-2.588A9.25 9.25 0 0 0 12 2.75Z" clip-rule="evenodd"></path></svg></span>添加评论</button><div class="Popover ShareMenu"><div class="ShareMenu-toggler" id="Popover2-toggle" aria-haspopup="true" aria-expanded="false" aria-owns="Popover2-content"><button type="button" class="Button Button--plain Button--withIcon Button--withLabel"><span style="display:inline-flex;align-items:center">​<svg width="1.2em" height="1.2em" viewBox="0 0 24 24" class="Zi Zi--Share Button-zi" fill="currentColor"><path d="M19.47 1.914a.8.8 0 0 1 1.204.778l-1.872 16.386a.9.9 0 0 1-1.204.743l-4.615-1.692a.7.7 0 0 0-.831.28l-1.927 3.02c-.43.674-1.474.369-1.474-.43v-3.865a.8.8 0 0 1 .179-.504l5.808-7.148a.595.595 0 0 0-.897-.781l-5.93 6.354a1.1 1.1 0 0 1-1.258.252L2.57 13.46a.8.8 0 0 1-.08-1.415l16.98-10.13Z"></path></svg></span>分享</button></div></div><button aria-live="polite" type="button" class="Button ContentItem-action Button--plain Button--withIcon Button--withLabel"><span style="display:inline-flex;align-items:center">​<svg width="1.2em" height="1.2em" viewBox="0 0 24 24" class="Zi Zi--Heart Button-zi" fill="currentColor"><path fill-rule="evenodd" d="M12.004 4.934c1.015-.944 2.484-1.618 3.98-1.618 3.48 0 6.53 3.265 6.15 7.614-.11 1.254-.686 2.55-1.458 3.753-.778 1.215-1.79 2.392-2.845 3.419-1.054 1.028-2.168 1.923-3.161 2.566a9.96 9.96 0 0 1-1.41.777c-.418.182-.862.32-1.268.32s-.848-.137-1.267-.317a9.918 9.918 0 0 1-1.407-.771c-.992-.64-2.103-1.53-3.156-2.555-1.052-1.024-2.062-2.2-2.84-3.417-.77-1.208-1.346-2.51-1.456-3.775-.38-4.349 2.67-7.614 6.15-7.614 1.484 0 2.983.673 3.988 1.618Z" clip-rule="evenodd"></path></svg></span>喜欢</button><button type="button" class="Button Button--plain Button--withIcon Button--withLabel"><span style="display:inline-flex;align-items:center">​<svg width="1.2em" height="1.2em" viewBox="0 0 24 24" class="Zi Zi--Star Button-zi" fill="currentColor"><path d="M10.484 3.307c.673-1.168 2.358-1.168 3.032 0l2.377 4.122a.25.25 0 0 0 .165.12l4.655.987c1.319.28 1.84 1.882.937 2.884l-3.186 3.535a.25.25 0 0 0-.063.193l.5 4.733c.142 1.34-1.222 2.33-2.453 1.782l-4.346-1.938a.25.25 0 0 0-.204 0l-4.346 1.938c-1.231.549-2.595-.442-2.453-1.782l.5-4.733a.25.25 0 0 0-.064-.193L2.35 11.42c-.903-1.002-.382-2.604.937-2.884l4.655-.987a.25.25 0 0 0 .164-.12l2.378-4.122Z"></path></svg></span>收藏</button><button type="button" class="Button ContentItem-action Button--plain Button--withIcon Button--withLabel"><span style="display:inline-flex;align-items:center">​<svg width="1.2em" height="1.2em" viewBox="0 0 24 24" class="Zi Zi--Deliver Button-zi" fill="currentColor"><g fill-rule="evenodd" clip-rule="evenodd"><path d="M7.821 12a.75.75 0 0 1 .75-.75h6.857a.75.75 0 0 1 0 1.5H8.571a.75.75 0 0 1-.75-.75ZM8.965 8a.75.75 0 0 1 .75-.75h4.571a.75.75 0 0 1 0 1.5H9.715a.75.75 0 0 1-.75-.75Z"></path><path d="M7.527 3.15a2.35 2.35 0 0 0-2.309 1.91L3.165 15.84a.85.85 0 0 0-.015.16v2.5a2.35 2.35 0 0 0 2.35 2.35h13a2.35 2.35 0 0 0 2.35-2.35V16a.848.848 0 0 0-.015-.16L18.78 5.06a2.35 2.35 0 0 0-2.308-1.91H7.527Zm0 1.7a.65.65 0 0 0-.639.528l-1.88 9.872h13.984l-1.88-9.872a.65.65 0 0 0-.64-.528H7.528Z"></path></g></svg></span>申请转载</button><div class="Post-ActionMenuButton"><div class="Popover"><div id="Popover3-toggle" aria-haspopup="true" aria-expanded="false" aria-owns="Popover3-content"><button type="button" class="Button Button--plain Button--withIcon Button--iconOnly undefined"><span style="display:inline-flex;align-items:center">​<svg width="1.2em" height="1.2em" viewBox="0 0 24 24" class="Zi Zi--Dots Button-zi" fill="currentColor"><path d="M5.34 12a1.665 1.665 0 1 1-3.33 0 1.665 1.665 0 0 1 3.33 0ZM13.665 12a1.665 1.665 0 1 1-3.33 0 1.665 1.665 0 0 1 3.33 0ZM21.99 12a1.665 1.665 0 1 1-3.33 0 1.665 1.665 0 0 1 3.33 0Z"></path></svg></span></button></div></div></div></div><div class="Post-SideActions" style="opacity: 1;" data-za-detail-view-path-module="LeftTabBar" data-za-extra-module="{&quot;card&quot;:{&quot;content&quot;:{&quot;type&quot;:&quot;Post&quot;,&quot;id&quot;:&quot;270547809&quot;}}}"><button class="like"><div class="Post-SideActions-icon"><svg width="16" height="16" viewBox="0 0 24 24" class="Zi Zi--TriangleUp Post-SideActions-upIcon" fill="currentColor"><path fill-rule="evenodd" d="M13.792 3.681c-.781-1.406-2.803-1.406-3.584 0l-7.79 14.023c-.76 1.367.228 3.046 1.791 3.046h15.582c1.563 0 2.55-1.68 1.791-3.046l-7.79-14.023Z" clip-rule="evenodd"></path></svg></div><div class="likeCount"><div class="likeCount-inner" data-previous="已赞同 59">赞同 58</div></div></button><div class="Popover ShareMenu"><div class="ShareMenu-toggler" id="Popover6-toggle" aria-haspopup="true" aria-expanded="false" aria-owns="Popover6-content"><img class="ShareMenu-fakeQRCode" src="https://www.zhihu.com/qrcode?url=https%3A%2F%2Fzhuanlan.zhihu.com%2Fp%2F270547809%23showWechatShareTip" alt="微信二维码"><button><div class="Post-SideActions-icon"><span style="display: inline-flex; align-items: center;">​<svg width="20" height="20" viewBox="0 0 24 24" class="Zi Zi--Share" fill="currentColor"><path d="M19.47 1.914a.8.8 0 0 1 1.204.778l-1.872 16.386a.9.9 0 0 1-1.204.743l-4.615-1.692a.7.7 0 0 0-.831.28l-1.927 3.02c-.43.674-1.474.369-1.474-.43v-3.865a.8.8 0 0 1 .179-.504l5.808-7.148a.595.595 0 0 0-.897-.781l-5.93 6.354a1.1 1.1 0 0 1-1.258.252L2.57 13.46a.8.8 0 0 1-.08-1.415l16.98-10.13Z"></path></svg></span></div>分享</button></div></div></div></div><div class="Sticky--holder" style="position: static; inset: auto auto 0px 0px; display: block; float: none; margin: 0px 0px 10px; height: 53.6px;"></div></div></article><div class="Post-Sub Post-NormalSub"><div style="overflow: unset;" data-za-detail-view-path-module="CommentList" data-za-extra-module="{}"><div class="Comments-container css-plbgu"><div class="css-79elbk"><div><div class="css-1fo89v5"><img class="Avatar css-1oi6kgx" src="./自监督学习_ 人工智能的未来 - 知乎_files/v2-38755e385b05dcd213fa98d8bef04c46_l.jpg" srcset="https://picx.zhimg.com/v2-38755e385b05dcd213fa98d8bef04c46_l.jpg?source=32738c0c 2x"><div class="css-x0pxoz"><div class="css-i6bazn"><div class="css-0"><div class="InputLike css-ip4bff Editable is-focus"><div class="Dropzone Editable-content RichText RichText--editable RichText--clearBoth ztext" style="min-height: 38px;"><div class="DraftEditor-root"><div class="public-DraftEditorPlaceholder-root public-DraftEditorPlaceholder-hasFocus"><div class="public-DraftEditorPlaceholder-inner" id="placeholder-3npmc" style="white-space: pre-wrap;">评论千万条，友善第一条</div></div><div class="DraftEditor-editorContainer"><div aria-describedby="placeholder-3npmc" class="notranslate public-DraftEditor-content" contenteditable="true" role="textbox" spellcheck="true" tabindex="0" style="outline: none; user-select: text; white-space: pre-wrap; overflow-wrap: break-word;"><div data-contents="true"><div class="Editable-unstyled" data-block="true" data-editor="3npmc" data-offset-key="bnddu-0-0"><div data-offset-key="bnddu-0-0" class="public-DraftStyleDefault-block public-DraftStyleDefault-ltr"><span data-offset-key="bnddu-0-0"><br data-text="true"></span></div></div></div></div></div></div></div><div></div><input multiple="" type="file" accept="image/webp,image/jpg,image/jpeg,image/png,image/gif" style="display: none;"></div></div></div></div></div></div><div class="css-z07uxh"><div class="css-vr7jd6"><svg width="1.2em" height="1.2em" viewBox="0 0 16 16" class="ZDI ZDI--Fliter16" fill="currentColor"><path fill-rule="evenodd" d="M2.06 3.078c-.577-.682-.092-1.728.802-1.728H13.14c.893 0 1.378 1.043.803 1.726l-3.418 4.061v5.092c0 .43-.261.816-.66.975l-2.95 1.18a1.05 1.05 0 0 1-1.44-.975V7.113L2.06 3.078Zm1.341-.428 3.125 3.693c.16.19.249.43.249.679v6.018l2.45-.98V7.046c0-.247.087-.487.247-.676l3.131-3.72H3.401Z" clip-rule="evenodd"></path></svg><div class="css-1tdhe7b">评论内容由作者筛选后展示</div></div><div class="css-jzppqm"><div class="css-1vjm6vs"></div><div class="css-1goiptz"><div class="css-1jroejq"><svg width="160" height="160" viewBox="0 0 160 160" fill="none" xmlns="http://www.w3.org/2000/svg"><path d="M128.05 86.7399H137.6C140.497 86.7242 143.27 85.5665 145.318 83.5183C147.367 81.47 148.524 78.6965 148.54 75.7999C148.524 72.9033 147.367 70.1298 145.318 68.0816C143.27 66.0333 140.497 64.8757 137.6 64.8599H135.11C134.109 64.3682 133.269 63.6005 132.69 62.6473C132.11 61.6941 131.815 60.5951 131.84 59.4799C131.823 58.4532 132.073 57.4397 132.566 56.5387C133.059 55.6377 133.777 54.88 134.65 54.3399H136.38C137.624 54.3808 138.863 54.1711 140.024 53.7233C141.185 53.2756 142.244 52.599 143.138 51.7337C144.032 50.8684 144.743 49.8321 145.229 48.6865C145.714 47.5408 145.965 46.3092 145.965 45.0649C145.965 43.8206 145.714 42.589 145.229 41.4434C144.743 40.2977 144.032 39.2614 143.138 38.3961C142.244 37.5308 141.185 36.8542 140.024 36.4065C138.863 35.9587 137.624 35.7491 136.38 35.7899H51.38C50.1363 35.7491 48.8972 35.9587 47.7362 36.4065C46.5753 36.8542 45.5162 37.5308 44.622 38.3961C43.7278 39.2614 43.0168 40.2977 42.5312 41.4434C42.0456 42.589 41.7954 43.8206 41.7954 45.0649C41.7954 46.3092 42.0456 47.5408 42.5312 48.6865C43.0168 49.8321 43.7278 50.8684 44.622 51.7337C45.5162 52.599 46.5753 53.2756 47.7362 53.7233C48.8972 54.1711 50.1363 54.3808 51.38 54.3399H57C57.8686 54.8851 58.583 55.6438 59.0749 56.5437C59.5668 57.4435 59.8199 58.4544 59.81 59.4799C59.8321 60.5946 59.5364 61.6926 58.9573 62.6454C58.3782 63.5982 57.5397 64.3663 56.54 64.8599H42.72C39.8234 64.8757 37.0499 66.0333 35.0016 68.0816C32.9534 70.1298 31.7957 72.9033 31.78 75.7999C31.7957 78.6965 32.9534 81.47 35.0016 83.5183C37.0499 85.5665 39.8234 86.7242 42.72 86.7399H53C55.0187 86.8334 56.9221 87.7078 58.3083 89.1785C59.6944 90.6491 60.4548 92.6009 60.4288 94.6216C60.4029 96.6424 59.5926 98.574 58.1692 100.009C56.7457 101.443 54.8205 102.268 52.8 102.31H24.26C21.509 102.394 18.899 103.547 16.9829 105.523C15.0669 107.498 13.9954 110.143 13.9954 112.895C13.9954 115.647 15.0669 118.291 16.9829 120.267C18.899 122.243 21.509 123.395 24.26 123.48H134.84C136.257 123.523 137.669 123.282 138.991 122.77C140.314 122.257 141.52 121.485 142.538 120.497C143.556 119.51 144.365 118.328 144.918 117.023C145.471 115.717 145.755 114.313 145.755 112.895C145.755 111.477 145.471 110.073 144.918 108.767C144.365 107.461 143.556 106.28 142.538 105.292C141.52 104.305 140.314 103.533 138.991 103.02C137.669 102.508 136.257 102.266 134.84 102.31H128.25C126.229 102.268 124.304 101.443 122.881 100.009C121.457 98.574 120.647 96.6424 120.621 94.6216C120.595 92.6009 121.356 90.6491 122.742 89.1785C124.128 87.7078 126.031 86.8334 128.05 86.7399Z" fill="#E6E6E6"></path><path d="M68.4899 123.75H66.1899V118.25C66.9407 118.471 67.7391 118.471 68.4899 118.25V123.75Z" fill="white"></path><path d="M68.4899 118.25H66.1899V120.01H68.4899V118.25Z" fill="#E6E6E6"></path><path d="M46.7201 65.48H43.5601V114.51H46.7201V65.48Z" fill="white"></path><path d="M149.83 124.21H10.17C10.048 124.21 9.93096 124.162 9.84469 124.075C9.75843 123.989 9.70996 123.872 9.70996 123.75C9.71251 123.629 9.7618 123.513 9.84752 123.428C9.93324 123.342 10.0488 123.293 10.17 123.29H149.83C149.951 123.293 150.067 123.342 150.152 123.428C150.238 123.513 150.287 123.629 150.29 123.75C150.29 123.81 150.278 123.87 150.255 123.926C150.232 123.982 150.198 124.033 150.155 124.075C150.113 124.118 150.062 124.152 150.006 124.175C149.95 124.198 149.89 124.21 149.83 124.21Z" fill="#333333"></path><path d="M66.7801 46.6299H58.4801C57.4859 46.6299 56.6801 47.4358 56.6801 48.4299V56.7299C56.6801 57.724 57.4859 58.5299 58.4801 58.5299H66.7801C67.7742 58.5299 68.5801 57.724 68.5801 56.7299V48.4299C68.5801 47.4358 67.7742 46.6299 66.7801 46.6299Z" fill="#F2F2F2"></path><path d="M89.89 46.6299H81.59C80.5959 46.6299 79.79 47.4358 79.79 48.4299V56.7299C79.79 57.724 80.5959 58.5299 81.59 58.5299H89.89C90.8842 58.5299 91.69 57.724 91.69 56.7299V48.4299C91.69 47.4358 90.8842 46.6299 89.89 46.6299Z" fill="#F2F2F2"></path><path d="M66.57 67.5901H58.27C57.2759 67.5901 56.47 68.396 56.47 69.3901V77.6901C56.47 78.6842 57.2759 79.4901 58.27 79.4901H66.57C67.5641 79.4901 68.37 78.6842 68.37 77.6901V69.3901C68.37 68.396 67.5641 67.5901 66.57 67.5901Z" fill="#F2F2F2"></path><path d="M89.89 67.5901H81.59C80.5959 67.5901 79.79 68.396 79.79 69.3901V77.6901C79.79 78.6842 80.5959 79.4901 81.59 79.4901H89.89C90.8842 79.4901 91.69 78.6842 91.69 77.6901V69.3901C91.69 68.396 90.8842 67.5901 89.89 67.5901Z" fill="#F2F2F2"></path><path d="M60.0499 50.25C60.6575 50.25 61.15 49.7576 61.15 49.1501C61.15 48.5425 60.6575 48.05 60.0499 48.05C59.4424 48.05 58.95 48.5425 58.95 49.1501C58.95 49.7576 59.4424 50.25 60.0499 50.25Z" fill="white"></path><path d="M64.9 56.9799C66.0266 56.9799 66.94 56.0666 66.94 54.9399C66.94 53.8132 66.0266 52.8999 64.9 52.8999C63.7733 52.8999 62.86 53.8132 62.86 54.9399C62.86 56.0666 63.7733 56.9799 64.9 56.9799Z" fill="white"></path><path d="M65.84 70.9499C66.3978 70.9499 66.85 70.4977 66.85 69.9399C66.85 69.3821 66.3978 68.9299 65.84 68.9299C65.2821 68.9299 64.83 69.3821 64.83 69.9399C64.83 70.4977 65.2821 70.9499 65.84 70.9499Z" fill="white"></path><path d="M62.26 71.46C62.4644 71.46 62.63 71.2943 62.63 71.09C62.63 70.8856 62.4644 70.72 62.26 70.72C62.0557 70.72 61.89 70.8856 61.89 71.09C61.89 71.2943 62.0557 71.46 62.26 71.46Z" fill="white"></path><path d="M82.86 52.4701C83.3129 52.4701 83.68 52.103 83.68 51.6501C83.68 51.1972 83.3129 50.8301 82.86 50.8301C82.4072 50.8301 82.04 51.1972 82.04 51.6501C82.04 52.103 82.4072 52.4701 82.86 52.4701Z" fill="white"></path><path d="M87.61 55.89C87.7978 55.89 87.95 55.7377 87.95 55.55C87.95 55.3622 87.7978 55.21 87.61 55.21C87.4222 55.21 87.27 55.3622 87.27 55.55C87.27 55.7377 87.4222 55.89 87.61 55.89Z" fill="white"></path><path d="M65.32 48.87C65.5465 48.87 65.73 48.6865 65.73 48.46C65.73 48.2336 65.5465 48.05 65.32 48.05C65.0936 48.05 64.91 48.2336 64.91 48.46C64.91 48.6865 65.0936 48.87 65.32 48.87Z" fill="white"></path><path d="M59.74 54.87C59.9664 54.87 60.15 54.6865 60.15 54.46C60.15 54.2336 59.9664 54.05 59.74 54.05C59.5135 54.05 59.33 54.2336 59.33 54.46C59.33 54.6865 59.5135 54.87 59.74 54.87Z" fill="white"></path><path d="M58.7999 49.86C58.5299 49.86 58.3099 49.86 58.2699 49.65C58.2299 49.44 58.3799 49.32 58.7599 49.13C59.137 48.9536 59.5284 48.8097 59.9299 48.7C60.2299 48.62 61.7199 48.24 61.8399 48.7C61.9599 49.14 60.4799 49.56 60.1799 49.64C59.7294 49.7638 59.2666 49.8376 58.7999 49.86ZM58.5399 49.58C59.0713 49.6149 59.6043 49.547 60.1099 49.38C60.6312 49.2767 61.1251 49.0655 61.5599 48.76C61.0287 48.7265 60.4965 48.8084 59.9999 49C59.4789 49.092 58.9821 49.2894 58.5399 49.58Z" fill="white"></path><path d="M112.25 46.6299H103.95C102.956 46.6299 102.15 47.4358 102.15 48.4299V56.7299C102.15 57.724 102.956 58.5299 103.95 58.5299H112.25C113.244 58.5299 114.05 57.724 114.05 56.7299V48.4299C114.05 47.4358 113.244 46.6299 112.25 46.6299Z" fill="#F2F2F2"></path><path d="M135.37 46.6299H127.07C126.076 46.6299 125.27 47.4358 125.27 48.4299V56.7299C125.27 57.724 126.076 58.5299 127.07 58.5299H135.37C136.364 58.5299 137.17 57.724 137.17 56.7299V48.4299C137.17 47.4358 136.364 46.6299 135.37 46.6299Z" fill="#F2F2F2"></path><path d="M116.28 82.3999H99.49C99.3688 82.3974 99.2533 82.3481 99.1676 82.2624C99.0819 82.1767 99.0326 82.0611 99.03 81.9399V65.1499C99.03 65.027 99.0781 64.9089 99.1641 64.8211C99.2501 64.7332 99.3671 64.6825 99.49 64.6799H116.28C116.341 64.6799 116.402 64.6921 116.458 64.7159C116.515 64.7396 116.566 64.7744 116.609 64.8182C116.652 64.862 116.685 64.9139 116.708 64.9708C116.73 65.0278 116.741 65.0887 116.74 65.1499V81.9399C116.74 82.0619 116.692 82.1789 116.605 82.2652C116.519 82.3515 116.402 82.3999 116.28 82.3999ZM100 81.4799H115.87V65.6099H100V81.4799Z" fill="#E8E8E8"></path><path d="M112.04 67.5901H103.74C102.746 67.5901 101.94 68.396 101.94 69.3901V77.6901C101.94 78.6842 102.746 79.4901 103.74 79.4901H112.04C113.034 79.4901 113.84 78.6842 113.84 77.6901V69.3901C113.84 68.396 113.034 67.5901 112.04 67.5901Z" fill="#F2F2F2"></path><path d="M135.37 67.5901H127.07C126.076 67.5901 125.27 68.396 125.27 69.3901V77.6901C125.27 78.6842 126.076 79.4901 127.07 79.4901H135.37C136.364 79.4901 137.17 78.6842 137.17 77.6901V69.3901C137.17 68.396 136.364 67.5901 135.37 67.5901Z" fill="#F2F2F2"></path><path d="M105.52 50.25C106.128 50.25 106.62 49.7576 106.62 49.1501C106.62 48.5425 106.128 48.05 105.52 48.05C104.913 48.05 104.42 48.5425 104.42 49.1501C104.42 49.7576 104.913 50.25 105.52 50.25Z" fill="white"></path><path d="M110.37 56.9799C111.497 56.9799 112.41 56.0666 112.41 54.9399C112.41 53.8132 111.497 52.8999 110.37 52.8999C109.243 52.8999 108.33 53.8132 108.33 54.9399C108.33 56.0666 109.243 56.9799 110.37 56.9799Z" fill="white"></path><path d="M111.31 70.9499C111.868 70.9499 112.32 70.4977 112.32 69.9399C112.32 69.3821 111.868 68.9299 111.31 68.9299C110.752 68.9299 110.3 69.3821 110.3 69.9399C110.3 70.4977 110.752 70.9499 111.31 70.9499Z" fill="white"></path><path d="M107.74 71.46C107.944 71.46 108.11 71.2943 108.11 71.09C108.11 70.8856 107.944 70.72 107.74 70.72C107.536 70.72 107.37 70.8856 107.37 71.09C107.37 71.2943 107.536 71.46 107.74 71.46Z" fill="white"></path><path d="M128.33 52.4701C128.783 52.4701 129.15 52.103 129.15 51.6501C129.15 51.1972 128.783 50.8301 128.33 50.8301C127.877 50.8301 127.51 51.1972 127.51 51.6501C127.51 52.103 127.877 52.4701 128.33 52.4701Z" fill="white"></path><path d="M133.08 55.89C133.268 55.89 133.42 55.7377 133.42 55.55C133.42 55.3622 133.268 55.21 133.08 55.21C132.892 55.21 132.74 55.3622 132.74 55.55C132.74 55.7377 132.892 55.89 133.08 55.89Z" fill="white"></path><path d="M110.79 48.87C111.016 48.87 111.2 48.6865 111.2 48.46C111.2 48.2336 111.016 48.05 110.79 48.05C110.564 48.05 110.38 48.2336 110.38 48.46C110.38 48.6865 110.564 48.87 110.79 48.87Z" fill="white"></path><path d="M105.21 54.87C105.436 54.87 105.62 54.6865 105.62 54.46C105.62 54.2336 105.436 54.05 105.21 54.05C104.984 54.05 104.8 54.2336 104.8 54.46C104.8 54.6865 104.984 54.87 105.21 54.87Z" fill="white"></path><path d="M104.27 49.86C104 49.86 103.78 49.86 103.74 49.65C103.7 49.44 103.85 49.32 104.23 49.13C104.607 48.9536 104.999 48.8097 105.4 48.7C105.7 48.62 107.19 48.24 107.31 48.7C107.31 48.86 107.2 49.03 106.82 49.22C106.444 49.397 106.053 49.5377 105.65 49.64C105.2 49.7638 104.737 49.8376 104.27 49.86ZM104.02 49.58C104.548 49.6144 105.078 49.5464 105.58 49.38C106.101 49.2767 106.595 49.0655 107.03 48.76C105.973 48.7505 104.935 49.0343 104.03 49.58H104.02Z" fill="white"></path><path d="M96.8199 84.0001C96.8199 84.0001 99.0999 75.3701 99.8199 74.0001C100.54 72.6301 131.2 75.4601 135.75 77.0801C140.3 78.7001 136.33 123.75 136.33 123.75H96.8199V84.0001Z" fill="white"></path><path d="M121.94 114.69C121.94 114.69 125.76 115.86 128.25 113.42C130.74 110.98 130.75 78.2501 135.74 77.0801C140.73 75.9101 136.62 110.39 136.32 123.75H121.94V114.69Z" fill="#F2F2F2"></path><path d="M97.9999 81.38C97.9999 81.38 101.81 80.38 101.89 87.65H111.41C111.41 87.65 117.5 82.18 119.41 82.27C121.32 82.36 121.86 91.56 121.86 91.56C121.86 91.56 124.6 92.66 124.7 99.84C124.7 99.84 121.27 112.56 120.59 115.84C119.91 119.12 105.72 107.92 105.72 107.92L96.8899 95.86L96.8199 84L97.9999 81.38Z" fill="#F2F2F2"></path><path d="M136.32 124.21H96.82C96.7592 124.211 96.6988 124.2 96.6424 124.178C96.5859 124.155 96.5347 124.121 96.4917 124.078C96.4487 124.035 96.4149 123.984 96.3922 123.928C96.3696 123.871 96.3586 123.811 96.36 123.75V84.0001C96.3551 83.9602 96.3551 83.9199 96.36 83.8801C96.45 83.5201 98.67 75.1801 99.36 73.8801C99.53 73.5601 99.74 73.1601 105.05 73.3301C107.98 73.4301 111.98 73.6901 116.33 74.0801C124.02 74.7601 133.41 75.8801 135.85 76.7401C137.21 77.2301 138.92 80.2801 138.16 101.14C137.74 112.48 136.75 123.78 136.74 123.89C136.711 123.98 136.656 124.059 136.581 124.116C136.505 124.174 136.414 124.206 136.32 124.21ZM97.32 123.29H135.9C136.08 121.14 136.9 111.09 137.28 101C137.52 94.3601 137.53 89.0001 137.28 85.0801C136.88 78.4801 135.87 77.6201 135.57 77.5101C133.57 76.8101 125.91 75.7601 116.84 74.9401C106.95 74.0501 101.03 73.9401 100.16 74.2901C99.57 75.5301 97.9 81.6301 97.27 84.0101L97.32 123.29Z" fill="#333333"></path><path d="M121.94 112.6C121.88 112.594 121.821 112.576 121.768 112.547C121.715 112.517 121.669 112.478 121.632 112.43C121.594 112.383 121.567 112.328 121.551 112.269C121.535 112.211 121.532 112.15 121.54 112.09C122.632 104.661 124.767 97.4232 127.88 90.5901C127.932 90.4787 128.026 90.3924 128.141 90.3502C128.256 90.308 128.384 90.3134 128.495 90.3651C128.606 90.4168 128.693 90.5106 128.735 90.626C128.777 90.7413 128.772 90.8687 128.72 90.9801C125.644 97.7239 123.533 104.867 122.45 112.2C122.433 112.319 122.37 112.428 122.275 112.502C122.18 112.577 122.06 112.612 121.94 112.6Z" fill="#333333"></path><path d="M46.72 115H43.56C43.4388 114.998 43.3233 114.948 43.2375 114.863C43.1518 114.777 43.1025 114.661 43.1 114.54V65.5401C43.1025 65.4189 43.1518 65.3033 43.2375 65.2176C43.3233 65.1319 43.4388 65.0826 43.56 65.0801H46.72C46.8412 65.0826 46.9567 65.1319 47.0424 65.2176C47.1281 65.3033 47.1774 65.4189 47.18 65.5401V114.54C47.1774 114.661 47.1281 114.777 47.0424 114.863C46.9567 114.948 46.8412 114.998 46.72 115ZM44.02 114.08H46.26V65.9401H44L44.02 114.08Z" fill="#333333"></path><path d="M37.25 73.38V106.61L43.56 114.51V65.48L37.25 73.38Z" fill="#DCDCDC"></path><path d="M43.56 115C43.4914 114.999 43.4237 114.983 43.3616 114.954C43.2995 114.925 43.2444 114.882 43.2 114.83L36.89 106.93C36.8279 106.85 36.7929 106.752 36.79 106.65V73.38C36.7908 73.275 36.8259 73.1731 36.89 73.09L43.2 65.19C43.2619 65.1162 43.345 65.0632 43.438 65.0381C43.531 65.0131 43.6295 65.0172 43.72 65.05C43.8087 65.0813 43.8853 65.1397 43.9391 65.2168C43.9929 65.2939 44.0212 65.386 44.02 65.48V114.48C44.02 114.602 43.9716 114.719 43.8853 114.805C43.799 114.892 43.682 114.94 43.56 114.94V115ZM37.71 106.48L43.1 113.23V66.79L37.71 73.54V106.48Z" fill="#333333"></path><path d="M37.25 82.54C37.25 82.54 24.25 89.41 24.25 123.75H37.25L32.41 116.16C32.41 116.16 30.94 100.94 37.25 96.27V82.54Z" fill="#DCDCDC"></path><path d="M37.25 124.21H24.25C24.128 124.21 24.011 124.162 23.9248 124.075C23.8385 123.989 23.79 123.872 23.79 123.75C23.79 109.84 25.88 98.75 30 90.75C32.95 85 36.07 82.75 37 82.21C37.0805 82.158 37.1742 82.1302 37.27 82.13C37.3308 82.1286 37.3913 82.1396 37.4477 82.1622C37.5041 82.1849 37.5554 82.2187 37.5984 82.2617C37.6413 82.3047 37.6752 82.3559 37.6978 82.4124C37.7204 82.4688 37.7314 82.5292 37.73 82.59V96.27C37.7308 96.3414 37.715 96.412 37.6837 96.4762C37.6525 96.5405 37.6067 96.5965 37.55 96.64C31.79 100.9 32.76 114.64 32.88 116.01L37.66 123.5C37.7052 123.57 37.7306 123.65 37.7336 123.733C37.7366 123.816 37.7171 123.899 37.6771 123.971C37.6371 124.044 37.5781 124.105 37.5065 124.147C37.4348 124.189 37.3531 124.211 37.27 124.21H37.25ZM24.72 123.29H36.41L32 116.41C31.9634 116.345 31.9397 116.274 31.93 116.2C31.7143 113.443 31.7344 110.673 31.99 107.92C32.55 102.1 34.16 98.11 36.77 96.05V83.41C34.2279 85.516 32.1796 88.1549 30.77 91.14C26.82 98.93 24.77 109.74 24.72 123.29Z" fill="#333333"></path><path d="M72.34 113.7C72.084 113.7 71.8322 113.635 71.6083 113.51C71.3844 113.386 71.1957 113.207 71.06 112.99C70.69 112.41 69.06 111.91 68 111.78C67.7863 111.778 67.5754 111.731 67.3811 111.642C67.1868 111.553 67.0134 111.424 66.8723 111.264C66.7312 111.103 66.6255 110.915 66.5622 110.71C66.4988 110.506 66.4793 110.291 66.5047 110.079C66.5302 109.867 66.6001 109.662 66.71 109.479C66.8198 109.296 66.967 109.137 67.1421 109.015C67.3172 108.892 67.5162 108.808 67.726 108.767C67.9359 108.727 68.1519 108.731 68.36 108.78C69.01 108.86 72.36 109.35 73.62 111.38C73.8344 111.719 73.9063 112.128 73.8202 112.52C73.734 112.911 73.4968 113.253 73.16 113.47C72.9133 113.621 72.6294 113.701 72.34 113.7Z" fill="#333333"></path><path d="M67.3399 118.41C69.4938 118.41 71.2399 116.664 71.2399 114.51C71.2399 112.356 69.4938 110.61 67.3399 110.61C65.186 110.61 63.4399 112.356 63.4399 114.51C63.4399 116.664 65.186 118.41 67.3399 118.41Z" fill="white"></path><path d="M67.3399 118.41C69.4938 118.41 71.2399 116.664 71.2399 114.51C71.2399 112.356 69.4938 110.61 67.3399 110.61C65.186 110.61 63.4399 112.356 63.4399 114.51C63.4399 116.664 65.186 118.41 67.3399 118.41Z" fill="#DCDCDC"></path><path d="M67.34 118.87C66.4752 118.87 65.6299 118.613 64.9111 118.133C64.1922 117.652 63.6321 116.969 63.3017 116.17C62.9712 115.371 62.8852 114.492 63.0547 113.644C63.2241 112.796 63.6413 112.017 64.2535 111.406C64.8656 110.796 65.6452 110.38 66.4936 110.213C67.3419 110.045 68.2209 110.133 69.0192 110.465C69.8176 110.798 70.4994 111.359 70.9783 112.079C71.4573 112.799 71.7119 113.645 71.71 114.51C71.71 115.083 71.5969 115.651 71.3771 116.181C71.1574 116.71 70.8353 117.191 70.4294 117.596C70.0235 118.001 69.5417 118.322 69.0115 118.541C68.4814 118.759 67.9134 118.871 67.34 118.87ZM67.34 111.06C66.6576 111.06 65.9906 111.262 65.4232 111.641C64.8559 112.02 64.4137 112.559 64.1526 113.19C63.8915 113.82 63.8231 114.514 63.9563 115.183C64.0894 115.852 64.418 116.467 64.9004 116.949C65.3829 117.432 65.9977 117.76 66.6669 117.894C67.3361 118.027 68.0298 117.958 68.6602 117.697C69.2906 117.436 69.8294 116.994 70.2085 116.427C70.5876 115.859 70.79 115.192 70.79 114.51C70.79 113.595 70.4265 112.717 69.7795 112.07C69.1325 111.423 68.255 111.06 67.34 111.06Z" fill="#333333"></path><path d="M68.49 124.21H66.19C66.068 124.21 65.951 124.162 65.8647 124.075C65.7784 123.989 65.73 123.872 65.73 123.75V118.25C65.7314 118.179 65.7482 118.109 65.7793 118.045C65.8103 117.981 65.8549 117.925 65.91 117.88C65.9675 117.84 66.0331 117.813 66.1022 117.801C66.1713 117.789 66.2422 117.792 66.31 117.81C66.9644 117.991 67.6555 117.991 68.31 117.81C68.3774 117.789 68.4489 117.784 68.5185 117.796C68.5882 117.808 68.6538 117.837 68.71 117.88C68.7691 117.922 68.8172 117.977 68.8503 118.042C68.8834 118.106 68.9004 118.178 68.9 118.25V123.75C68.9007 123.864 68.8591 123.974 68.7834 124.059C68.7077 124.144 68.6031 124.198 68.49 124.21ZM66.65 123.29H68V118.82C67.5426 118.89 67.0773 118.89 66.62 118.82L66.65 123.29Z" fill="#333333"></path><path d="M77.19 114.58C75.3943 114.573 73.6223 114.17 72 113.4C71.8195 113.317 71.6571 113.199 71.5222 113.052C71.3873 112.906 71.2825 112.735 71.2137 112.548C71.145 112.362 71.1137 112.163 71.1217 111.965C71.1296 111.766 71.1766 111.571 71.26 111.39C71.3434 111.209 71.4616 111.047 71.6077 110.912C71.7538 110.777 71.9251 110.672 72.1118 110.604C72.2984 110.535 72.4968 110.504 72.6955 110.512C72.8942 110.52 73.0895 110.567 73.27 110.65C74.503 111.262 75.8632 111.574 77.2396 111.56C78.6161 111.546 79.9697 111.207 81.19 110.57C81.5339 110.363 81.9455 110.301 82.3351 110.397C82.7247 110.492 83.0608 110.738 83.27 111.08C83.3748 111.247 83.4454 111.434 83.478 111.628C83.5106 111.823 83.5044 112.022 83.4598 112.215C83.4152 112.407 83.3331 112.589 83.2181 112.749C83.1032 112.91 82.9577 113.046 82.79 113.15C81.0799 114.107 79.1494 114.6 77.19 114.58Z" fill="#333333"></path><path d="M67.34 114C66.9971 113.996 66.6656 113.876 66.3995 113.66C66.1334 113.444 65.9482 113.144 65.8741 112.809C65.7999 112.474 65.8412 112.124 65.9911 111.816C66.141 111.507 66.3909 111.259 66.7 111.11C68.27 110.419 69.9988 110.17 71.7 110.39C71.9025 110.415 72.0978 110.481 72.2744 110.583C72.4509 110.686 72.6051 110.822 72.7276 110.986C72.8502 111.149 72.9386 111.335 72.9876 111.533C73.0367 111.731 73.0454 111.937 73.0131 112.139C72.9809 112.34 72.9084 112.533 72.8 112.706C72.6916 112.879 72.5495 113.028 72.3821 113.145C72.2148 113.262 72.0256 113.344 71.826 113.386C71.6263 113.428 71.4202 113.429 71.22 113.39C70.1076 113.261 68.9808 113.423 67.95 113.86C67.7592 113.951 67.5511 113.998 67.34 114Z" fill="#333333"></path><path d="M69.06 116.1C68.7907 116.096 68.5271 116.022 68.2958 115.884C68.0646 115.746 67.8739 115.549 67.743 115.314C67.6121 115.078 67.5457 114.812 67.5504 114.543C67.5551 114.274 67.6309 114.011 67.77 113.78C68.5471 112.671 69.532 111.724 70.67 110.99C71.0072 110.807 71.4013 110.757 71.7732 110.853C72.1451 110.948 72.4672 111.18 72.6749 111.503C72.8825 111.826 72.9603 112.215 72.8926 112.593C72.8249 112.971 72.6168 113.309 72.31 113.54C71.5349 114.024 70.8567 114.648 70.31 115.38C70.1762 115.593 69.9924 115.77 69.7745 115.895C69.5566 116.021 69.3113 116.091 69.06 116.1Z" fill="#333333"></path><path d="M82.4801 102.76C82.4801 102.76 81.63 115.63 81.63 123.76H121.94C121.94 123.76 122.04 102.43 121.16 98.3199C120.802 96.1368 119.867 94.0892 118.45 92.3899C118.45 92.3899 119.15 83.3299 115.85 82.9599C111.06 82.4299 104.33 90.2999 104.33 90.2999C104.33 90.2999 93.4701 90.4899 90.3301 90.7899C90.3301 90.7899 81.33 88.9299 77.6 88.4399C73.87 87.9499 72.7201 95.6699 73.8001 98.5999C74.8801 101.53 82.4801 102.76 82.4801 102.76Z" fill="white"></path><path d="M81.65 118.25H114.17V123.75H81.63L81.65 118.25Z" fill="#F2F2F2"></path><path d="M121.16 98.31C120.802 96.1269 119.867 94.0793 118.45 92.38C118.45 92.38 119.15 83.32 115.85 82.95C115.518 82.9128 115.182 82.9128 114.85 82.95C114.85 82.95 115.08 89.26 113.1 91.95C113.1 91.95 115.74 92.4701 113.98 123.73H121.91C121.91 123.73 122 102.42 121.16 98.31Z" fill="#F2F2F2"></path><path d="M86.08 103.91H86L82.41 103.22C82.3505 103.21 82.2935 103.189 82.2424 103.157C82.1914 103.125 82.1473 103.083 82.1128 103.033C82.0782 102.984 82.054 102.928 82.0415 102.869C82.029 102.81 82.0285 102.749 82.04 102.69C82.0508 102.63 82.0734 102.573 82.1065 102.522C82.1395 102.471 82.1824 102.426 82.2327 102.392C82.2829 102.358 82.3395 102.334 82.3991 102.321C82.4587 102.309 82.5202 102.308 82.58 102.32L86.18 103.01C86.2386 103.021 86.2945 103.043 86.3444 103.076C86.3944 103.108 86.4373 103.15 86.4708 103.2C86.5043 103.249 86.5277 103.304 86.5395 103.363C86.5514 103.421 86.5516 103.481 86.54 103.54C86.5189 103.646 86.4613 103.741 86.3772 103.808C86.2931 103.876 86.1879 103.912 86.08 103.91Z" fill="#333333"></path><path d="M87.37 90.18C87.37 90.18 93.52 83 96.53 83C99.54 83 98.82 90.46 98.82 90.46L90.34 90.83L87.37 90.18Z" fill="white"></path><path d="M95.46 83.19C95.46 83.19 95.36 86.91 94.08 88.6L88.64 88.76L87.37 90.18L90.37 90.78L98.85 90.41C98.85 90.41 99.83 81.6 95.46 83.19Z" fill="#F2F2F2"></path><path d="M121.94 124.21H81.63C81.5692 124.211 81.5088 124.2 81.4523 124.178C81.3959 124.155 81.3446 124.121 81.3017 124.078C81.2587 124.035 81.2248 123.984 81.2022 123.928C81.1796 123.871 81.1686 123.811 81.17 123.75C81.17 116.52 81.85 105.41 82 103.14C80.35 102.83 74.38 101.53 73.37 98.76C72.5 96.39 72.99 91.54 74.82 89.26C75.1349 88.8075 75.5684 88.4505 76.0729 88.2282C76.5775 88.0059 77.1335 87.927 77.68 88C81.2 88.47 89.53 90.17 90.36 90.34C93.36 90.07 102.61 89.89 104.12 89.86C105.12 88.79 111.21 82.55 115.85 82.52C116.112 82.5189 116.372 82.5736 116.612 82.6805C116.851 82.7874 117.065 82.944 117.24 83.14C119 85 119 91 118.92 92.21C120.295 93.9595 121.218 96.0199 121.61 98.21C122.49 102.33 122.4 122.88 122.4 123.75C122.4 123.872 122.351 123.989 122.265 124.075C122.179 124.162 122.062 124.21 121.94 124.21ZM82.09 123.29H121.48C121.48 120.17 121.48 102.09 120.71 98.41C120.36 96.3152 119.47 94.3475 118.13 92.7C118.085 92.6553 118.05 92.6023 118.026 92.544C118.002 92.4856 117.99 92.4231 117.99 92.36C118.08 90.46 117.99 85.2 116.57 83.75C116.48 83.6471 116.369 83.5638 116.245 83.5053C116.121 83.4468 115.987 83.4144 115.85 83.41C111.29 83.41 104.73 90.53 104.67 90.6C104.581 90.6873 104.464 90.7404 104.34 90.75C104.23 90.75 93.46 90.95 90.34 91.23H90.2C90.11 91.23 81.2 89.37 77.51 88.88C77.1172 88.8495 76.7236 88.9264 76.3712 89.1026C76.0189 89.2788 75.7212 89.5475 75.51 89.88C73.94 91.88 73.44 96.4 74.21 98.5C74.98 100.6 80.49 102.03 82.53 102.36C82.643 102.377 82.7456 102.436 82.8177 102.525C82.8898 102.613 82.9262 102.726 82.92 102.84C82.93 102.92 82.12 115.26 82.09 123.29Z" fill="#333333"></path><path d="M90.34 91.2401H90.24L87.24 90.6401C87.1612 90.6235 87.0878 90.5873 87.0266 90.5349C86.9655 90.4824 86.9185 90.4154 86.89 90.3401C86.8643 90.2638 86.8591 90.182 86.8749 90.1031C86.8907 90.0241 86.9269 89.9507 86.98 89.8901C87.62 89.1401 93.36 82.5301 96.48 82.5001C96.7698 82.4974 97.0567 82.5577 97.3209 82.6768C97.5851 82.796 97.8202 82.9711 98.01 83.1901C99.71 85.0401 99.23 90.2501 99.21 90.4701C99.2002 90.5807 99.1498 90.6837 99.0686 90.7594C98.9874 90.8351 98.881 90.8781 98.77 90.8801L90.28 91.2501L90.34 91.2401ZM88.24 89.8801L90.37 90.3101L98.37 89.9701C98.48 88.3401 98.46 84.9701 97.37 83.8001C97.2631 83.6745 97.1295 83.5744 96.979 83.5069C96.8285 83.4395 96.6649 83.4064 96.5 83.4101C94.44 83.4301 90.24 87.6501 88.24 89.8801Z" fill="#333333"></path><path d="M107 95.81L101 95.3C100.879 95.2874 100.767 95.2282 100.689 95.1349C100.611 95.0416 100.572 94.9216 100.58 94.8C100.584 94.74 100.6 94.6813 100.626 94.6274C100.653 94.5736 100.691 94.5257 100.736 94.4865C100.782 94.4474 100.835 94.4178 100.892 94.3995C100.95 94.3812 101.01 94.3746 101.07 94.38L107.07 94.89C107.192 94.89 107.309 94.9385 107.395 95.0248C107.482 95.111 107.53 95.228 107.53 95.35C107.53 95.472 107.482 95.589 107.395 95.6753C107.309 95.7616 107.192 95.81 107.07 95.81H107Z" fill="#333333"></path><path d="M102 94.9199C102 94.9199 101.52 97.6299 103.79 97.6299C104.093 97.6352 104.394 97.5802 104.675 97.468C104.956 97.3558 105.212 97.1887 105.428 96.9764C105.644 96.7642 105.816 96.511 105.933 96.2318C106.05 95.9525 106.11 95.6527 106.11 95.3499L102 94.9199Z" fill="#333333"></path><path d="M98 121C95.2944 121.29 92.5656 121.29 89.86 121C89.3913 120.997 88.9427 120.81 88.6113 120.479C88.2799 120.147 88.0926 119.699 88.09 119.23V110C88.0926 109.531 88.2799 109.083 88.6113 108.751C88.9427 108.42 89.3913 108.233 89.86 108.23C92.5627 107.89 95.2973 107.89 98 108.23C98.2354 108.226 98.4693 108.269 98.688 108.356C98.9068 108.443 99.1062 108.573 99.2745 108.737C99.4429 108.902 99.5769 109.098 99.6688 109.315C99.7607 109.532 99.8087 109.765 99.81 110V119.27C99.7969 119.739 99.5994 120.183 99.2606 120.507C98.9217 120.831 98.4687 121.008 98 121Z" fill="white"></path><path d="M99.3599 121C98.6858 121.251 97.9441 121.251 97.2699 121C97.0199 121 96.8199 120.3 96.8199 119.45V111.34C96.8199 110.49 97.0199 109.79 97.2699 109.79C97.5984 109.642 97.9546 109.565 98.3149 109.565C98.6753 109.565 99.0315 109.642 99.3599 109.79C99.5999 109.79 99.8099 110.49 99.8099 111.34V119.45C99.8099 120.33 99.5999 121 99.3599 121Z" fill="#DCDCDC"></path><path d="M100.2 116.19H101.2C101.311 116.187 101.416 116.142 101.494 116.064C101.572 115.986 101.617 115.88 101.62 115.77V113.62C101.62 113.509 101.576 113.402 101.497 113.323C101.418 113.244 101.311 113.2 101.2 113.2H100.14V111.14H101.14C101.799 111.14 102.43 111.401 102.897 111.866C103.364 112.33 103.627 112.961 103.63 113.62V115.77C103.627 116.429 103.364 117.059 102.897 117.524C102.43 117.989 101.799 118.25 101.14 118.25H100.08L100.2 116.19Z" fill="white"></path><path d="M101.37 111.22V113.23H100.16V111.14L101.37 111.22Z" fill="#DCDCDC"></path><path d="M101.37 116.25V118.25H100.16V116.16L101.37 116.25Z" fill="#DCDCDC"></path><path d="M94 121.68C92.6304 121.677 91.262 121.603 89.9 121.46C89.3146 121.45 88.7565 121.21 88.3453 120.794C87.9341 120.377 87.7025 119.816 87.7 119.23V110C87.6999 109.415 87.9311 108.853 88.3431 108.437C88.7551 108.022 89.3146 107.785 89.9 107.78C92.6221 107.43 95.3778 107.43 98.1 107.78C98.6853 107.785 99.2448 108.022 99.6568 108.437C100.069 108.853 100.3 109.415 100.3 110V119.27C100.297 119.856 100.066 120.417 99.6546 120.834C99.2434 121.25 98.6854 121.49 98.1 121.5C96.7373 121.63 95.3688 121.69 94 121.68ZM94 108.4C92.6627 108.399 91.3267 108.482 90 108.65H89.9C89.5542 108.65 89.2226 108.787 88.9772 109.03C88.7318 109.274 88.5926 109.604 88.59 109.95V119.22C88.59 119.568 88.728 119.901 88.9736 120.146C89.2193 120.392 89.5525 120.53 89.9 120.53C92.5589 120.817 95.241 120.817 97.9 120.53H98C98.3474 120.53 98.6806 120.392 98.9263 120.146C99.1719 119.901 99.31 119.568 99.31 119.22V110C99.3073 109.654 99.1681 109.324 98.9227 109.08C98.6774 108.837 98.3457 108.7 98 108.7C96.6743 108.516 95.3383 108.415 94 108.4Z" fill="#333333"></path><path d="M101.22 118.71H100.16C100.036 118.709 99.9172 118.658 99.8301 118.57C99.7876 118.526 99.7543 118.475 99.732 118.418C99.7097 118.361 99.6988 118.301 99.7 118.24V116.18C99.7027 116.06 99.7523 115.945 99.8383 115.861C99.9243 115.777 100.04 115.73 100.16 115.73H101.16V113.66H100.16C100.036 113.659 99.9172 113.608 99.8301 113.52C99.747 113.43 99.7007 113.312 99.7 113.19V111.13C99.7027 111.01 99.7523 110.895 99.8383 110.811C99.9243 110.727 100.04 110.68 100.16 110.68H101.16C101.945 110.68 102.7 110.988 103.26 111.537C103.821 112.087 104.144 112.835 104.16 113.62V115.77C104.145 116.545 103.83 117.284 103.282 117.832C102.734 118.38 101.995 118.695 101.22 118.71ZM100.63 117.79H101.22C101.75 117.79 102.259 117.579 102.634 117.204C103.009 116.829 103.22 116.32 103.22 115.79V113.64C103.22 113.109 103.009 112.601 102.634 112.226C102.259 111.851 101.75 111.64 101.22 111.64H100.64V112.78H101.23C101.346 112.779 101.461 112.8 101.568 112.844C101.676 112.888 101.773 112.953 101.855 113.035C101.937 113.117 102.002 113.214 102.046 113.322C102.09 113.429 102.111 113.544 102.11 113.66V115.81C102.11 116.043 102.017 116.267 101.852 116.432C101.687 116.597 101.463 116.69 101.23 116.69H100.66L100.63 117.79Z" fill="#333333"></path><path d="M90.6399 112.5C91.1646 112.5 91.5899 112.075 91.5899 111.55C91.5899 111.025 91.1646 110.6 90.6399 110.6C90.1153 110.6 89.6899 111.025 89.6899 111.55C89.6899 112.075 90.1153 112.5 90.6399 112.5Z" fill="#333333"></path><path d="M97.09 112.5C97.6147 112.5 98.04 112.075 98.04 111.55C98.04 111.025 97.6147 110.6 97.09 110.6C96.5653 110.6 96.14 111.025 96.14 111.55C96.14 112.075 96.5653 112.5 97.09 112.5Z" fill="#333333"></path><path d="M93.82 113.32C94.7976 113.32 95.59 112.528 95.59 111.55C95.59 110.572 94.7976 109.78 93.82 109.78C92.8425 109.78 92.05 110.572 92.05 111.55C92.05 112.528 92.8425 113.32 93.82 113.32Z" fill="#333333"></path><path d="M83.39 89.41C81.22 89 79.03 88.61 77.61 88.41C73.9 87.92 72.72 95.65 73.8 98.58C74.51 100.51 78.02 101.7 80.36 102.29C84.86 97.45 83.39 89.41 83.39 89.41Z" fill="#333333"></path><path d="M79 100.63L80 100.84C80.2616 100.555 80.4533 100.212 80.56 99.8401C80.3943 100.099 80.1623 100.309 79.8879 100.448C79.6135 100.587 79.3069 100.65 79 100.63Z" fill="white"></path><path d="M76 90.6199C76.4599 90.6996 76.924 90.753 77.39 90.7799L77.58 89.6799C77.2644 89.7134 76.9598 89.8147 76.6871 89.977C76.4144 90.1392 76.18 90.3586 76 90.6199Z" fill="white"></path><path d="M75.54 97.35L76.38 97.63L76.97 96.71L75.54 97.35Z" fill="white"></path><path d="M110.75 119.93C107.95 119.93 105.14 117.59 104.81 117.31C104.659 117.179 104.535 117.019 104.446 116.84C104.357 116.661 104.304 116.466 104.29 116.266C104.276 116.066 104.301 115.866 104.365 115.676C104.428 115.486 104.529 115.311 104.66 115.16C104.791 115.009 104.951 114.885 105.13 114.796C105.309 114.707 105.504 114.654 105.704 114.64C105.904 114.626 106.104 114.651 106.294 114.715C106.484 114.778 106.659 114.879 106.81 115.01C107.81 115.81 109.74 117.01 111 116.88C111.179 116.861 111.351 116.801 111.503 116.706C111.656 116.611 111.784 116.482 111.88 116.33C112.364 115.604 112.637 114.758 112.668 113.886C112.7 113.013 112.49 112.15 112.06 111.39C111.857 111.044 111.797 110.633 111.895 110.244C111.992 109.855 112.238 109.519 112.58 109.31C112.921 109.103 113.33 109.039 113.718 109.133C114.106 109.227 114.441 109.47 114.65 109.81C115.388 111.076 115.751 112.525 115.698 113.989C115.645 115.452 115.177 116.871 114.35 118.08C114.01 118.581 113.566 119.003 113.049 119.318C112.531 119.632 111.952 119.83 111.35 119.9C111.151 119.921 110.95 119.931 110.75 119.93Z" fill="#333333"></path><path d="M106.41 115.41C106.192 114.835 105.824 114.33 105.343 113.946C104.863 113.563 104.289 113.315 103.68 113.23V113.92C103.68 113.92 101.26 112.86 101.13 113.63C100.83 115.53 99.78 119.48 106.19 118.25L106.41 115.41Z" fill="#333333"></path><path d="M101.97 73.6501L103.67 77.6601L110.2 74.5801L108 70.3301L101.97 73.6501Z" fill="#F2F2F2"></path><path d="M100.51 71.57C99.2307 69.6425 97.3864 68.158 95.23 67.32C91.78 66.07 86.64 59.24 95.23 54.55L96.23 58.07L99.09 58.51L98.79 62.25L101.51 62.4701L102 66L104 69.3701L100.51 71.57Z" fill="white"></path><path d="M105.52 51.3899L104.75 54.9999L107.05 56.7499L105.05 59.8999L107.34 61.3899L106.04 64.6899L106.24 68.6899L108.09 68.5299C108.066 66.672 108.463 64.8328 109.25 63.1499C110.51 60.3499 115.08 52.2199 105.52 51.3899Z" fill="white"></path><path d="M99.4799 50.8799L98.8199 53.6699L101.97 52.7599L99.4799 50.8799Z" fill="white"></path><path d="M101.97 59.5399L103.37 54.9099L101.03 55.3499L101.97 59.5399Z" fill="white"></path><path d="M103.67 78.1199C103.614 78.1294 103.556 78.1294 103.5 78.1199C103.442 78.0979 103.389 78.0644 103.344 78.0215C103.3 77.9785 103.264 77.927 103.24 77.8699L101.55 73.8699C101.506 73.7656 101.502 73.6487 101.539 73.5417C101.576 73.4347 101.651 73.3451 101.75 73.2899L107.75 69.9599C107.807 69.9352 107.868 69.9224 107.93 69.9224C107.992 69.9224 108.053 69.9352 108.11 69.9599C108.169 69.9751 108.225 70.0025 108.273 70.0403C108.321 70.0782 108.361 70.1258 108.39 70.1799L110.59 74.4399C110.635 74.5558 110.635 74.6841 110.59 74.7999C110.572 74.8584 110.543 74.9126 110.503 74.9591C110.464 75.0056 110.415 75.0433 110.36 75.0699L103.82 78.1499C103.769 78.149 103.718 78.1388 103.67 78.1199ZM102.56 73.8499L103.9 76.9999L109.57 74.3299L107.81 70.9999L102.56 73.8499Z" fill="#333333"></path><path d="M100.51 72H100.41C100.35 71.9867 100.293 71.9612 100.243 71.9251C100.193 71.889 100.151 71.843 100.12 71.79C98.888 69.9552 97.1266 68.539 95.07 67.73C93.7397 67.1774 92.5778 66.2853 91.7004 65.1429C90.823 64.0005 90.2608 62.6478 90.07 61.22C89.92 59.29 90.66 56.5 95 54.12C95.0613 54.0927 95.1278 54.0786 95.195 54.0786C95.2621 54.0786 95.3286 54.0927 95.39 54.12C95.4544 54.1468 95.5121 54.1877 95.5588 54.2396C95.6055 54.2914 95.6401 54.3531 95.66 54.42L96.6 57.66L99.18 58.06C99.2948 58.0773 99.3989 58.1375 99.4712 58.2284C99.5436 58.3193 99.5788 58.4342 99.57 58.55L99.31 61.83L101.57 62.01C101.675 62.0222 101.774 62.0691 101.85 62.1433C101.926 62.2175 101.975 62.3148 101.99 62.42L102.41 65.84L104.41 69.13C104.473 69.2336 104.493 69.3579 104.465 69.4759C104.437 69.5939 104.363 69.696 104.26 69.76L100.76 72C100.678 72.0236 100.592 72.0236 100.51 72ZM95 55.24C92.24 56.88 90.88 58.92 91 61.17C91.1783 62.4177 91.675 63.5985 92.4422 64.5986C93.2093 65.5987 94.2211 66.3844 95.38 66.88C97.4982 67.7098 99.3269 69.1422 100.64 71L103.4 69.27L101.58 66.27C101.545 66.2124 101.524 66.1472 101.52 66.08L101.13 62.93L98.78 62.74C98.6597 62.7298 98.5482 62.6727 98.4696 62.581C98.391 62.4893 98.3517 62.3705 98.36 62.25L98.62 58.93L96.19 58.56C96.1023 58.5456 96.0203 58.5071 95.9532 58.4488C95.8861 58.3905 95.8365 58.3148 95.81 58.23L95 55.24Z" fill="#333333"></path><path d="M106.2 69.07C106.09 69.0733 105.982 69.0339 105.9 68.96C105.808 68.8773 105.751 68.7631 105.74 68.64L105.54 64.64C105.53 64.5771 105.53 64.5129 105.54 64.45L106.7 61.45L104.7 60.17C104.6 60.1009 104.53 59.9964 104.504 59.8776C104.478 59.7588 104.498 59.6346 104.56 59.53L106.37 56.74L104.37 55.25C104.301 55.1972 104.249 55.126 104.219 55.0446C104.189 54.9632 104.182 54.8749 104.2 54.79L104.97 51.2C104.992 51.0896 105.054 50.9914 105.145 50.9248C105.236 50.8581 105.348 50.8279 105.46 50.84C108.36 51.09 110.3 52.04 111.21 53.66C112.81 56.5 110.83 60.53 109.76 62.66L109.52 63.14C108.772 64.7563 108.392 66.519 108.41 68.3C108.416 68.4188 108.375 68.5352 108.297 68.6246C108.218 68.7141 108.108 68.7698 107.99 68.78L106.15 68.95L106.2 69.07ZM106.46 64.74L106.64 68.11L107.58 68.03C107.625 66.2452 108.037 64.4887 108.79 62.87C108.87 62.72 108.95 62.55 109.03 62.38C110.03 60.38 111.87 56.62 110.51 54.2C109.79 52.93 108.24 52.2 105.88 51.88L105.26 54.78L107.33 56.36C107.423 56.4282 107.486 56.5296 107.507 56.6431C107.527 56.7566 107.503 56.8736 107.44 56.97L105.65 59.74L107.55 60.97C107.637 61.0304 107.702 61.1186 107.733 61.2203C107.764 61.322 107.759 61.4312 107.72 61.53L106.46 64.74Z" fill="#333333"></path><path d="M98.82 54.1299C98.7009 54.1278 98.5869 54.0814 98.5 53.9999C98.4448 53.9423 98.4042 53.8722 98.3816 53.7957C98.359 53.7191 98.355 53.6383 98.37 53.5599L99 50.7699C99.0158 50.6957 99.051 50.627 99.1019 50.5708C99.1529 50.5146 99.2178 50.4729 99.29 50.4499C99.3608 50.4219 99.4376 50.4127 99.5129 50.4232C99.5883 50.4337 99.6597 50.4636 99.72 50.5099L102.21 52.3899C102.279 52.4431 102.332 52.5139 102.364 52.595C102.395 52.676 102.405 52.7641 102.39 52.8499C102.374 52.9335 102.334 53.0107 102.276 53.0727C102.217 53.1346 102.143 53.1788 102.06 53.1999L99 54.1099L98.82 54.1299ZM99.82 51.6699L99.45 52.9999L100.96 52.5599L99.82 51.6699Z" fill="#333333"></path><path d="M102 60.0001C101.898 59.9947 101.8 59.957 101.721 59.8922C101.642 59.8274 101.585 59.7391 101.56 59.6401L100.62 55.4601C100.604 55.3992 100.601 55.3357 100.612 55.2737C100.622 55.2117 100.645 55.1525 100.68 55.1001C100.753 54.9997 100.859 54.9288 100.98 54.9001L103.33 54.4601C103.408 54.4462 103.488 54.4525 103.562 54.4786C103.637 54.5046 103.704 54.5494 103.756 54.6087C103.808 54.668 103.844 54.7398 103.861 54.8171C103.877 54.8944 103.874 54.9746 103.85 55.0501L102.45 59.6701C102.425 59.7686 102.366 59.8554 102.284 59.9155C102.202 59.9757 102.102 60.0056 102 60.0001ZM101.61 55.7201L102.07 57.7201L102.74 55.4901L101.61 55.7201Z" fill="#333333"></path><path d="M101.54 69.0699C101.459 69.0702 101.379 69.0491 101.309 69.0087C101.239 68.9683 101.18 68.9101 101.14 68.8399C100.536 67.6437 99.7647 66.5394 98.85 65.5599C98.54 65.2999 98.44 65.3299 98.1 65.4499C97.7945 65.5868 97.4595 65.6447 97.1258 65.6185C96.7921 65.5923 96.4704 65.4828 96.19 65.2999C95.6421 65.0371 95.1958 64.6014 94.92 64.0599H94.63C94.3575 64.1426 94.066 64.1389 93.7957 64.0494C93.5253 63.9598 93.2893 63.7889 93.12 63.5599C92.95 63.2899 92.53 62.2199 94.18 59.7099C94.2133 59.6592 94.2563 59.6155 94.3066 59.5815C94.3569 59.5475 94.4134 59.5237 94.4729 59.5116C94.5324 59.4996 94.5937 59.4994 94.6533 59.5111C94.7128 59.5228 94.7695 59.5462 94.82 59.5799C94.8704 59.6123 94.9138 59.6544 94.9477 59.7038C94.9817 59.7532 95.0054 59.8089 95.0175 59.8676C95.0296 59.9263 95.0298 59.9868 95.0182 60.0456C95.0067 60.1044 94.9834 60.1602 94.95 60.2099C94.26 61.2099 93.66 62.6899 93.95 63.0699C94.01 63.1599 94.24 63.1999 94.55 63.1499H94.8C94.7952 63.0934 94.7952 63.0365 94.8 62.9799C94.8955 62.4786 95.1358 62.0163 95.4913 61.6502C95.8467 61.2841 96.3017 61.0302 96.8 60.9199C96.9728 60.8656 97.1571 60.8593 97.3332 60.9015C97.5093 60.9438 97.6707 61.0331 97.8 61.1599C97.9046 61.2888 97.9697 61.4451 97.9875 61.6101C98.0052 61.7752 97.9748 61.9418 97.9 62.0899C97.4863 62.9025 96.7669 63.5175 95.9 63.7999C96.1018 64.0959 96.3776 64.3338 96.7 64.4899C96.8725 64.6047 97.0716 64.6733 97.2782 64.689C97.4848 64.7048 97.692 64.6672 97.88 64.5799C99.01 64.1899 99.74 64.4299 102.02 68.3799C102.06 68.45 102.082 68.5294 102.082 68.6103C102.082 68.6912 102.06 68.7707 102.02 68.8406C101.979 68.9106 101.921 68.9687 101.851 69.0089C101.78 69.0492 101.701 69.0702 101.62 69.0699H101.54ZM96.93 61.8199C96.6725 61.8857 96.4338 62.0102 96.2325 62.1837C96.0313 62.3572 95.8729 62.575 95.77 62.8199C96.2493 62.6122 96.6539 62.2634 96.93 61.8199Z" fill="#333333"></path></svg></div><div class="css-1lvgb89">还没有评论，发表第一个评论吧</div></div><div class="css-1vjm6vs"></div></div></div></div></div></div><div class="PostIndex-Contributions" data-za-detail-view-path-module="ColumnList" data-za-detail-view-path-module_name="文章被以下专栏收录" data-za-extra-module="{}"><h3 class="BlockTitle">文章被以下专栏收录</h3><ul><div class="ContentItem Column-ColumnItem"><div class="ContentItem-main"><div class="ContentItem-image"><a class="ColumnLink" href="https://www.zhihu.com/column/c_1288235772122718208"><div class="css-1gomreu"><img class="Avatar css-1any501" src="./自监督学习_ 人工智能的未来 - 知乎_files/4b70deef7_l.jpg" srcset="https://picx.zhimg.com/4b70deef7_l.jpg?source=172ae18b 2x" alt="互联网学社"></div></a></div><div class="ContentItem-head"><h2 class="ContentItem-title"><span><a class="ColumnLink ColumnItem-Title" href="https://www.zhihu.com/column/c_1288235772122718208"><div class="css-1gomreu">互联网学社</div></a></span></h2><div class="ContentItem-meta">聚焦深度学习在互联网搜索推荐广告的排序算法。</div></div></div></div><div class="ContentItem Column-ColumnItem"><div class="ContentItem-main"><div class="ContentItem-image"><a class="ColumnLink" href="https://www.zhihu.com/column/c_1290942732261769216"><div class="css-1gomreu"><img class="Avatar css-1any501" src="./自监督学习_ 人工智能的未来 - 知乎_files/4b70deef7_l.jpg" srcset="https://picx.zhimg.com/4b70deef7_l.jpg?source=172ae18b 2x" alt="推荐+广告+搜索"></div></a></div><div class="ContentItem-head"><h2 class="ContentItem-title"><span><a class="ColumnLink ColumnItem-Title" href="https://www.zhihu.com/column/c_1290942732261769216"><div class="css-1gomreu">推荐+广告+搜索</div></a></span></h2><div class="ContentItem-meta">搜广推文章合集</div></div></div></div></ul></div><div role="complementary" aria-label="推荐阅读" class="Recommendations-Main" style="width: 1485px;"><h3 class="BlockTitle Recommendations-BlockTitle">推荐阅读</h3><ul class="Recommendations-List"><button class="PagingButton PagingButton-Previous" disabled="" data-za-detail-view-path-module="Unknown" data-za-detail-view-path-module_name="推荐阅读" data-za-extra-module="{}"><svg width="40" height="40" viewBox="0 0 24 24" fill="#d3d3d3" class="Zi Zi--ArrowLeft"><path fill-rule="evenodd" d="m10.752 12 4.025-3.78a.684.684 0 0 0 0-1.01.796.796 0 0 0-1.075 0l-4.42 4.15a.866.866 0 0 0 0 1.28l4.42 4.15a.796.796 0 0 0 1.075 0 .684.684 0 0 0 0-1.01L10.752 12Z" clip-rule="evenodd"></path></svg></button><a href="https://zhuanlan.zhihu.com/p/94385056" class="PostItem"><div><h1 class="PostItem-Title">人工智能的下半场，一定少不了自监督学习</h1><p class="PostItem-Summary">雷锋网AI科技评论按：深度学习在计算机视觉、自然语言处理、语音识别等领域的广泛应用催生了人工智能的第四次爆发。然而，当前主流的监督式学习任务往往过度依赖于人工标注，即所谓「有多少…</p><div class="PostItem-Footer"><span>雷峰网</span><span class="PostItem-FooterTitle"></span></div></div></a><a href="https://zhuanlan.zhihu.com/p/71487236" class="PostItem"><div><img src="./自监督学习_ 人工智能的未来 - 知乎_files/v2-67af8b52f3bfc5e9fcac326a0480ff4e_250x0.jpg" srcset="https://picx.zhimg.com/v2-67af8b52f3bfc5e9fcac326a0480ff4e_qhd.jpg?source=172ae18b 2x" class="PostItem-TitleImage" alt="人工智能学习心得"><h1 class="PostItem-Title">人工智能学习心得</h1><div class="PostItem-Footer"><span>刘思哲</span><span class="PostItem-FooterTitle"></span></div></div></a><a href="https://zhuanlan.zhihu.com/p/402902793" class="PostItem"><div><img src="./自监督学习_ 人工智能的未来 - 知乎_files/v2-0c0bc3f500010db39bfd8373ddfbf78b_250x0.jpg" srcset="https://pica.zhimg.com/v2-0c0bc3f500010db39bfd8373ddfbf78b_qhd.jpg?source=172ae18b 2x" class="PostItem-TitleImage" alt="深入理解人工智能——什么是表示学习？"><h1 class="PostItem-Title">深入理解人工智能——什么是表示学习？</h1><div class="PostItem-Footer"><span>药老算法</span><span class="PostItem-FooterTitle">发表于药老算法</span></div></div></a><a href="https://zhuanlan.zhihu.com/p/252366379" class="PostItem"><div><h1 class="PostItem-Title">浅谈人工智能：现状、任务、构架与统一 | 朱松纯</h1><p class="PostItem-Summary">关注风云之声 提升思维层次 导读 最近，人工智能领军科学家、美国加州大学洛杉矶分校教授、科大校友朱松纯据传将回国在清华大学任教。朱教授的女儿朱易是2018年全美花样滑冰锦标赛女单新人…</p><div class="PostItem-Footer"><span>袁岚峰</span><span class="PostItem-FooterTitle">发表于风云之声</span></div></div></a><button class="PagingButton PagingButton-Next" data-za-detail-view-path-module="Unknown" data-za-detail-view-path-module_name="推荐阅读" data-za-extra-module="{}"><svg width="40" height="40" viewBox="0 0 24 24" fill="#d3d3d3" class="Zi Zi--ArrowRight"><path fill-rule="evenodd" d="m13.248 12-4.025 3.78a.684.684 0 0 0 0 1.01.796.796 0 0 0 1.075 0l4.42-4.15a.867.867 0 0 0 0-1.28l-4.42-4.15a.796.796 0 0 0-1.075 0 .684.684 0 0 0 0 1.01L13.248 12Z" clip-rule="evenodd"></path></svg></button></ul></div></div></div></main><div role="complementary"><div class="CornerButtons"><div class="CornerAnimayedFlex"><button data-tooltip="回到顶部" data-tooltip-position="left" data-tooltip-will-hide-on-click="true" aria-label="回到顶部" type="button" class="Button CornerButton Button--plain css-gdd4kf"><svg width="24" height="24" viewBox="0 0 24 24" aria-hidden="true" class="Zi Zi--BackToTop" fill="currentColor"><path fill-rule="evenodd" d="M13.204 3.107a1.75 1.75 0 0 0-2.408 0L3.806 9.73c-1.148 1.088-.378 3.02 1.204 3.02h2.24V20c0 .966.784 1.75 1.75 1.75h6A1.75 1.75 0 0 0 16.75 20v-7.25h2.24c1.582 0 2.353-1.932 1.204-3.02l-6.99-6.623Z" clip-rule="evenodd"></path></svg></button></div></div></div></div></div><script id="js-clientConfig" type="text/json">{"fetchRoot":{"www":"https:\u002F\u002Fwww.zhihu.com","api":"https:\u002F\u002Fapi.zhihu.com","lens":"https:\u002F\u002Flens.zhihu.com","zhuanlan":"https:\u002F\u002Fzhuanlan.zhihu.com\u002Fapi\u002F","walletpay":"https:\u002F\u002Fwalletpay.zhihu.com","captcha":"https:\u002F\u002Fcaptcha.zhihu.com","vzuu":"https:\u002F\u002Fv.vzuu.com","openapi":"https:\u002F\u002Fopenapi.zhihu.com","svip":"https:\u002F\u002Fsvip.zhihu.com"},"host":"zhihu.com","protocol":"https:","wwwHost":"www.zhihu.com","videoHost":"video.zhihu.com","zhuanlanHost":"zhuanlan.zhihu.com","allowSignUp":true,"refreshValidityPeriod":"30","refreshLimit":"1","release":"538-e2fa9a97","currentEntry":"column","isMobileEntry":false,"apollo":{"env":"prod","globalSilence":"","topstory_rec_adp":"1","grayScale":"25,4"}}</script><script id="js-initialData" type="text/json">{"initialState":{"common":{"ask":{}},"loading":{"global":{"count":0},"local":{"env\u002FgetIpinfo\u002F":false,"article\u002Fget\u002F":false,"brand\u002FgetUrl\u002F":false,"article\u002FloadPostSearchEntity\u002F":false}},"entities":{"users":{"9f2f6a5227e07ce2e98d5ab7c43ce1f5":{"uid":1117354419397857300,"userType":"people","id":"9f2f6a5227e07ce2e98d5ab7c43ce1f5"},"gu-yu-long-eric":{"isFollowed":false,"avatarUrlTemplate":"https:\u002F\u002Fpicx.zhimg.com\u002Fv2-5e488c2bdf6db9763d3defb3b20d6089.jpg?source=172ae18b","uid":"926456305063985152","userType":"people","isFollowing":false,"urlToken":"gu-yu-long-eric","id":"bcbc6dd5b960ac12029354ab6fa06769","description":"guyulongcs@gmail.com","name":"谷育龙Eric","isAdvertiser":false,"headline":"字节跳动抖音推荐长期招聘算法工程师和实习生，欢迎联系!","gender":1,"url":"\u002Fpeople\u002Fbcbc6dd5b960ac12029354ab6fa06769","avatarUrl":"https:\u002F\u002Fpica.zhimg.com\u002Fv2-5e488c2bdf6db9763d3defb3b20d6089_l.jpg?source=172ae18b","isOrg":false,"type":"people","vipInfo":{"isVip":true,"vipIcon":{"url":"https:\u002F\u002Fpicx.zhimg.com\u002Fv2-4812630bc27d642f7cafcd6cdeca3d7a.jpg?source=88ceefae","nightModeUrl":"https:\u002F\u002Fpicx.zhimg.com\u002Fv2-c9686ff064ea3579730756ac6c289978.jpg?source=88ceefae"}},"badge":[{"type":"identity","topics":[],"description":"清华大学 计算机系博士"}],"badgeV2":{"title":"清华大学 计算机系博士","mergedBadges":[{"type":"identity","detailType":"identity","title":"认证","description":"清华大学 计算机系博士","url":"https:\u002F\u002Fwww.zhihu.com\u002Faccount\u002Fverification\u002Fintro","sources":[],"icon":"","nightIcon":""}],"detailBadges":[{"type":"identity","detailType":"identity_people","title":"已认证的个人","description":"清华大学 计算机系博士","url":"https:\u002F\u002Fwww.zhihu.com\u002Faccount\u002Fverification\u002Fintro","sources":[],"icon":"https:\u002F\u002Fpicx.zhimg.com\u002Fv2-235258cecb8a0f184c4d38483cd6f6b6_l.png?source=32738c0c","nightIcon":"https:\u002F\u002Fpicx.zhimg.com\u002Fv2-45e870b8f0982bcd7537ea4627afbd00_l.png?source=32738c0c"}],"icon":"https:\u002F\u002Fpicx.zhimg.com\u002Fv2-235258cecb8a0f184c4d38483cd6f6b6_l.png?source=32738c0c","nightIcon":"https:\u002F\u002Fpic1.zhimg.com\u002Fv2-45e870b8f0982bcd7537ea4627afbd00_l.png?source=32738c0c"},"exposedMedal":{"medalId":"972460827463282688","medalName":"表示赞同","avatarUrl":"https:\u002F\u002Fpic1.zhimg.com\u002Fv2-c38ebd5a7d1f43a5ed9ac0411a3e233a_r.png?source=172ae18b","miniAvatarUrl":"https:\u002F\u002Fpicx.zhimg.com\u002Fv2-c38ebd5a7d1f43a5ed9ac0411a3e233a_l.png?source=172ae18b","description":"赞同 5 次","medalAvatarFrame":""}}},"questions":{},"answers":{},"articles":{"270547809":{"trackUrl":["https:\u002F\u002Fsugar.zhihu.com\u002Fplutus_adreaper\u002Fcontent_monitor_log?si=__SESSIONID__&ti=__ATOKEN__&at=view&pf=__OS__&ed=BiBUKF0xBSkqGGBRCWB8All3DOONMGAZe7rb&idfa=__IDFA__&imei=__IMEI__&androidid=__ANDROIDID__&oaid=__OAID__&ci=__CREATIVEID__&zid=__ZONEID__"],"id":270547809,"title":"自监督学习: 人工智能的未来","type":"article","articleType":"normal","excerptTitle":"","url":"https:\u002F\u002Fzhuanlan.zhihu.com\u002Fp\u002F270547809","imageUrl":"https:\u002F\u002Fpicx.zhimg.com\u002Fv2-11303ae8c2a58596a23d75fe9cd6e1c4_720w.jpg?source=172ae18b","titleImage":"https:\u002F\u002Fpicx.zhimg.com\u002Fv2-11303ae8c2a58596a23d75fe9cd6e1c4_720w.jpg?source=172ae18b","excerpt":"\u003Cimg src=\"https:\u002F\u002Fpic4.zhimg.com\u002Fv2-40069111d6becaf29815576a6cb607bf_200x112.png\" data-caption=\"Self-supervised Visual Feature Learning with Deep Neural Networks: A Survey\" data-size=\"normal\" data-rawwidth=\"986\" data-rawheight=\"1502\" data-watermark=\"original\" data-original-src=\"v2-40069111d6becaf29815576a6cb607bf\" data-watermark-src=\"v2-3e1157710d00d266c21ef9bc2443b15a\" data-private-watermark-src=\"\" data-original=\"https:\u002F\u002Fpic4.zhimg.com\u002Fv2-40069111d6becaf29815576a6cb607bf_r.png\" class=\"origin_image inline-img zh-lightbox-thumb\"\u002F\u003E原创：知乎专栏 \u003Ca href=\"https:\u002F\u002Fwww.zhihu.com\u002Fcolumn\u002Fc_1288235772122718208\" class=\"internal\"\u003E搜索推荐广告排序艺术\u003C\u002Fa\u003E，转载请联系作者。导读：什么是自监督学习？为什么自监督学习是AI的未来？自监督学习如何实现？本文将回顾下自监督学习的前世今生，介绍它在CV、NLP、Graph、RecSys、RL等领域已经取得的令人惊叹的效果！ 1. 什么是自…","created":1605878766,"updated":1620701350,"author":{"isFollowed":false,"avatarUrlTemplate":"https:\u002F\u002Fpicx.zhimg.com\u002Fv2-5e488c2bdf6db9763d3defb3b20d6089.jpg?source=172ae18b","uid":"926456305063985152","userType":"people","isFollowing":false,"urlToken":"gu-yu-long-eric","id":"bcbc6dd5b960ac12029354ab6fa06769","description":"guyulongcs@gmail.com","name":"谷育龙Eric","isAdvertiser":false,"headline":"字节跳动抖音推荐长期招聘算法工程师和实习生，欢迎联系!","gender":1,"url":"\u002Fpeople\u002Fbcbc6dd5b960ac12029354ab6fa06769","avatarUrl":"https:\u002F\u002Fpica.zhimg.com\u002Fv2-5e488c2bdf6db9763d3defb3b20d6089_l.jpg?source=172ae18b","isOrg":false,"type":"people","vipInfo":{"isVip":true,"vipIcon":{"url":"https:\u002F\u002Fpicx.zhimg.com\u002Fv2-4812630bc27d642f7cafcd6cdeca3d7a.jpg?source=88ceefae","nightModeUrl":"https:\u002F\u002Fpicx.zhimg.com\u002Fv2-c9686ff064ea3579730756ac6c289978.jpg?source=88ceefae"}},"badge":[{"type":"identity","topics":[],"description":"清华大学 计算机系博士"}],"badgeV2":{"title":"清华大学 计算机系博士","mergedBadges":[{"type":"identity","detailType":"identity","title":"认证","description":"清华大学 计算机系博士","url":"https:\u002F\u002Fwww.zhihu.com\u002Faccount\u002Fverification\u002Fintro","sources":[],"icon":"","nightIcon":""}],"detailBadges":[{"type":"identity","detailType":"identity_people","title":"已认证的个人","description":"清华大学 计算机系博士","url":"https:\u002F\u002Fwww.zhihu.com\u002Faccount\u002Fverification\u002Fintro","sources":[],"icon":"https:\u002F\u002Fpicx.zhimg.com\u002Fv2-235258cecb8a0f184c4d38483cd6f6b6_l.png?source=32738c0c","nightIcon":"https:\u002F\u002Fpicx.zhimg.com\u002Fv2-45e870b8f0982bcd7537ea4627afbd00_l.png?source=32738c0c"}],"icon":"https:\u002F\u002Fpicx.zhimg.com\u002Fv2-235258cecb8a0f184c4d38483cd6f6b6_l.png?source=32738c0c","nightIcon":"https:\u002F\u002Fpic1.zhimg.com\u002Fv2-45e870b8f0982bcd7537ea4627afbd00_l.png?source=32738c0c"},"exposedMedal":{"medalId":"972460827463282688","medalName":"表示赞同","avatarUrl":"https:\u002F\u002Fpic1.zhimg.com\u002Fv2-c38ebd5a7d1f43a5ed9ac0411a3e233a_r.png?source=172ae18b","miniAvatarUrl":"https:\u002F\u002Fpicx.zhimg.com\u002Fv2-c38ebd5a7d1f43a5ed9ac0411a3e233a_l.png?source=172ae18b","description":"赞同 5 次","medalAvatarFrame":""}},"commentPermission":"censor","copyrightPermission":"need_review","state":"published","ipInfo":"","imageWidth":864,"imageHeight":486,"content":"\u003Cp data-pid=\"No2mR7k8\"\u003E原创：知乎专栏\u003Ca href=\"https:\u002F\u002Fwww.zhihu.com\u002Fcolumn\u002Fc_1288235772122718208\" class=\"internal\"\u003E搜索推荐广告排序艺术\u003C\u002Fa\u003E，转载请联系作者。\u003C\u002Fp\u003E\u003Cp data-pid=\"nMdro9nh\"\u003E导读：什么是自监督学习？为什么自监督学习是AI的未来？自监督学习如何实现？本文将回顾下自监督学习的前世今生，介绍它在CV、NLP、Graph、RecSys、RL等领域已经取得的令人惊叹的效果！\u003C\u002Fp\u003E\u003Chr\u002F\u003E\u003Ch2\u003E1. 什么是自监督学习？\u003C\u002Fh2\u003E\u003Cp data-pid=\"IteaOGre\"\u003E实际的场景里通常有海量的无监督数据，而有监督数据却很少。那么能否利用这些海量的无监督数据，用来改进监督学习任务的效果呢？\u003C\u002Fp\u003E\u003Cp data-pid=\"A-lpXem_\"\u003E▲自监督学习（Self-supervised Learning）作为Unsupervised Learning的一个重要分支，给出了很好地解决方案。它的目标是更好地利用无监督数据，提升后续监督学习任务的效果。\u003C\u002Fp\u003E\u003Cp data-pid=\"sgZz8pm9\"\u003E其基本思想是：Predicting everything from everything else。\u003C\u002Fp\u003E\u003Cp data-pid=\"xxZnPx4X\"\u003E具体方法是首先定义一个Pretext task (辅助任务)，即从无监督的数据中，通过巧妙地设计自动构造出有监督（伪标签）数据，学习一个预训练模型。构造有监督（伪标签）数据的方法可以是：假装输入中的一部分不存在，然后基于其余的部分用模型预测缺失的这部分。如果学习的预训练模型能准确预测缺失部分的数据，说明它的表示学习能力很强，能够学习到输入中的高级语义信息、泛化能力比较强。而深度学习的精髓正在于强大的表示学习能力。\u003C\u002Fp\u003E\u003Cp data-pid=\"wnsp_epY\"\u003E然后可以将预训练模型，通过简单的Finetune，应用到下游的多个应用场景，能比只使用监督数据训练的模型有更好的效果。\u003C\u002Fp\u003E\u003Cp data-pid=\"8Q3OpKz6\"\u003E▲通常来说有标签数据越少的场景，自监督学习能带来的提升越大。事实上，在一些论文的实验结果里，在大量无标签数据上自监督学习的模型，不需要finetune，能取得比使用标签数据学得的监督模型更好的效果……\u003C\u002Fp\u003E\u003Cp data-pid=\"ZoLIHRyU\"\u003E对于有大量标签数据的场景，自监督学习也能进一步提升模型的泛化能力和效果。\u003C\u002Fp\u003E\u003Cp data-pid=\"Ak7WNd7q\"\u003E▲下图展示了在CV领域自监督学习的标准流程：\u003C\u002Fp\u003E\u003Cp data-pid=\"ESdnMRDV\"\u003E在自监督学习中，最重要的问题是：如何定义Pretext任务、如何从Pretext任务学习预训练模型。\u003C\u002Fp\u003E\u003Cfigure data-size=\"normal\"\u003E\u003Cnoscript\u003E\u003Cimg src=\"https:\u002F\u002Fpic4.zhimg.com\u002Fv2-40069111d6becaf29815576a6cb607bf_b.jpg\" data-size=\"normal\" data-rawwidth=\"986\" data-rawheight=\"1502\" class=\"origin_image zh-lightbox-thumb\" width=\"986\" data-original=\"https:\u002F\u002Fpic4.zhimg.com\u002Fv2-40069111d6becaf29815576a6cb607bf_r.jpg\"\u002F\u003E\u003C\u002Fnoscript\u003E\u003Cimg src=\"data:image\u002Fsvg+xml;utf8,&lt;svg xmlns=&#39;http:\u002F\u002Fwww.w3.org\u002F2000\u002Fsvg&#39; width=&#39;986&#39; height=&#39;1502&#39;&gt;&lt;\u002Fsvg&gt;\" data-size=\"normal\" data-rawwidth=\"986\" data-rawheight=\"1502\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"986\" data-original=\"https:\u002F\u002Fpic4.zhimg.com\u002Fv2-40069111d6becaf29815576a6cb607bf_r.jpg\" data-actualsrc=\"https:\u002F\u002Fpic4.zhimg.com\u002Fv2-40069111d6becaf29815576a6cb607bf_b.jpg\"\u002F\u003E\u003Cfigcaption\u003ESelf-supervised Visual Feature Learning with Deep Neural Networks: A Survey\u003C\u002Ffigcaption\u003E\u003C\u002Ffigure\u003E\u003Chr\u002F\u003E\u003Ch2\u003E2. 为什么自监督学习是AI的未来？\u003C\u002Fh2\u003E\u003Cp data-pid=\"cLcOyN35\"\u003EYann Lecun在AAAI 2020的演讲中，指出目前深度学习遇到的挑战：\u003C\u002Fp\u003E\u003Cp data-pid=\"1E7qQDPy\"\u003E▲监督学习：深度模型有海量参数，需要大量的label数据，标注成本高、扩展性差，难以应用到无标记或标记数据少的场景。\u003C\u002Fp\u003E\u003Cp data-pid=\"0YTAdJFC\"\u003E▲强化学习：agent需要和环境大量的交互尝试，很多实际场景（例如互联网搜索推荐、无人驾驶）中交互成本大、代价高，很难应用。\u003C\u002Fp\u003E\u003Cp data-pid=\"TofRecPs\"\u003E而人类和动物学习快速的原因：最重要的是观察世界，而不是靠大量的监督、强化学习。\u003C\u002Fp\u003E\u003Cfigure data-size=\"normal\"\u003E\u003Cnoscript\u003E\u003Cimg src=\"https:\u002F\u002Fpic3.zhimg.com\u002Fv2-b357fb6b1189f5bcd0ebc9f7b3b0adda_b.jpg\" data-size=\"normal\" data-rawwidth=\"2034\" data-rawheight=\"1144\" class=\"origin_image zh-lightbox-thumb\" width=\"2034\" data-original=\"https:\u002F\u002Fpic3.zhimg.com\u002Fv2-b357fb6b1189f5bcd0ebc9f7b3b0adda_r.jpg\"\u002F\u003E\u003C\u002Fnoscript\u003E\u003Cimg src=\"data:image\u002Fsvg+xml;utf8,&lt;svg xmlns=&#39;http:\u002F\u002Fwww.w3.org\u002F2000\u002Fsvg&#39; width=&#39;2034&#39; height=&#39;1144&#39;&gt;&lt;\u002Fsvg&gt;\" data-size=\"normal\" data-rawwidth=\"2034\" data-rawheight=\"1144\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"2034\" data-original=\"https:\u002F\u002Fpic3.zhimg.com\u002Fv2-b357fb6b1189f5bcd0ebc9f7b3b0adda_r.jpg\" data-actualsrc=\"https:\u002F\u002Fpic3.zhimg.com\u002Fv2-b357fb6b1189f5bcd0ebc9f7b3b0adda_b.jpg\"\u002F\u003E\u003Cfigcaption\u003EYann LeCun\u003C\u002Ffigcaption\u003E\u003C\u002Ffigure\u003E\u003Cp data-pid=\"Ru4_LURK\"\u003E智能的精髓在于预测：我们通过观察世界、理解世界、尝试预测未来，并根据实际结果的反馈信息，来不断调整自己的世界模型，变得越来越有智能。简单来说，无论人还是机器，预测的准确度越高，说明智能越强。\u003C\u002Fp\u003E\u003Cp data-pid=\"xxPME0zL\"\u003E自监督学习的思想就是通过构造任务来提升预训练模型预测能力，即Predicting everything from everything else。具体方法是假装输入中的一部分不存在，然后基于其余的部分用模型预测这个部分，从而学习得到一个能很好地建模输入语义信息的表示学习模型。\u003C\u002Fp\u003E\u003Cfigure data-size=\"normal\"\u003E\u003Cnoscript\u003E\u003Cimg src=\"https:\u002F\u002Fpic1.zhimg.com\u002Fv2-a09225e56ba51f8a5c1bd012f7013be8_b.jpg\" data-size=\"normal\" data-rawwidth=\"1440\" data-rawheight=\"822\" class=\"origin_image zh-lightbox-thumb\" width=\"1440\" data-original=\"https:\u002F\u002Fpic1.zhimg.com\u002Fv2-a09225e56ba51f8a5c1bd012f7013be8_r.jpg\"\u002F\u003E\u003C\u002Fnoscript\u003E\u003Cimg src=\"data:image\u002Fsvg+xml;utf8,&lt;svg xmlns=&#39;http:\u002F\u002Fwww.w3.org\u002F2000\u002Fsvg&#39; width=&#39;1440&#39; height=&#39;822&#39;&gt;&lt;\u002Fsvg&gt;\" data-size=\"normal\" data-rawwidth=\"1440\" data-rawheight=\"822\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1440\" data-original=\"https:\u002F\u002Fpic1.zhimg.com\u002Fv2-a09225e56ba51f8a5c1bd012f7013be8_r.jpg\" data-actualsrc=\"https:\u002F\u002Fpic1.zhimg.com\u002Fv2-a09225e56ba51f8a5c1bd012f7013be8_b.jpg\"\u002F\u003E\u003Cfigcaption\u003EYann LeCun\u003C\u002Ffigcaption\u003E\u003C\u002Ffigure\u003E\u003Cp data-pid=\"NXv-EFre\"\u003E例如，在NLP中，自监督学习Word2Vec、BERT、GPT、GPT2、GPT3等模型，可以很好地应用到语言模型、机器翻译、对话系统等多个任务中。\u003C\u002Fp\u003E\u003Cp data-pid=\"w-9yiZBr\"\u003E在CV中，自监督学习SimCLR等模型，可以应用到图像分类、补全等任务。\u003C\u002Fp\u003E\u003Cfigure data-size=\"normal\"\u003E\u003Cnoscript\u003E\u003Cimg src=\"https:\u002F\u002Fpic4.zhimg.com\u002Fv2-56599f893490139a121a3aeada412877_b.jpg\" data-size=\"normal\" data-rawwidth=\"2006\" data-rawheight=\"1146\" class=\"origin_image zh-lightbox-thumb\" width=\"2006\" data-original=\"https:\u002F\u002Fpic4.zhimg.com\u002Fv2-56599f893490139a121a3aeada412877_r.jpg\"\u002F\u003E\u003C\u002Fnoscript\u003E\u003Cimg src=\"data:image\u002Fsvg+xml;utf8,&lt;svg xmlns=&#39;http:\u002F\u002Fwww.w3.org\u002F2000\u002Fsvg&#39; width=&#39;2006&#39; height=&#39;1146&#39;&gt;&lt;\u002Fsvg&gt;\" data-size=\"normal\" data-rawwidth=\"2006\" data-rawheight=\"1146\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"2006\" data-original=\"https:\u002F\u002Fpic4.zhimg.com\u002Fv2-56599f893490139a121a3aeada412877_r.jpg\" data-actualsrc=\"https:\u002F\u002Fpic4.zhimg.com\u002Fv2-56599f893490139a121a3aeada412877_b.jpg\"\u002F\u003E\u003Cfigcaption\u003EYann LeCun\u003C\u002Ffigcaption\u003E\u003C\u002Ffigure\u003E\u003Cp data-pid=\"0XiY1DT_\"\u003E关于机器学习的作用，LeCun做了一个形象的比喻，如下图所示：强化学习像蛋糕上的樱桃，监督学习像蛋糕上的糖霜，而自监督学习是蛋糕本身。\u003C\u002Fp\u003E\u003Cfigure data-size=\"normal\"\u003E\u003Cnoscript\u003E\u003Cimg src=\"https:\u002F\u002Fpic4.zhimg.com\u002Fv2-23b7d4df2c260ff77d79ec03833b85ab_b.jpg\" data-size=\"normal\" data-rawwidth=\"720\" data-rawheight=\"414\" class=\"origin_image zh-lightbox-thumb\" width=\"720\" data-original=\"https:\u002F\u002Fpic4.zhimg.com\u002Fv2-23b7d4df2c260ff77d79ec03833b85ab_r.jpg\"\u002F\u003E\u003C\u002Fnoscript\u003E\u003Cimg src=\"data:image\u002Fsvg+xml;utf8,&lt;svg xmlns=&#39;http:\u002F\u002Fwww.w3.org\u002F2000\u002Fsvg&#39; width=&#39;720&#39; height=&#39;414&#39;&gt;&lt;\u002Fsvg&gt;\" data-size=\"normal\" data-rawwidth=\"720\" data-rawheight=\"414\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"720\" data-original=\"https:\u002F\u002Fpic4.zhimg.com\u002Fv2-23b7d4df2c260ff77d79ec03833b85ab_r.jpg\" data-actualsrc=\"https:\u002F\u002Fpic4.zhimg.com\u002Fv2-23b7d4df2c260ff77d79ec03833b85ab_b.jpg\"\u002F\u003E\u003Cfigcaption\u003EYann LeCun\u003C\u002Ffigcaption\u003E\u003C\u002Ffigure\u003E\u003Cp data-pid=\"MDNPbaHH\"\u003E通过这个比喻，可以很好的理解自监督学习在人工智能中的重要基石作用。\u003C\u002Fp\u003E\u003Cp data-pid=\"Z_-X0Vjp\"\u003E数据时代，很有前景的人工智能的实现方式是：在底层，首先基于海量的无标签数据，利用自监督学习，学习得到一个强大的通用表示模型。在上层，基于监督学习实现具体任务的目标、基于强化学习实现智能控制。自监督学习的作用在于，可以增强监督学习和强化学习模型的性能、泛化能力、鲁棒性等。\u003C\u002Fp\u003E\u003Cp data-pid=\"oYvpxr5p\"\u003E事实上，在CV、NLP等人工智能领域，自监督学习已经开始发挥着至关重要的作用。\u003C\u002Fp\u003E\u003Chr\u002F\u003E\u003Ch2\u003E3. 自监督学习如何实现？\u003C\u002Fh2\u003E\u003Cp data-pid=\"1sQLH_c8\"\u003E自监督学习的关键在于如何设计Pretext任务、损失函数，来优化预训练模型参数。\u003C\u002Fp\u003E\u003Cp data-pid=\"eruolaIY\"\u003E下面我们通过介绍自监督学习在CV、NLP、Graph、Recsys等领域的经典工作，了解自监督学习的具体实现方式。\u003C\u002Fp\u003E\u003Ch2\u003E3.1 Computer Vision:\u003C\u002Fh2\u003E\u003Cp data-pid=\"KZcoFBPl\"\u003ECV领域中，自监督学习的Pretext任务可以是预测图片相对位置信息、旋转角度、视频中帧的顺序等。\u003C\u002Fp\u003E\u003Cp data-pid=\"5raaaPii\"\u003E\u003Cb\u003E[1] 2015 (ICCV) Unsupervised Learning of Visual Representations Using Videos\u003C\u002Fb\u003E\u003C\u002Fp\u003E\u003Cp data-pid=\"H8B-xzsv\"\u003EICCV 2015这篇论文的思想是：视频场景，对物体跟踪，对于一个包含物体o的帧X，包含物体o的另外一个帧 \u003Cimg src=\"https:\u002F\u002Fwww.zhihu.com\u002Fequation?tex=X_i%5E%2B\" alt=\"X_i^+\" eeimg=\"1\"\u002F\u003E 应该比一个随机的帧 \u003Cimg src=\"https:\u002F\u002Fwww.zhihu.com\u002Fequation?tex=X_i%5E-\" alt=\"X_i^-\" eeimg=\"1\"\u002F\u003E 和X的相似度更高。\u003C\u002Fp\u003E\u003Cfigure data-size=\"normal\"\u003E\u003Cnoscript\u003E\u003Cimg src=\"https:\u002F\u002Fpic2.zhimg.com\u002Fv2-64c10a3743a84f95d97e56c80c048825_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1300\" data-rawheight=\"1154\" class=\"origin_image zh-lightbox-thumb\" width=\"1300\" data-original=\"https:\u002F\u002Fpic2.zhimg.com\u002Fv2-64c10a3743a84f95d97e56c80c048825_r.jpg\"\u002F\u003E\u003C\u002Fnoscript\u003E\u003Cimg src=\"data:image\u002Fsvg+xml;utf8,&lt;svg xmlns=&#39;http:\u002F\u002Fwww.w3.org\u002F2000\u002Fsvg&#39; width=&#39;1300&#39; height=&#39;1154&#39;&gt;&lt;\u002Fsvg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1300\" data-rawheight=\"1154\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1300\" data-original=\"https:\u002F\u002Fpic2.zhimg.com\u002Fv2-64c10a3743a84f95d97e56c80c048825_r.jpg\" data-actualsrc=\"https:\u002F\u002Fpic2.zhimg.com\u002Fv2-64c10a3743a84f95d97e56c80c048825_b.jpg\"\u002F\u003E\u003C\u002Ffigure\u003E\u003Cp data-pid=\"1eQMvn7W\"\u003E这篇论文里使用CNN作为Encoder, 对于图片X，f代表CNN，f(X)为X新的表示向量，定义两个图片的距离为如下：\u003C\u002Fp\u003E\u003Cfigure data-size=\"normal\"\u003E\u003Cnoscript\u003E\u003Cimg src=\"https:\u002F\u002Fpic2.zhimg.com\u002Fv2-4c4bdbab0ac9baf233fc6f6c868b3d9d_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"602\" data-rawheight=\"134\" class=\"origin_image zh-lightbox-thumb\" width=\"602\" data-original=\"https:\u002F\u002Fpic2.zhimg.com\u002Fv2-4c4bdbab0ac9baf233fc6f6c868b3d9d_r.jpg\"\u002F\u003E\u003C\u002Fnoscript\u003E\u003Cimg src=\"data:image\u002Fsvg+xml;utf8,&lt;svg xmlns=&#39;http:\u002F\u002Fwww.w3.org\u002F2000\u002Fsvg&#39; width=&#39;602&#39; height=&#39;134&#39;&gt;&lt;\u002Fsvg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"602\" data-rawheight=\"134\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"602\" data-original=\"https:\u002F\u002Fpic2.zhimg.com\u002Fv2-4c4bdbab0ac9baf233fc6f6c868b3d9d_r.jpg\" data-actualsrc=\"https:\u002F\u002Fpic2.zhimg.com\u002Fv2-4c4bdbab0ac9baf233fc6f6c868b3d9d_b.jpg\"\u002F\u003E\u003C\u002Ffigure\u003E\u003Cp data-pid=\"FSJAuJqx\"\u003E然后就可以基于hinge loss来学习CNN的参数了：\u003C\u002Fp\u003E\u003Cfigure data-size=\"normal\"\u003E\u003Cnoscript\u003E\u003Cimg src=\"https:\u002F\u002Fpic4.zhimg.com\u002Fv2-663548a78f4651ffef5ef8d2c581c11b_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"958\" data-rawheight=\"132\" class=\"origin_image zh-lightbox-thumb\" width=\"958\" data-original=\"https:\u002F\u002Fpic4.zhimg.com\u002Fv2-663548a78f4651ffef5ef8d2c581c11b_r.jpg\"\u002F\u003E\u003C\u002Fnoscript\u003E\u003Cimg src=\"data:image\u002Fsvg+xml;utf8,&lt;svg xmlns=&#39;http:\u002F\u002Fwww.w3.org\u002F2000\u002Fsvg&#39; width=&#39;958&#39; height=&#39;132&#39;&gt;&lt;\u002Fsvg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"958\" data-rawheight=\"132\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"958\" data-original=\"https:\u002F\u002Fpic4.zhimg.com\u002Fv2-663548a78f4651ffef5ef8d2c581c11b_r.jpg\" data-actualsrc=\"https:\u002F\u002Fpic4.zhimg.com\u002Fv2-663548a78f4651ffef5ef8d2c581c11b_b.jpg\"\u002F\u003E\u003C\u002Ffigure\u003E\u003Cfigure data-size=\"normal\"\u003E\u003Cnoscript\u003E\u003Cimg src=\"https:\u002F\u002Fpic4.zhimg.com\u002Fv2-aad32041da3bc3bf37909da832a77f7f_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1092\" data-rawheight=\"748\" class=\"origin_image zh-lightbox-thumb\" width=\"1092\" data-original=\"https:\u002F\u002Fpic4.zhimg.com\u002Fv2-aad32041da3bc3bf37909da832a77f7f_r.jpg\"\u002F\u003E\u003C\u002Fnoscript\u003E\u003Cimg src=\"data:image\u002Fsvg+xml;utf8,&lt;svg xmlns=&#39;http:\u002F\u002Fwww.w3.org\u002F2000\u002Fsvg&#39; width=&#39;1092&#39; height=&#39;748&#39;&gt;&lt;\u002Fsvg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1092\" data-rawheight=\"748\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1092\" data-original=\"https:\u002F\u002Fpic4.zhimg.com\u002Fv2-aad32041da3bc3bf37909da832a77f7f_r.jpg\" data-actualsrc=\"https:\u002F\u002Fpic4.zhimg.com\u002Fv2-aad32041da3bc3bf37909da832a77f7f_b.jpg\"\u002F\u003E\u003C\u002Ffigure\u003E\u003Cp data-pid=\"oIvp1bJc\"\u003E学习完CNN参数，就可以将它作为表示学习器（特征抽取器），应用到后续的其他CV任务了。这篇论文里，作者对比了基于100k无标签的视频无监督预训练CNN，和基于ImageNet千万级的监督数据预训练CNN，在后续的CV任务中取得了很接近的效果。\u003C\u002Fp\u003E\u003Cp data-pid=\"TkFqar7T\"\u003E\u003Cb\u003E[2] 2015 (ICCV) Unsupervised Visual Representation Learning by Context Prediction  \u003C\u002Fb\u003E\u003C\u002Fp\u003E\u003Cp data-pid=\"YFmTnrCx\"\u003EICCV 2015这篇论文里，构造训练数据的方法是：随机从图片采样一个patch，然后从它的邻居里随机采样一个patch，监督标签对应是邻居的位置信息。作者认为，准确地预测两个patch的位置关系，需要模型学会物体整体和部分的关系。\u003C\u002Fp\u003E\u003Cfigure data-size=\"normal\"\u003E\u003Cnoscript\u003E\u003Cimg src=\"https:\u002F\u002Fpic3.zhimg.com\u002Fv2-c7c466638b869d8dc0e6cb71b646fc7e_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"834\" data-rawheight=\"688\" class=\"origin_image zh-lightbox-thumb\" width=\"834\" data-original=\"https:\u002F\u002Fpic3.zhimg.com\u002Fv2-c7c466638b869d8dc0e6cb71b646fc7e_r.jpg\"\u002F\u003E\u003C\u002Fnoscript\u003E\u003Cimg src=\"data:image\u002Fsvg+xml;utf8,&lt;svg xmlns=&#39;http:\u002F\u002Fwww.w3.org\u002F2000\u002Fsvg&#39; width=&#39;834&#39; height=&#39;688&#39;&gt;&lt;\u002Fsvg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"834\" data-rawheight=\"688\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"834\" data-original=\"https:\u002F\u002Fpic3.zhimg.com\u002Fv2-c7c466638b869d8dc0e6cb71b646fc7e_r.jpg\" data-actualsrc=\"https:\u002F\u002Fpic3.zhimg.com\u002Fv2-c7c466638b869d8dc0e6cb71b646fc7e_b.jpg\"\u002F\u003E\u003C\u002Ffigure\u003E\u003Cp data-pid=\"77mSgjRs\"\u003E\u003Cb\u003E[3] 2016 (ECCV) Unsupervised learning of visual representations by solving jigsaw puzzles\u003C\u002Fb\u003E\u003C\u002Fp\u003E\u003Cp data-pid=\"W5h4PEiB\"\u003EEECV 2016这篇论文里，自监督的方法是：学习解决Jigsaw Puzzles（拼图）问题。随机打乱图片位置，学习恢复拼图，即生成原有图片的顺序。\u003C\u002Fp\u003E\u003Cfigure data-size=\"normal\"\u003E\u003Cnoscript\u003E\u003Cimg src=\"https:\u002F\u002Fpic3.zhimg.com\u002Fv2-abf7f22c1cc68fdb02c5759d851b08ea_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"2106\" data-rawheight=\"742\" class=\"origin_image zh-lightbox-thumb\" width=\"2106\" data-original=\"https:\u002F\u002Fpic3.zhimg.com\u002Fv2-abf7f22c1cc68fdb02c5759d851b08ea_r.jpg\"\u002F\u003E\u003C\u002Fnoscript\u003E\u003Cimg src=\"data:image\u002Fsvg+xml;utf8,&lt;svg xmlns=&#39;http:\u002F\u002Fwww.w3.org\u002F2000\u002Fsvg&#39; width=&#39;2106&#39; height=&#39;742&#39;&gt;&lt;\u002Fsvg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"2106\" data-rawheight=\"742\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"2106\" data-original=\"https:\u002F\u002Fpic3.zhimg.com\u002Fv2-abf7f22c1cc68fdb02c5759d851b08ea_r.jpg\" data-actualsrc=\"https:\u002F\u002Fpic3.zhimg.com\u002Fv2-abf7f22c1cc68fdb02c5759d851b08ea_b.jpg\"\u002F\u003E\u003C\u002Ffigure\u003E\u003Cp class=\"ztext-empty-paragraph\"\u003E\u003Cbr\u002F\u003E\u003C\u002Fp\u003E\u003Cp data-pid=\"NS_fYMZt\"\u003E\u003Cb\u003E[4] 2016 (CVPR) Deepak Pathak et al. Context Encoders: Feature Learning by Inpainting\u003C\u002Fb\u003E\u003C\u002Fp\u003E\u003Cp data-pid=\"-TvKhRWi\"\u003ECVPR 2016这篇论文里，自监督的方法是：学习恢复图片缺失部分。\u003C\u002Fp\u003E\u003Cfigure data-size=\"normal\"\u003E\u003Cnoscript\u003E\u003Cimg src=\"https:\u002F\u002Fpic1.zhimg.com\u002Fv2-ddf4e4c71bb113c22ce9ff2e35b9b9b4_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"950\" data-rawheight=\"1068\" class=\"origin_image zh-lightbox-thumb\" width=\"950\" data-original=\"https:\u002F\u002Fpic1.zhimg.com\u002Fv2-ddf4e4c71bb113c22ce9ff2e35b9b9b4_r.jpg\"\u002F\u003E\u003C\u002Fnoscript\u003E\u003Cimg src=\"data:image\u002Fsvg+xml;utf8,&lt;svg xmlns=&#39;http:\u002F\u002Fwww.w3.org\u002F2000\u002Fsvg&#39; width=&#39;950&#39; height=&#39;1068&#39;&gt;&lt;\u002Fsvg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"950\" data-rawheight=\"1068\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"950\" data-original=\"https:\u002F\u002Fpic1.zhimg.com\u002Fv2-ddf4e4c71bb113c22ce9ff2e35b9b9b4_r.jpg\" data-actualsrc=\"https:\u002F\u002Fpic1.zhimg.com\u002Fv2-ddf4e4c71bb113c22ce9ff2e35b9b9b4_b.jpg\"\u002F\u003E\u003C\u002Ffigure\u003E\u003Cp data-pid=\"pZiUC5RH\"\u003E\u003Cb\u003E[5] 2016 (ECCV) A Colorful image colorization\u003C\u002Fb\u003E\u003C\u002Fp\u003E\u003Cp data-pid=\"MY9MPLkd\"\u003EECCV 2016这篇论文里，自监督的方法是：由黑白图片生成彩色图片。\u003C\u002Fp\u003E\u003Cfigure data-size=\"normal\"\u003E\u003Cnoscript\u003E\u003Cimg src=\"https:\u002F\u002Fpic3.zhimg.com\u002Fv2-2b09014fac74adb83b5f433dc091e0e2_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"2136\" data-rawheight=\"922\" class=\"origin_image zh-lightbox-thumb\" width=\"2136\" data-original=\"https:\u002F\u002Fpic3.zhimg.com\u002Fv2-2b09014fac74adb83b5f433dc091e0e2_r.jpg\"\u002F\u003E\u003C\u002Fnoscript\u003E\u003Cimg src=\"data:image\u002Fsvg+xml;utf8,&lt;svg xmlns=&#39;http:\u002F\u002Fwww.w3.org\u002F2000\u002Fsvg&#39; width=&#39;2136&#39; height=&#39;922&#39;&gt;&lt;\u002Fsvg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"2136\" data-rawheight=\"922\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"2136\" data-original=\"https:\u002F\u002Fpic3.zhimg.com\u002Fv2-2b09014fac74adb83b5f433dc091e0e2_r.jpg\" data-actualsrc=\"https:\u002F\u002Fpic3.zhimg.com\u002Fv2-2b09014fac74adb83b5f433dc091e0e2_b.jpg\"\u002F\u003E\u003C\u002Ffigure\u003E\u003Cp data-pid=\"WdLNNc8H\"\u003E\u003Cb\u003E[6] 2017 (CVPR) Split-Brain Autoencoders: Unsupervised Learning by Cross-Channel Prediction\u003C\u002Fb\u003E\u003C\u002Fp\u003E\u003Cp data-pid=\"24Bq98H5\"\u003ECVPR 2017这篇论文里，学习使用输入的一个channel取预测另一个channel。\u003C\u002Fp\u003E\u003Cfigure data-size=\"normal\"\u003E\u003Cnoscript\u003E\u003Cimg src=\"https:\u002F\u002Fpic3.zhimg.com\u002Fv2-abff632b6a22e977050018ed611b1b12_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"722\" data-rawheight=\"516\" class=\"origin_image zh-lightbox-thumb\" width=\"722\" data-original=\"https:\u002F\u002Fpic3.zhimg.com\u002Fv2-abff632b6a22e977050018ed611b1b12_r.jpg\"\u002F\u003E\u003C\u002Fnoscript\u003E\u003Cimg src=\"data:image\u002Fsvg+xml;utf8,&lt;svg xmlns=&#39;http:\u002F\u002Fwww.w3.org\u002F2000\u002Fsvg&#39; width=&#39;722&#39; height=&#39;516&#39;&gt;&lt;\u002Fsvg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"722\" data-rawheight=\"516\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"722\" data-original=\"https:\u002F\u002Fpic3.zhimg.com\u002Fv2-abff632b6a22e977050018ed611b1b12_r.jpg\" data-actualsrc=\"https:\u002F\u002Fpic3.zhimg.com\u002Fv2-abff632b6a22e977050018ed611b1b12_b.jpg\"\u002F\u003E\u003C\u002Ffigure\u003E\u003Cfigure data-size=\"normal\"\u003E\u003Cnoscript\u003E\u003Cimg src=\"https:\u002F\u002Fpic2.zhimg.com\u002Fv2-b5bab5b6cc331e6470f9b2b78a12f365_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1904\" data-rawheight=\"570\" class=\"origin_image zh-lightbox-thumb\" width=\"1904\" data-original=\"https:\u002F\u002Fpic2.zhimg.com\u002Fv2-b5bab5b6cc331e6470f9b2b78a12f365_r.jpg\"\u002F\u003E\u003C\u002Fnoscript\u003E\u003Cimg src=\"data:image\u002Fsvg+xml;utf8,&lt;svg xmlns=&#39;http:\u002F\u002Fwww.w3.org\u002F2000\u002Fsvg&#39; width=&#39;1904&#39; height=&#39;570&#39;&gt;&lt;\u002Fsvg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1904\" data-rawheight=\"570\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1904\" data-original=\"https:\u002F\u002Fpic2.zhimg.com\u002Fv2-b5bab5b6cc331e6470f9b2b78a12f365_r.jpg\" data-actualsrc=\"https:\u002F\u002Fpic2.zhimg.com\u002Fv2-b5bab5b6cc331e6470f9b2b78a12f365_b.jpg\"\u002F\u003E\u003C\u002Ffigure\u003E\u003Cp data-pid=\"wM61Z5PR\"\u003E\u003Cb\u003E[7] 2018 (ICLR) Unsupervised Representation Learning by Predicting Image Rotations\u003C\u002Fb\u003E\u003C\u002Fp\u003E\u003Cp data-pid=\"SWCUd5DX\"\u003E2018 ICLR这篇论文里，采取的自监督的方法是：预测图片的旋转角度。\u003C\u002Fp\u003E\u003Cfigure data-size=\"normal\"\u003E\u003Cnoscript\u003E\u003Cimg src=\"https:\u002F\u002Fpic4.zhimg.com\u002Fv2-2dc667812617528ce1950f8b9f38bb3b_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1434\" data-rawheight=\"914\" class=\"origin_image zh-lightbox-thumb\" width=\"1434\" data-original=\"https:\u002F\u002Fpic4.zhimg.com\u002Fv2-2dc667812617528ce1950f8b9f38bb3b_r.jpg\"\u002F\u003E\u003C\u002Fnoscript\u003E\u003Cimg src=\"data:image\u002Fsvg+xml;utf8,&lt;svg xmlns=&#39;http:\u002F\u002Fwww.w3.org\u002F2000\u002Fsvg&#39; width=&#39;1434&#39; height=&#39;914&#39;&gt;&lt;\u002Fsvg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1434\" data-rawheight=\"914\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1434\" data-original=\"https:\u002F\u002Fpic4.zhimg.com\u002Fv2-2dc667812617528ce1950f8b9f38bb3b_r.jpg\" data-actualsrc=\"https:\u002F\u002Fpic4.zhimg.com\u002Fv2-2dc667812617528ce1950f8b9f38bb3b_b.jpg\"\u002F\u003E\u003C\u002Ffigure\u003E\u003Cp data-pid=\"ZmGz1G-n\"\u003E\u003Cb\u003E[8] 2017 (ICCV) Unsupervised Representation Learning by Sorting Sequences\u003C\u002Fb\u003E\u003C\u002Fp\u003E\u003Cp data-pid=\"Q7ePcz3K\"\u003EICCV 2017 这篇论文里，采取的自监督的方法是：随机打乱视频帧，然后学习对它们排序。\u003C\u002Fp\u003E\u003Cfigure data-size=\"normal\"\u003E\u003Cnoscript\u003E\u003Cimg src=\"https:\u002F\u002Fpic3.zhimg.com\u002Fv2-1d83a09ddf08a58728cc3ee724d63f1a_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1132\" data-rawheight=\"686\" class=\"origin_image zh-lightbox-thumb\" width=\"1132\" data-original=\"https:\u002F\u002Fpic3.zhimg.com\u002Fv2-1d83a09ddf08a58728cc3ee724d63f1a_r.jpg\"\u002F\u003E\u003C\u002Fnoscript\u003E\u003Cimg src=\"data:image\u002Fsvg+xml;utf8,&lt;svg xmlns=&#39;http:\u002F\u002Fwww.w3.org\u002F2000\u002Fsvg&#39; width=&#39;1132&#39; height=&#39;686&#39;&gt;&lt;\u002Fsvg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1132\" data-rawheight=\"686\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1132\" data-original=\"https:\u002F\u002Fpic3.zhimg.com\u002Fv2-1d83a09ddf08a58728cc3ee724d63f1a_r.jpg\" data-actualsrc=\"https:\u002F\u002Fpic3.zhimg.com\u002Fv2-1d83a09ddf08a58728cc3ee724d63f1a_b.jpg\"\u002F\u003E\u003C\u002Ffigure\u003E\u003Cp data-pid=\"Vwm8hpfz\"\u003E\u003Cb\u003E[9] 2018 (Google) (ICRA) Time-Contrastive Networks: Self-Supervised Learning from Video\u003C\u002Fb\u003E\u003C\u002Fp\u003E\u003Cp data-pid=\"N71M5V8v\"\u003EICRA 2018这篇论文里，采取的自监督的方法是：从视频里取一帧，然后选临近的帧作为正例，随机的离得远的帧作为负例，来学习DNN网络，然后结合强化学习，应用到机器人控制。\u003C\u002Fp\u003E\u003Cfigure data-size=\"normal\"\u003E\u003Cnoscript\u003E\u003Cimg src=\"https:\u002F\u002Fpic4.zhimg.com\u002Fv2-bb5bf678605cbde87c04d840ca81ed9b_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1204\" data-rawheight=\"976\" class=\"origin_image zh-lightbox-thumb\" width=\"1204\" data-original=\"https:\u002F\u002Fpic4.zhimg.com\u002Fv2-bb5bf678605cbde87c04d840ca81ed9b_r.jpg\"\u002F\u003E\u003C\u002Fnoscript\u003E\u003Cimg src=\"data:image\u002Fsvg+xml;utf8,&lt;svg xmlns=&#39;http:\u002F\u002Fwww.w3.org\u002F2000\u002Fsvg&#39; width=&#39;1204&#39; height=&#39;976&#39;&gt;&lt;\u002Fsvg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1204\" data-rawheight=\"976\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1204\" data-original=\"https:\u002F\u002Fpic4.zhimg.com\u002Fv2-bb5bf678605cbde87c04d840ca81ed9b_r.jpg\" data-actualsrc=\"https:\u002F\u002Fpic4.zhimg.com\u002Fv2-bb5bf678605cbde87c04d840ca81ed9b_b.jpg\"\u002F\u003E\u003C\u002Ffigure\u003E\u003Cfigure data-size=\"normal\"\u003E\u003Cnoscript\u003E\u003Cimg src=\"https:\u002F\u002Fpic4.zhimg.com\u002Fv2-75b6f0bf1e724c89210e9febd8fa9f43_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1224\" data-rawheight=\"836\" class=\"origin_image zh-lightbox-thumb\" width=\"1224\" data-original=\"https:\u002F\u002Fpic4.zhimg.com\u002Fv2-75b6f0bf1e724c89210e9febd8fa9f43_r.jpg\"\u002F\u003E\u003C\u002Fnoscript\u003E\u003Cimg src=\"data:image\u002Fsvg+xml;utf8,&lt;svg xmlns=&#39;http:\u002F\u002Fwww.w3.org\u002F2000\u002Fsvg&#39; width=&#39;1224&#39; height=&#39;836&#39;&gt;&lt;\u002Fsvg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1224\" data-rawheight=\"836\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1224\" data-original=\"https:\u002F\u002Fpic4.zhimg.com\u002Fv2-75b6f0bf1e724c89210e9febd8fa9f43_r.jpg\" data-actualsrc=\"https:\u002F\u002Fpic4.zhimg.com\u002Fv2-75b6f0bf1e724c89210e9febd8fa9f43_b.jpg\"\u002F\u003E\u003C\u002Ffigure\u003E\u003Cp data-pid=\"JC6ArrR2\"\u003E\u003Cb\u003E[10] 2018 (DeepMind) (Arxiv) (CPC) Representation Learning with Contrastive Predictive Coding\u003C\u002Fb\u003E\u003C\u002Fp\u003E\u003Cp data-pid=\"zXKWSNzo\"\u003E2018年DeepMind提出CPC，基本的思想是context信息能用来预测target的原因是，context的high-level表示是自回归依赖的。所以可以对输入x通过编码器encode到high-level表示，然后在high-level表示层学习出 \u003Cimg src=\"https:\u002F\u002Fwww.zhihu.com\u002Fequation?tex=c_t\" alt=\"c_t\" eeimg=\"1\"\u002F\u003E ，用来进行预测。\u003C\u002Fp\u003E\u003Cfigure data-size=\"normal\"\u003E\u003Cnoscript\u003E\u003Cimg src=\"https:\u002F\u002Fpic4.zhimg.com\u002Fv2-9fdf0416f48c779add2c77cbf3de10cf_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1380\" data-rawheight=\"572\" class=\"origin_image zh-lightbox-thumb\" width=\"1380\" data-original=\"https:\u002F\u002Fpic4.zhimg.com\u002Fv2-9fdf0416f48c779add2c77cbf3de10cf_r.jpg\"\u002F\u003E\u003C\u002Fnoscript\u003E\u003Cimg src=\"data:image\u002Fsvg+xml;utf8,&lt;svg xmlns=&#39;http:\u002F\u002Fwww.w3.org\u002F2000\u002Fsvg&#39; width=&#39;1380&#39; height=&#39;572&#39;&gt;&lt;\u002Fsvg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1380\" data-rawheight=\"572\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1380\" data-original=\"https:\u002F\u002Fpic4.zhimg.com\u002Fv2-9fdf0416f48c779add2c77cbf3de10cf_r.jpg\" data-actualsrc=\"https:\u002F\u002Fpic4.zhimg.com\u002Fv2-9fdf0416f48c779add2c77cbf3de10cf_b.jpg\"\u002F\u003E\u003C\u002Ffigure\u003E\u003Cp data-pid=\"NkuGDl5k\"\u003E优化的目标是x和c的Mutual Information最大：\u003C\u002Fp\u003E\u003Cfigure data-size=\"normal\"\u003E\u003Cnoscript\u003E\u003Cimg src=\"https:\u002F\u002Fpic3.zhimg.com\u002Fv2-cabb9f9d86acdb37f47f9cf4184c66ae_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"462\" data-rawheight=\"118\" class=\"origin_image zh-lightbox-thumb\" width=\"462\" data-original=\"https:\u002F\u002Fpic3.zhimg.com\u002Fv2-cabb9f9d86acdb37f47f9cf4184c66ae_r.jpg\"\u002F\u003E\u003C\u002Fnoscript\u003E\u003Cimg src=\"data:image\u002Fsvg+xml;utf8,&lt;svg xmlns=&#39;http:\u002F\u002Fwww.w3.org\u002F2000\u002Fsvg&#39; width=&#39;462&#39; height=&#39;118&#39;&gt;&lt;\u002Fsvg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"462\" data-rawheight=\"118\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"462\" data-original=\"https:\u002F\u002Fpic3.zhimg.com\u002Fv2-cabb9f9d86acdb37f47f9cf4184c66ae_r.jpg\" data-actualsrc=\"https:\u002F\u002Fpic3.zhimg.com\u002Fv2-cabb9f9d86acdb37f47f9cf4184c66ae_b.jpg\"\u002F\u003E\u003C\u002Ffigure\u003E\u003Cp data-pid=\"8uEY9jCC\"\u003E直接通过生成模型用 \u003Cimg src=\"https:\u002F\u002Fwww.zhihu.com\u002Fequation?tex=c_t\" alt=\"c_t\" eeimg=\"1\"\u002F\u003E 预测未来的 \u003Cimg src=\"https:\u002F\u002Fwww.zhihu.com\u002Fequation?tex=x_%7Bt%2Bk%7D\" alt=\"x_{t+k}\" eeimg=\"1\"\u002F\u003E 比较困难， 所以作者定义了概率密度来保持 \u003Cimg src=\"https:\u002F\u002Fwww.zhihu.com\u002Fequation?tex=x_%7Bt%2Bk%7D\" alt=\"x_{t+k}\" eeimg=\"1\"\u002F\u003E 和 \u003Cimg src=\"https:\u002F\u002Fwww.zhihu.com\u002Fequation?tex=c_t\" alt=\"c_t\" eeimg=\"1\"\u002F\u003E 的互信息：\u003C\u002Fp\u003E\u003Cfigure data-size=\"normal\"\u003E\u003Cnoscript\u003E\u003Cimg src=\"https:\u002F\u002Fpic3.zhimg.com\u002Fv2-5b48479e4efa988eecd1bf873600fb0e_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"420\" data-rawheight=\"128\" class=\"content_image\" width=\"420\"\u002F\u003E\u003C\u002Fnoscript\u003E\u003Cimg src=\"data:image\u002Fsvg+xml;utf8,&lt;svg xmlns=&#39;http:\u002F\u002Fwww.w3.org\u002F2000\u002Fsvg&#39; width=&#39;420&#39; height=&#39;128&#39;&gt;&lt;\u002Fsvg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"420\" data-rawheight=\"128\" class=\"content_image lazy\" width=\"420\" data-actualsrc=\"https:\u002F\u002Fpic3.zhimg.com\u002Fv2-5b48479e4efa988eecd1bf873600fb0e_b.jpg\"\u002F\u003E\u003C\u002Ffigure\u003E\u003Cp data-pid=\"UwiDLtxk\"\u003E作者采用的公式是：\u003C\u002Fp\u003E\u003Cfigure data-size=\"normal\"\u003E\u003Cnoscript\u003E\u003Cimg src=\"https:\u002F\u002Fpic4.zhimg.com\u002Fv2-3cc478a1f3cbc7b72115a02f48a912df_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"516\" data-rawheight=\"106\" class=\"origin_image zh-lightbox-thumb\" width=\"516\" data-original=\"https:\u002F\u002Fpic4.zhimg.com\u002Fv2-3cc478a1f3cbc7b72115a02f48a912df_r.jpg\"\u002F\u003E\u003C\u002Fnoscript\u003E\u003Cimg src=\"data:image\u002Fsvg+xml;utf8,&lt;svg xmlns=&#39;http:\u002F\u002Fwww.w3.org\u002F2000\u002Fsvg&#39; width=&#39;516&#39; height=&#39;106&#39;&gt;&lt;\u002Fsvg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"516\" data-rawheight=\"106\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"516\" data-original=\"https:\u002F\u002Fpic4.zhimg.com\u002Fv2-3cc478a1f3cbc7b72115a02f48a912df_r.jpg\" data-actualsrc=\"https:\u002F\u002Fpic4.zhimg.com\u002Fv2-3cc478a1f3cbc7b72115a02f48a912df_b.jpg\"\u002F\u003E\u003C\u002Ffigure\u003E\u003Cp data-pid=\"3GOP7JgK\"\u003E作者使用InfoNCE loss来优化，使Mutual Information最大：\u003C\u002Fp\u003E\u003Cfigure data-size=\"normal\"\u003E\u003Cnoscript\u003E\u003Cimg src=\"https:\u002F\u002Fpic1.zhimg.com\u002Fv2-57b52a5645eb083714a303e4e8dfad6c_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"570\" data-rawheight=\"134\" class=\"origin_image zh-lightbox-thumb\" width=\"570\" data-original=\"https:\u002F\u002Fpic1.zhimg.com\u002Fv2-57b52a5645eb083714a303e4e8dfad6c_r.jpg\"\u002F\u003E\u003C\u002Fnoscript\u003E\u003Cimg src=\"data:image\u002Fsvg+xml;utf8,&lt;svg xmlns=&#39;http:\u002F\u002Fwww.w3.org\u002F2000\u002Fsvg&#39; width=&#39;570&#39; height=&#39;134&#39;&gt;&lt;\u002Fsvg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"570\" data-rawheight=\"134\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"570\" data-original=\"https:\u002F\u002Fpic1.zhimg.com\u002Fv2-57b52a5645eb083714a303e4e8dfad6c_r.jpg\" data-actualsrc=\"https:\u002F\u002Fpic1.zhimg.com\u002Fv2-57b52a5645eb083714a303e4e8dfad6c_b.jpg\"\u002F\u003E\u003C\u002Ffigure\u003E\u003Cp data-pid=\"dMTk1-GC\"\u003E其中X为N个随机sample，包含1个正例和N-1个负例。\u003C\u002Fp\u003E\u003Cp data-pid=\"egka5e9K\"\u003E\u003Cb\u003E[11] 2019 (ICLR) [DIM] Learning deep representations by mutual information estimation and maximization\u003C\u002Fb\u003E\u003C\u002Fp\u003E\u003Cfigure data-size=\"normal\"\u003E\u003Cnoscript\u003E\u003Cimg src=\"https:\u002F\u002Fpic1.zhimg.com\u002Fv2-f80bc359f9c6aa8b5602c21b5e2a9850_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1378\" data-rawheight=\"830\" class=\"origin_image zh-lightbox-thumb\" width=\"1378\" data-original=\"https:\u002F\u002Fpic1.zhimg.com\u002Fv2-f80bc359f9c6aa8b5602c21b5e2a9850_r.jpg\"\u002F\u003E\u003C\u002Fnoscript\u003E\u003Cimg src=\"data:image\u002Fsvg+xml;utf8,&lt;svg xmlns=&#39;http:\u002F\u002Fwww.w3.org\u002F2000\u002Fsvg&#39; width=&#39;1378&#39; height=&#39;830&#39;&gt;&lt;\u002Fsvg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1378\" data-rawheight=\"830\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1378\" data-original=\"https:\u002F\u002Fpic1.zhimg.com\u002Fv2-f80bc359f9c6aa8b5602c21b5e2a9850_r.jpg\" data-actualsrc=\"https:\u002F\u002Fpic1.zhimg.com\u002Fv2-f80bc359f9c6aa8b5602c21b5e2a9850_b.jpg\"\u002F\u003E\u003C\u002Ffigure\u003E\u003Cp data-pid=\"fjv6U_sq\"\u003E\u003Cb\u003E[12] 2020 (Hinton) (ICML) [SimCLR] A Simple Framework for Contrastive Learning of Visual Representations\u003C\u002Fb\u003E\u003C\u002Fp\u003E\u003Cp data-pid=\"h6skrCQ4\"\u003EHinton等人，在这篇论文里提出SimCLR:\u003C\u002Fp\u003E\u003Cp data-pid=\"VFdxxa_7\"\u003E对一个输入x，首先通过两次独立的数据增强生成两个样本，然后分别通过编码器，再分别通过投影:\u003C\u002Fp\u003E\u003Cfigure data-size=\"normal\"\u003E\u003Cnoscript\u003E\u003Cimg src=\"https:\u002F\u002Fpic1.zhimg.com\u002Fv2-90ecb20682b34725d86ad3e32a5745b8_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"710\" data-rawheight=\"688\" class=\"origin_image zh-lightbox-thumb\" width=\"710\" data-original=\"https:\u002F\u002Fpic1.zhimg.com\u002Fv2-90ecb20682b34725d86ad3e32a5745b8_r.jpg\"\u002F\u003E\u003C\u002Fnoscript\u003E\u003Cimg src=\"data:image\u002Fsvg+xml;utf8,&lt;svg xmlns=&#39;http:\u002F\u002Fwww.w3.org\u002F2000\u002Fsvg&#39; width=&#39;710&#39; height=&#39;688&#39;&gt;&lt;\u002Fsvg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"710\" data-rawheight=\"688\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"710\" data-original=\"https:\u002F\u002Fpic1.zhimg.com\u002Fv2-90ecb20682b34725d86ad3e32a5745b8_r.jpg\" data-actualsrc=\"https:\u002F\u002Fpic1.zhimg.com\u002Fv2-90ecb20682b34725d86ad3e32a5745b8_b.jpg\"\u002F\u003E\u003C\u002Ffigure\u003E\u003Cp data-pid=\"1BF7aWzJ\"\u003E最后在投影空间定义对比loss，其中负样本从batch中的其他图片：\u003C\u002Fp\u003E\u003Cfigure data-size=\"normal\"\u003E\u003Cnoscript\u003E\u003Cimg src=\"https:\u002F\u002Fpic4.zhimg.com\u002Fv2-0ba303520713a355d4f660b0b1b2f67f_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"566\" data-rawheight=\"92\" class=\"origin_image zh-lightbox-thumb\" width=\"566\" data-original=\"https:\u002F\u002Fpic4.zhimg.com\u002Fv2-0ba303520713a355d4f660b0b1b2f67f_r.jpg\"\u002F\u003E\u003C\u002Fnoscript\u003E\u003Cimg src=\"data:image\u002Fsvg+xml;utf8,&lt;svg xmlns=&#39;http:\u002F\u002Fwww.w3.org\u002F2000\u002Fsvg&#39; width=&#39;566&#39; height=&#39;92&#39;&gt;&lt;\u002Fsvg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"566\" data-rawheight=\"92\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"566\" data-original=\"https:\u002F\u002Fpic4.zhimg.com\u002Fv2-0ba303520713a355d4f660b0b1b2f67f_r.jpg\" data-actualsrc=\"https:\u002F\u002Fpic4.zhimg.com\u002Fv2-0ba303520713a355d4f660b0b1b2f67f_b.jpg\"\u002F\u003E\u003C\u002Ffigure\u003E\u003Cp class=\"ztext-empty-paragraph\"\u003E\u003Cbr\u002F\u003E\u003C\u002Fp\u003E\u003Cp data-pid=\"p1G2lbzD\"\u003E\u003Cb\u003E[13] 2020 (Hinton) (Arxiv) Big Self-Supervised Models are Strong Semi-Supervised Learners\u003C\u002Fb\u003E\u003C\u002Fp\u003E\u003Cp data-pid=\"gHZi-U0z\"\u003EHinton等人在这篇论文里，首先在大量无监督数据上自监督学习task-agnostic的encoder，然后基于少量的label数据finetune，再通过蒸馏技术，利用无监督的标签或有标签数据，学习得到更强的task-specific的encoder，对面向具体任务的迁移学习很有指导意义。\u003C\u002Fp\u003E\u003Cfigure data-size=\"normal\"\u003E\u003Cnoscript\u003E\u003Cimg src=\"https:\u002F\u002Fpic3.zhimg.com\u002Fv2-3bee785243d01ba30a4b6d7080d78af6_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1374\" data-rawheight=\"734\" class=\"origin_image zh-lightbox-thumb\" width=\"1374\" data-original=\"https:\u002F\u002Fpic3.zhimg.com\u002Fv2-3bee785243d01ba30a4b6d7080d78af6_r.jpg\"\u002F\u003E\u003C\u002Fnoscript\u003E\u003Cimg src=\"data:image\u002Fsvg+xml;utf8,&lt;svg xmlns=&#39;http:\u002F\u002Fwww.w3.org\u002F2000\u002Fsvg&#39; width=&#39;1374&#39; height=&#39;734&#39;&gt;&lt;\u002Fsvg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1374\" data-rawheight=\"734\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1374\" data-original=\"https:\u002F\u002Fpic3.zhimg.com\u002Fv2-3bee785243d01ba30a4b6d7080d78af6_r.jpg\" data-actualsrc=\"https:\u002F\u002Fpic3.zhimg.com\u002Fv2-3bee785243d01ba30a4b6d7080d78af6_b.jpg\"\u002F\u003E\u003C\u002Ffigure\u003E\u003Cp data-pid=\"3YGoFZbE\"\u003EDistillation：\u003C\u002Fp\u003E\u003Cp data-pid=\"T9mEKHL2\"\u003E无标签数据：\u003C\u002Fp\u003E\u003Cfigure data-size=\"normal\"\u003E\u003Cnoscript\u003E\u003Cimg src=\"https:\u002F\u002Fpic1.zhimg.com\u002Fv2-94f03de5a4cad1d61b17b833ec4bd74c_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"940\" data-rawheight=\"168\" class=\"origin_image zh-lightbox-thumb\" width=\"940\" data-original=\"https:\u002F\u002Fpic1.zhimg.com\u002Fv2-94f03de5a4cad1d61b17b833ec4bd74c_r.jpg\"\u002F\u003E\u003C\u002Fnoscript\u003E\u003Cimg src=\"data:image\u002Fsvg+xml;utf8,&lt;svg xmlns=&#39;http:\u002F\u002Fwww.w3.org\u002F2000\u002Fsvg&#39; width=&#39;940&#39; height=&#39;168&#39;&gt;&lt;\u002Fsvg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"940\" data-rawheight=\"168\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"940\" data-original=\"https:\u002F\u002Fpic1.zhimg.com\u002Fv2-94f03de5a4cad1d61b17b833ec4bd74c_r.jpg\" data-actualsrc=\"https:\u002F\u002Fpic1.zhimg.com\u002Fv2-94f03de5a4cad1d61b17b833ec4bd74c_b.jpg\"\u002F\u003E\u003C\u002Ffigure\u003E\u003Cp data-pid=\"QiBTbGgS\"\u003E无标签数据+标签数据：\u003C\u002Fp\u003E\u003Cfigure data-size=\"normal\"\u003E\u003Cnoscript\u003E\u003Cimg src=\"https:\u002F\u002Fpic3.zhimg.com\u002Fv2-dae93d4aae43b79da822931b1ca511d6_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1610\" data-rawheight=\"168\" class=\"origin_image zh-lightbox-thumb\" width=\"1610\" data-original=\"https:\u002F\u002Fpic3.zhimg.com\u002Fv2-dae93d4aae43b79da822931b1ca511d6_r.jpg\"\u002F\u003E\u003C\u002Fnoscript\u003E\u003Cimg src=\"data:image\u002Fsvg+xml;utf8,&lt;svg xmlns=&#39;http:\u002F\u002Fwww.w3.org\u002F2000\u002Fsvg&#39; width=&#39;1610&#39; height=&#39;168&#39;&gt;&lt;\u002Fsvg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1610\" data-rawheight=\"168\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1610\" data-original=\"https:\u002F\u002Fpic3.zhimg.com\u002Fv2-dae93d4aae43b79da822931b1ca511d6_r.jpg\" data-actualsrc=\"https:\u002F\u002Fpic3.zhimg.com\u002Fv2-dae93d4aae43b79da822931b1ca511d6_b.jpg\"\u002F\u003E\u003C\u002Ffigure\u003E\u003Cp data-pid=\"CUddVT9s\"\u003E\u003Cb\u003E[14] 2020 (Google) (Arxiv) Supervised Contrastive Learning\u003C\u002Fb\u003E\u003C\u002Fp\u003E\u003Cp data-pid=\"bDFhif0k\"\u003E一般自监督学习，对每个anchor，一般是一个positive, 多个negative。这篇论文里，提出在学习时，使用多个positive和多个negative一起学习。\u003C\u002Fp\u003E\u003Cfigure data-size=\"normal\"\u003E\u003Cnoscript\u003E\u003Cimg src=\"https:\u002F\u002Fpic3.zhimg.com\u002Fv2-498e318c08bd29174814496e48c169e6_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1330\" data-rawheight=\"616\" class=\"origin_image zh-lightbox-thumb\" width=\"1330\" data-original=\"https:\u002F\u002Fpic3.zhimg.com\u002Fv2-498e318c08bd29174814496e48c169e6_r.jpg\"\u002F\u003E\u003C\u002Fnoscript\u003E\u003Cimg src=\"data:image\u002Fsvg+xml;utf8,&lt;svg xmlns=&#39;http:\u002F\u002Fwww.w3.org\u002F2000\u002Fsvg&#39; width=&#39;1330&#39; height=&#39;616&#39;&gt;&lt;\u002Fsvg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1330\" data-rawheight=\"616\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1330\" data-original=\"https:\u002F\u002Fpic3.zhimg.com\u002Fv2-498e318c08bd29174814496e48c169e6_r.jpg\" data-actualsrc=\"https:\u002F\u002Fpic3.zhimg.com\u002Fv2-498e318c08bd29174814496e48c169e6_b.jpg\"\u002F\u003E\u003C\u002Ffigure\u003E\u003Cp class=\"ztext-empty-paragraph\"\u003E\u003Cbr\u002F\u003E\u003C\u002Fp\u003E\u003Cp data-pid=\"r2LShtuI\"\u003E\u003Cb\u003E[15] 2020 (CVPR) [MoCo] Momentum Contrast for Unsupervised Visual Representation Learning\u003C\u002Fb\u003E\u003C\u002Fp\u003E\u003Cfigure data-size=\"normal\"\u003E\u003Cnoscript\u003E\u003Cimg src=\"https:\u002F\u002Fpic4.zhimg.com\u002Fv2-1e6ee69cf2feea5652c9c9021f565907_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1866\" data-rawheight=\"650\" class=\"origin_image zh-lightbox-thumb\" width=\"1866\" data-original=\"https:\u002F\u002Fpic4.zhimg.com\u002Fv2-1e6ee69cf2feea5652c9c9021f565907_r.jpg\"\u002F\u003E\u003C\u002Fnoscript\u003E\u003Cimg src=\"data:image\u002Fsvg+xml;utf8,&lt;svg xmlns=&#39;http:\u002F\u002Fwww.w3.org\u002F2000\u002Fsvg&#39; width=&#39;1866&#39; height=&#39;650&#39;&gt;&lt;\u002Fsvg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1866\" data-rawheight=\"650\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1866\" data-original=\"https:\u002F\u002Fpic4.zhimg.com\u002Fv2-1e6ee69cf2feea5652c9c9021f565907_r.jpg\" data-actualsrc=\"https:\u002F\u002Fpic4.zhimg.com\u002Fv2-1e6ee69cf2feea5652c9c9021f565907_b.jpg\"\u002F\u003E\u003C\u002Ffigure\u003E\u003Cp data-pid=\"MDbQVv0m\"\u003E\u003Cb\u003E[16] 2020 (DeepMind) (Arxiv) [BYOL] Bootstrap Your Own Latent: A New Approach to Self-Supervised Learning\u003C\u002Fb\u003E\u003C\u002Fp\u003E\u003Cp data-pid=\"qlJilgIA\"\u003E一般自监督学习，需要负样本。这篇论文里，DeepMing的作者提出了BYOL，不需要负样本，也能自监督学习。\u003C\u002Fp\u003E\u003Cfigure data-size=\"normal\"\u003E\u003Cnoscript\u003E\u003Cimg src=\"https:\u002F\u002Fpic1.zhimg.com\u002Fv2-9902c574b5d354e697f13b7190a8ecc0_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1742\" data-rawheight=\"660\" class=\"origin_image zh-lightbox-thumb\" width=\"1742\" data-original=\"https:\u002F\u002Fpic1.zhimg.com\u002Fv2-9902c574b5d354e697f13b7190a8ecc0_r.jpg\"\u002F\u003E\u003C\u002Fnoscript\u003E\u003Cimg src=\"data:image\u002Fsvg+xml;utf8,&lt;svg xmlns=&#39;http:\u002F\u002Fwww.w3.org\u002F2000\u002Fsvg&#39; width=&#39;1742&#39; height=&#39;660&#39;&gt;&lt;\u002Fsvg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1742\" data-rawheight=\"660\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1742\" data-original=\"https:\u002F\u002Fpic1.zhimg.com\u002Fv2-9902c574b5d354e697f13b7190a8ecc0_r.jpg\" data-actualsrc=\"https:\u002F\u002Fpic1.zhimg.com\u002Fv2-9902c574b5d354e697f13b7190a8ecc0_b.jpg\"\u002F\u003E\u003C\u002Ffigure\u003E\u003Cp data-pid=\"bBi0RlPW\"\u003E\u003Cb\u003E2020 (Arxiv) [SimSiam] Exploring Simple Siamese Representation Learning\u003C\u002Fb\u003E\u003C\u002Fp\u003E\u003Cfigure data-size=\"normal\"\u003E\u003Cnoscript\u003E\u003Cimg src=\"https:\u002F\u002Fpic4.zhimg.com\u002Fv2-b673b20abd42ba8e831c1158fffa6bbf_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1030\" data-rawheight=\"748\" class=\"origin_image zh-lightbox-thumb\" width=\"1030\" data-original=\"https:\u002F\u002Fpic4.zhimg.com\u002Fv2-b673b20abd42ba8e831c1158fffa6bbf_r.jpg\"\u002F\u003E\u003C\u002Fnoscript\u003E\u003Cimg src=\"data:image\u002Fsvg+xml;utf8,&lt;svg xmlns=&#39;http:\u002F\u002Fwww.w3.org\u002F2000\u002Fsvg&#39; width=&#39;1030&#39; height=&#39;748&#39;&gt;&lt;\u002Fsvg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1030\" data-rawheight=\"748\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1030\" data-original=\"https:\u002F\u002Fpic4.zhimg.com\u002Fv2-b673b20abd42ba8e831c1158fffa6bbf_r.jpg\" data-actualsrc=\"https:\u002F\u002Fpic4.zhimg.com\u002Fv2-b673b20abd42ba8e831c1158fffa6bbf_b.jpg\"\u002F\u003E\u003C\u002Ffigure\u003E\u003Cp data-pid=\"UOck5lx1\"\u003E\u003Cb\u003E2021 (Arxiv) [BarlowTwins] Barlow Twins: Self-Supervised Learning via Redundancy Reduction\u003C\u002Fb\u003E\u003C\u002Fp\u003E\u003Cfigure data-size=\"normal\"\u003E\u003Cnoscript\u003E\u003Cimg src=\"https:\u002F\u002Fpic3.zhimg.com\u002Fv2-62eda1cb54b747ed1bdb818a68a836ea_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1310\" data-rawheight=\"1222\" class=\"origin_image zh-lightbox-thumb\" width=\"1310\" data-original=\"https:\u002F\u002Fpic3.zhimg.com\u002Fv2-62eda1cb54b747ed1bdb818a68a836ea_r.jpg\"\u002F\u003E\u003C\u002Fnoscript\u003E\u003Cimg src=\"data:image\u002Fsvg+xml;utf8,&lt;svg xmlns=&#39;http:\u002F\u002Fwww.w3.org\u002F2000\u002Fsvg&#39; width=&#39;1310&#39; height=&#39;1222&#39;&gt;&lt;\u002Fsvg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1310\" data-rawheight=\"1222\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1310\" data-original=\"https:\u002F\u002Fpic3.zhimg.com\u002Fv2-62eda1cb54b747ed1bdb818a68a836ea_r.jpg\" data-actualsrc=\"https:\u002F\u002Fpic3.zhimg.com\u002Fv2-62eda1cb54b747ed1bdb818a68a836ea_b.jpg\"\u002F\u003E\u003C\u002Ffigure\u003E\u003Ch2\u003E3.2 Natural Language Processing：\u003C\u002Fh2\u003E\u003Cp data-pid=\"DfLIg2E6\"\u003ENLP领域，包括多个任务：POS tagging, Named-entity recognition, Language Modeling, Word sense disambiguation, Summarization, Sentiment analysis, Text Classification, Relation Extraction, Question Answering, Machine Translation等。这些任务的标注成本很高，因此训练数据规模一般比较小，而无标签的文本数据却是海量的，因此通过自监督学习学习预训练模型，然后应用到下游任务，既是自然、也是必须的，已经成为state-of-the-art的NLP技术。\u003C\u002Fp\u003E\u003Cp data-pid=\"I5TjvLNq\"\u003E▲NLP领域的自监督学习方法包括经典的Word2vec、ELMo、BERT、GPT、GPT3等。\u003C\u002Fp\u003E\u003Cp data-pid=\"pN9faQCm\"\u003E预训练模型，应用到下游任务的方式包括：\u003C\u002Fp\u003E\u003Cp data-pid=\"pjFqiIxG\"\u003E（1）Feature-based: 将训练好的embedding或模型输出，添加到下游任务的输入特征中。\u003C\u002Fp\u003E\u003Cp data-pid=\"-h8jwmpn\"\u003E          例如Word2vec, ELMo\u003C\u002Fp\u003E\u003Cp data-pid=\"9hpCIUa9\"\u003E  (2) Finetune: 预训练的模型基础上，添加输出层，用下游监督数据finetune模型参数。\u003C\u002Fp\u003E\u003Cp data-pid=\"MCYhK5zK\"\u003E          例如BERT，GPT\u003C\u002Fp\u003E\u003Cp data-pid=\"A3lrp-kB\"\u003E\u003Cb\u003E[17] 2013 (Google) (NIPS) [word2vec] Distributed representations of words and phrases and their compositionality\u003C\u002Fb\u003E     [\u003Ca href=\"https:\u002F\u002Flink.zhihu.com\u002F?target=http%3A\u002F\u002Fjalammar.github.io\u002Fillustrated-word2vec\u002F\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\"\u003EWord2vec\u003C\u002Fa\u003E]\u003C\u002Fp\u003E\u003Cp data-pid=\"40kiF71g\"\u003Eword2vec: Skip-gram模型，预测附近的单词。\u003C\u002Fp\u003E\u003Cfigure data-size=\"normal\"\u003E\u003Cnoscript\u003E\u003Cimg src=\"https:\u002F\u002Fpic1.zhimg.com\u002Fv2-e0569daec46370cd2fe921410f20057c_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"576\" data-rawheight=\"682\" class=\"origin_image zh-lightbox-thumb\" width=\"576\" data-original=\"https:\u002F\u002Fpic1.zhimg.com\u002Fv2-e0569daec46370cd2fe921410f20057c_r.jpg\"\u002F\u003E\u003C\u002Fnoscript\u003E\u003Cimg src=\"data:image\u002Fsvg+xml;utf8,&lt;svg xmlns=&#39;http:\u002F\u002Fwww.w3.org\u002F2000\u002Fsvg&#39; width=&#39;576&#39; height=&#39;682&#39;&gt;&lt;\u002Fsvg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"576\" data-rawheight=\"682\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"576\" data-original=\"https:\u002F\u002Fpic1.zhimg.com\u002Fv2-e0569daec46370cd2fe921410f20057c_r.jpg\" data-actualsrc=\"https:\u002F\u002Fpic1.zhimg.com\u002Fv2-e0569daec46370cd2fe921410f20057c_b.jpg\"\u002F\u003E\u003C\u002Ffigure\u003E\u003Cfigure data-size=\"normal\"\u003E\u003Cnoscript\u003E\u003Cimg src=\"https:\u002F\u002Fpic2.zhimg.com\u002Fv2-4ac9f1d8ed2d1254cb82f7b7917fecbd_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"636\" data-rawheight=\"184\" class=\"origin_image zh-lightbox-thumb\" width=\"636\" data-original=\"https:\u002F\u002Fpic2.zhimg.com\u002Fv2-4ac9f1d8ed2d1254cb82f7b7917fecbd_r.jpg\"\u002F\u003E\u003C\u002Fnoscript\u003E\u003Cimg src=\"data:image\u002Fsvg+xml;utf8,&lt;svg xmlns=&#39;http:\u002F\u002Fwww.w3.org\u002F2000\u002Fsvg&#39; width=&#39;636&#39; height=&#39;184&#39;&gt;&lt;\u002Fsvg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"636\" data-rawheight=\"184\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"636\" data-original=\"https:\u002F\u002Fpic2.zhimg.com\u002Fv2-4ac9f1d8ed2d1254cb82f7b7917fecbd_r.jpg\" data-actualsrc=\"https:\u002F\u002Fpic2.zhimg.com\u002Fv2-4ac9f1d8ed2d1254cb82f7b7917fecbd_b.jpg\"\u002F\u003E\u003C\u002Ffigure\u003E\u003Cp class=\"ztext-empty-paragraph\"\u003E\u003Cbr\u002F\u003E\u003C\u002Fp\u003E\u003Cp data-pid=\"HOdeihru\"\u003ENEG loss:\u003C\u002Fp\u003E\u003Cfigure data-size=\"normal\"\u003E\u003Cnoscript\u003E\u003Cimg src=\"https:\u002F\u002Fpic2.zhimg.com\u002Fv2-657feae54fd7a69f02f636fb77243fb5_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"880\" data-rawheight=\"148\" class=\"origin_image zh-lightbox-thumb\" width=\"880\" data-original=\"https:\u002F\u002Fpic2.zhimg.com\u002Fv2-657feae54fd7a69f02f636fb77243fb5_r.jpg\"\u002F\u003E\u003C\u002Fnoscript\u003E\u003Cimg src=\"data:image\u002Fsvg+xml;utf8,&lt;svg xmlns=&#39;http:\u002F\u002Fwww.w3.org\u002F2000\u002Fsvg&#39; width=&#39;880&#39; height=&#39;148&#39;&gt;&lt;\u002Fsvg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"880\" data-rawheight=\"148\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"880\" data-original=\"https:\u002F\u002Fpic2.zhimg.com\u002Fv2-657feae54fd7a69f02f636fb77243fb5_r.jpg\" data-actualsrc=\"https:\u002F\u002Fpic2.zhimg.com\u002Fv2-657feae54fd7a69f02f636fb77243fb5_b.jpg\"\u002F\u003E\u003C\u002Ffigure\u003E\u003Cp data-pid=\"2YfWCjFV\"\u003E\u003Cb\u003E[18] 2017 (Google) (NIPS) [Transformer] Attention is All you Need \u003C\u002Fb\u003E   [\u003Ca href=\"https:\u002F\u002Flink.zhihu.com\u002F?target=http%3A\u002F\u002Fjalammar.github.io\u002Fillustrated-transformer\u002F\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\"\u003ETransformer\u003C\u002Fa\u003E]\u003C\u002Fp\u003E\u003Cp data-pid=\"3pLktBh_\"\u003ETransformer作为目前最强大的序列建模模型，是BERT等预训练模型的基本单元。\u003C\u002Fp\u003E\u003Cp data-pid=\"0Nzc9iVJ\"\u003E\u003Cb\u003E[19] 2018 (ACL) (ELMo) Deep contextualized word representations\u003C\u002Fb\u003E\u003C\u002Fp\u003E\u003Cp data-pid=\"y9SoUXxH\"\u003Eword2vec的skip-gram模型中用一个单词预测附近的一个单词，ELMo模型的思想是使用整个句子学习更好的word embedding，来更好地建模单词的语法、语义、多义问题。\u003C\u002Fp\u003E\u003Cp data-pid=\"7w_XqIM3\"\u003E▲ELMo (Embeddings from Language Models) 模型中，作者使用Deep Bi-LSTM来建模一个句子，使用language model来学习模型参数：\u003C\u002Fp\u003E\u003Cfigure data-size=\"normal\"\u003E\u003Cnoscript\u003E\u003Cimg src=\"https:\u002F\u002Fpic4.zhimg.com\u002Fv2-09f64000144e9a2ae1ee443cdf65cfd7_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"756\" data-rawheight=\"224\" class=\"origin_image zh-lightbox-thumb\" width=\"756\" data-original=\"https:\u002F\u002Fpic4.zhimg.com\u002Fv2-09f64000144e9a2ae1ee443cdf65cfd7_r.jpg\"\u002F\u003E\u003C\u002Fnoscript\u003E\u003Cimg src=\"data:image\u002Fsvg+xml;utf8,&lt;svg xmlns=&#39;http:\u002F\u002Fwww.w3.org\u002F2000\u002Fsvg&#39; width=&#39;756&#39; height=&#39;224&#39;&gt;&lt;\u002Fsvg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"756\" data-rawheight=\"224\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"756\" data-original=\"https:\u002F\u002Fpic4.zhimg.com\u002Fv2-09f64000144e9a2ae1ee443cdf65cfd7_r.jpg\" data-actualsrc=\"https:\u002F\u002Fpic4.zhimg.com\u002Fv2-09f64000144e9a2ae1ee443cdf65cfd7_b.jpg\"\u002F\u003E\u003C\u002Ffigure\u003E\u003Cp data-pid=\"tbiEowIw\"\u003E▲把模型中多层的LSTM隐藏向量结合起来作为word的表示：\u003C\u002Fp\u003E\u003Cfigure data-size=\"normal\"\u003E\u003Cnoscript\u003E\u003Cimg src=\"https:\u002F\u002Fpic2.zhimg.com\u002Fv2-6aa652be7ab12fa4a031f1726162eb45_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"790\" data-rawheight=\"286\" class=\"origin_image zh-lightbox-thumb\" width=\"790\" data-original=\"https:\u002F\u002Fpic2.zhimg.com\u002Fv2-6aa652be7ab12fa4a031f1726162eb45_r.jpg\"\u002F\u003E\u003C\u002Fnoscript\u003E\u003Cimg src=\"data:image\u002Fsvg+xml;utf8,&lt;svg xmlns=&#39;http:\u002F\u002Fwww.w3.org\u002F2000\u002Fsvg&#39; width=&#39;790&#39; height=&#39;286&#39;&gt;&lt;\u002Fsvg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"790\" data-rawheight=\"286\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"790\" data-original=\"https:\u002F\u002Fpic2.zhimg.com\u002Fv2-6aa652be7ab12fa4a031f1726162eb45_r.jpg\" data-actualsrc=\"https:\u002F\u002Fpic2.zhimg.com\u002Fv2-6aa652be7ab12fa4a031f1726162eb45_b.jpg\"\u002F\u003E\u003C\u002Ffigure\u003E\u003Cp data-pid=\"NdiNyf43\"\u003E作者指出word的多层表示中，底层的表示一般包括语法（适合Pos tagging任务）、上层的表示更好地建模context信息（适合问答任务）。\u003C\u002Fp\u003E\u003Cp data-pid=\"eGN3tUOZ\"\u003E▲ELMo中将预训练模型应用到下游任务的方式是feature-based，即将模型输出作为下游任务的特征。通过把这些不同的表示通过权重结合起来，添加到下游任务输入中，来提高下游模型指标：\u003C\u002Fp\u003E\u003Cfigure data-size=\"normal\"\u003E\u003Cnoscript\u003E\u003Cimg src=\"https:\u002F\u002Fpic4.zhimg.com\u002Fv2-b3d99e5bcda9a995cb63d59d2c22eeb7_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"790\" data-rawheight=\"150\" class=\"origin_image zh-lightbox-thumb\" width=\"790\" data-original=\"https:\u002F\u002Fpic4.zhimg.com\u002Fv2-b3d99e5bcda9a995cb63d59d2c22eeb7_r.jpg\"\u002F\u003E\u003C\u002Fnoscript\u003E\u003Cimg src=\"data:image\u002Fsvg+xml;utf8,&lt;svg xmlns=&#39;http:\u002F\u002Fwww.w3.org\u002F2000\u002Fsvg&#39; width=&#39;790&#39; height=&#39;150&#39;&gt;&lt;\u002Fsvg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"790\" data-rawheight=\"150\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"790\" data-original=\"https:\u002F\u002Fpic4.zhimg.com\u002Fv2-b3d99e5bcda9a995cb63d59d2c22eeb7_r.jpg\" data-actualsrc=\"https:\u002F\u002Fpic4.zhimg.com\u002Fv2-b3d99e5bcda9a995cb63d59d2c22eeb7_b.jpg\"\u002F\u003E\u003C\u002Ffigure\u003E\u003Cp data-pid=\"ICy9Om-r\"\u003E\u003Cb\u003E[20] 2018 (Google) (NAACL) [BERT] Bert: Pre-training of deep bidirectional transformers for language understanding. \u003C\u002Fb\u003E [\u003Ca href=\"https:\u002F\u002Flink.zhihu.com\u002F?target=http%3A\u002F\u002Fjalammar.github.io\u002Fa-visual-guide-to-using-bert-for-the-first-time\u002F\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\"\u003EBERT example\u003C\u002Fa\u003E][\u003Ca href=\"https:\u002F\u002Flink.zhihu.com\u002F?target=http%3A\u002F\u002Fjalammar.github.io\u002Fillustrated-bert\u002F\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\"\u003EBERT\u003C\u002Fa\u003E]\u003C\u002Fp\u003E\u003Cp data-pid=\"mk8mQyfq\"\u003EELMo和GPT模型，预训练模型时采用的标准language model是单向的，对需要sentence-level或context-level信息的任务来说是不好的，BERT使用双向的Transformer来解决这个问题。\u003C\u002Fp\u003E\u003Cfigure data-size=\"normal\"\u003E\u003Cnoscript\u003E\u003Cimg src=\"https:\u002F\u002Fpic4.zhimg.com\u002Fv2-ae7b4f30628e63b82b5fa1eb87b736a3_b.jpg\" data-size=\"normal\" data-rawwidth=\"1814\" data-rawheight=\"768\" class=\"origin_image zh-lightbox-thumb\" width=\"1814\" data-original=\"https:\u002F\u002Fpic4.zhimg.com\u002Fv2-ae7b4f30628e63b82b5fa1eb87b736a3_r.jpg\"\u002F\u003E\u003C\u002Fnoscript\u003E\u003Cimg src=\"data:image\u002Fsvg+xml;utf8,&lt;svg xmlns=&#39;http:\u002F\u002Fwww.w3.org\u002F2000\u002Fsvg&#39; width=&#39;1814&#39; height=&#39;768&#39;&gt;&lt;\u002Fsvg&gt;\" data-size=\"normal\" data-rawwidth=\"1814\" data-rawheight=\"768\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1814\" data-original=\"https:\u002F\u002Fpic4.zhimg.com\u002Fv2-ae7b4f30628e63b82b5fa1eb87b736a3_r.jpg\" data-actualsrc=\"https:\u002F\u002Fpic4.zhimg.com\u002Fv2-ae7b4f30628e63b82b5fa1eb87b736a3_b.jpg\"\u002F\u003E\u003Cfigcaption\u003EBERT, GPT, ELMo\u003C\u002Ffigcaption\u003E\u003C\u002Ffigure\u003E\u003Cp data-pid=\"QP_Fu5OA\"\u003EBERT （Bidirectional Encoder Representations from Transformers）:\u003C\u002Fp\u003E\u003Cp data-pid=\"wU9BenWh\"\u003E\u003Cb\u003E▲阶段1 预训练模型:\u003C\u002Fb\u003E\u003C\u002Fp\u003E\u003Cp data-pid=\"OcWG28v4\"\u003E(1) 提出Masked Language Model (MLM)预训练目标，来消除标准language model是单向的限制。MLM通过随机mask掉输入的一部分单词，然后使用句子中的其他context单词（包括左边的和右边的）来预测被mask的部分。从而BERT能够实现深度双向表示学习。\u003C\u002Fp\u003E\u003Cp data-pid=\"oa7NdPse\"\u003E具体来说，构造训练数据时，随机选取输入句子中15%的token位置，对于第i个选取的token， 80%的概率替换为[MASK]，10%的概率替换为随机token， 10%的概率不变。然后用BERT模型输出层在第i个token位置的向量，预测原始的第i个token，使用cross entropy loss优化。\u003C\u002Fp\u003E\u003Cp data-pid=\"25WdggAE\"\u003E(2) 提出Next Sentence Prediction预训练目标: 预测两个句子是否在源数据中是相邻的。\u003C\u002Fp\u003E\u003Cp data-pid=\"YJA_-cx5\"\u003EQA等任务，基于两个句子的关系理解来完，但language model无法建模这些信息。BERT中提出的NSP任务，可以很好的解决这个问题。在构造训练数据时，对于A和B，50%的概率B是A后的下一个句子，50%的概率B是随机的一个句子。使用C来预测这个标签是否真的是下一个句子，使用cross entropy loss优化。\u003C\u002Fp\u003E\u003Cp data-pid=\"lRNJO43x\"\u003EBERT使用BooksCorpus和Wikipedia来预训练。\u003C\u002Fp\u003E\u003Cp data-pid=\"u58wNdFp\"\u003E▲\u003Cb\u003E阶段2 Finetune：\u003C\u002Fb\u003E\u003C\u002Fp\u003E\u003Cp data-pid=\"82hzX-yK\"\u003EBERT通过使用下游任务监督数据finetune所有参数，或将BERT在输入上的结果作为后续模型的输入，在下游多个任务取得很好地效果，已经成为NLP的主流技术。\u003C\u002Fp\u003E\u003Cfigure data-size=\"normal\"\u003E\u003Cnoscript\u003E\u003Cimg src=\"https:\u002F\u002Fpic3.zhimg.com\u002Fv2-d4b69cd199e1cd172d8c0c0083a3725e_b.jpg\" data-size=\"normal\" data-rawwidth=\"1844\" data-rawheight=\"1010\" class=\"origin_image zh-lightbox-thumb\" width=\"1844\" data-original=\"https:\u002F\u002Fpic3.zhimg.com\u002Fv2-d4b69cd199e1cd172d8c0c0083a3725e_r.jpg\"\u002F\u003E\u003C\u002Fnoscript\u003E\u003Cimg src=\"data:image\u002Fsvg+xml;utf8,&lt;svg xmlns=&#39;http:\u002F\u002Fwww.w3.org\u002F2000\u002Fsvg&#39; width=&#39;1844&#39; height=&#39;1010&#39;&gt;&lt;\u002Fsvg&gt;\" data-size=\"normal\" data-rawwidth=\"1844\" data-rawheight=\"1010\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1844\" data-original=\"https:\u002F\u002Fpic3.zhimg.com\u002Fv2-d4b69cd199e1cd172d8c0c0083a3725e_r.jpg\" data-actualsrc=\"https:\u002F\u002Fpic3.zhimg.com\u002Fv2-d4b69cd199e1cd172d8c0c0083a3725e_b.jpg\"\u002F\u003E\u003Cfigcaption\u003EBERT\u003C\u002Ffigcaption\u003E\u003C\u002Ffigure\u003E\u003Cp data-pid=\"ZjkN-TcB\"\u003E▲\u003Cb\u003E具体任务Finetune的设计：\u003C\u002Fb\u003E\u003C\u002Fp\u003E\u003Cfigure data-size=\"normal\"\u003E\u003Cnoscript\u003E\u003Cimg src=\"https:\u002F\u002Fpic3.zhimg.com\u002Fv2-f556aaa650e30145eb032342fdabbece_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1528\" data-rawheight=\"1624\" class=\"origin_image zh-lightbox-thumb\" width=\"1528\" data-original=\"https:\u002F\u002Fpic3.zhimg.com\u002Fv2-f556aaa650e30145eb032342fdabbece_r.jpg\"\u002F\u003E\u003C\u002Fnoscript\u003E\u003Cimg src=\"data:image\u002Fsvg+xml;utf8,&lt;svg xmlns=&#39;http:\u002F\u002Fwww.w3.org\u002F2000\u002Fsvg&#39; width=&#39;1528&#39; height=&#39;1624&#39;&gt;&lt;\u002Fsvg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1528\" data-rawheight=\"1624\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1528\" data-original=\"https:\u002F\u002Fpic3.zhimg.com\u002Fv2-f556aaa650e30145eb032342fdabbece_r.jpg\" data-actualsrc=\"https:\u002F\u002Fpic3.zhimg.com\u002Fv2-f556aaa650e30145eb032342fdabbece_b.jpg\"\u002F\u003E\u003C\u002Ffigure\u003E\u003Cp data-pid=\"OW5TMx2v\"\u003E▲\u003Cb\u003EBERT模型参数：\u003C\u002Fb\u003E\u003C\u002Fp\u003E\u003Cp data-pid=\"ej0d4FF9\"\u003EBERT(base)包括12层，hidden size 768，self-attention head为12，参数量为1.1亿，与GPT参数相当。BERT(large)包括24层，hidden size 1024，self-attention head为16，参数量为3.4亿。\u003C\u002Fp\u003E\u003Cp data-pid=\"VWfumLr6\"\u003E▲\u003Cb\u003EBERT输入：\u003C\u002Fb\u003E\u003C\u002Fp\u003E\u003Cp data-pid=\"RM7jtSZ2\"\u003E为了使BERT预训练后能应用到各种各样的任务中，BERT的输入是一个sentence，可以对应原始的一个句子或句子对。一个&lt;Question, Answer&gt;，可以通过[SEP]连接成一个sentence。\u003C\u002Fp\u003E\u003Cp data-pid=\"EtFNIhn_\"\u003E其中第一个token是[CLS] ，用于汇总整个句子的表示，可以用于句子分类等任务。BERT采用WordPiece embedding, vocabulary size是30000。\u003C\u002Fp\u003E\u003Cp data-pid=\"0AfnvLGK\"\u003E输入序列中每一个单元的表示包括：token embedding, 指示在句子A还是B的segment embedding, Position embedding。\u003C\u002Fp\u003E\u003Cfigure data-size=\"normal\"\u003E\u003Cnoscript\u003E\u003Cimg src=\"https:\u002F\u002Fpic4.zhimg.com\u002Fv2-fa5c5a4582990fc88bb29794d7222d27_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1470\" data-rawheight=\"508\" class=\"origin_image zh-lightbox-thumb\" width=\"1470\" data-original=\"https:\u002F\u002Fpic4.zhimg.com\u002Fv2-fa5c5a4582990fc88bb29794d7222d27_r.jpg\"\u002F\u003E\u003C\u002Fnoscript\u003E\u003Cimg src=\"data:image\u002Fsvg+xml;utf8,&lt;svg xmlns=&#39;http:\u002F\u002Fwww.w3.org\u002F2000\u002Fsvg&#39; width=&#39;1470&#39; height=&#39;508&#39;&gt;&lt;\u002Fsvg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1470\" data-rawheight=\"508\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1470\" data-original=\"https:\u002F\u002Fpic4.zhimg.com\u002Fv2-fa5c5a4582990fc88bb29794d7222d27_r.jpg\" data-actualsrc=\"https:\u002F\u002Fpic4.zhimg.com\u002Fv2-fa5c5a4582990fc88bb29794d7222d27_b.jpg\"\u002F\u003E\u003C\u002Ffigure\u003E\u003Cp data-pid=\"QcRmJQK6\"\u003E\u003Cb\u003E[21] 2018 (OpenAI) (Arxiv) (GPT) Improving Language Understanding by Generative Pre-Training\u003C\u002Fb\u003E\u003C\u002Fp\u003E\u003Cp data-pid=\"ja-2dPPX\"\u003EGPT模型的思想：基于大量无标签数据学习预训练模型，然后通过finetune应用到后续的多个任务。\u003C\u002Fp\u003E\u003Cp data-pid=\"tNNdBOLU\"\u003E▲\u003Cb\u003E阶段1：Unsupervised pre-training\u003C\u002Fb\u003E\u003C\u002Fp\u003E\u003Cp data-pid=\"OJW5v1UV\"\u003E使用language modeling目标来优化：\u003C\u002Fp\u003E\u003Cfigure data-size=\"normal\"\u003E\u003Cnoscript\u003E\u003Cimg src=\"https:\u002F\u002Fpic4.zhimg.com\u002Fv2-7cc105df426f41aa28d9e6f8344227fb_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1750\" data-rawheight=\"340\" class=\"origin_image zh-lightbox-thumb\" width=\"1750\" data-original=\"https:\u002F\u002Fpic4.zhimg.com\u002Fv2-7cc105df426f41aa28d9e6f8344227fb_r.jpg\"\u002F\u003E\u003C\u002Fnoscript\u003E\u003Cimg src=\"data:image\u002Fsvg+xml;utf8,&lt;svg xmlns=&#39;http:\u002F\u002Fwww.w3.org\u002F2000\u002Fsvg&#39; width=&#39;1750&#39; height=&#39;340&#39;&gt;&lt;\u002Fsvg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1750\" data-rawheight=\"340\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1750\" data-original=\"https:\u002F\u002Fpic4.zhimg.com\u002Fv2-7cc105df426f41aa28d9e6f8344227fb_r.jpg\" data-actualsrc=\"https:\u002F\u002Fpic4.zhimg.com\u002Fv2-7cc105df426f41aa28d9e6f8344227fb_b.jpg\"\u002F\u003E\u003C\u002Ffigure\u003E\u003Cp data-pid=\"iaUiiuwL\"\u003E对应的模型结构，作者选取的是multi-layer Transformer Decoder （比Transformer少一半参数）：将输入转化为embedding，经过Masked Multi-head Self-Attention，然后使用softmax层即可预测输出概率分布。\u003C\u002Fp\u003E\u003Cfigure data-size=\"normal\"\u003E\u003Cnoscript\u003E\u003Cimg src=\"https:\u002F\u002Fpic1.zhimg.com\u002Fv2-7b7ad4b3a2f9910602e3ffae324060e4_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1784\" data-rawheight=\"308\" class=\"origin_image zh-lightbox-thumb\" width=\"1784\" data-original=\"https:\u002F\u002Fpic1.zhimg.com\u002Fv2-7b7ad4b3a2f9910602e3ffae324060e4_r.jpg\"\u002F\u003E\u003C\u002Fnoscript\u003E\u003Cimg src=\"data:image\u002Fsvg+xml;utf8,&lt;svg xmlns=&#39;http:\u002F\u002Fwww.w3.org\u002F2000\u002Fsvg&#39; width=&#39;1784&#39; height=&#39;308&#39;&gt;&lt;\u002Fsvg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1784\" data-rawheight=\"308\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1784\" data-original=\"https:\u002F\u002Fpic1.zhimg.com\u002Fv2-7b7ad4b3a2f9910602e3ffae324060e4_r.jpg\" data-actualsrc=\"https:\u002F\u002Fpic1.zhimg.com\u002Fv2-7b7ad4b3a2f9910602e3ffae324060e4_b.jpg\"\u002F\u003E\u003C\u002Ffigure\u003E\u003Cp data-pid=\"4oyN-2zZ\"\u003E▲\u003Cb\u003E阶段2：Supervised fine-tuning\u003C\u002Fb\u003E\u003C\u002Fp\u003E\u003Cp data-pid=\"5AnGzzvj\"\u003E阶段1学习得到的预训练模型，后面连接一个Linear层，即可实现下游任务的预测。\u003C\u002Fp\u003E\u003Cp data-pid=\"u3fW_vvg\"\u003E对于下游任务，按下图的方式，构造模型的输入。例如对于文本分类问题：前后加入特殊字符后变成一个序列，经过预训练模型Transformer，Linear层即可实现预测。\u003C\u002Fp\u003E\u003Cfigure data-size=\"normal\"\u003E\u003Cnoscript\u003E\u003Cimg src=\"https:\u002F\u002Fpic4.zhimg.com\u002Fv2-f6b64178ab4eb125fb6353ff25415c53_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1814\" data-rawheight=\"542\" class=\"origin_image zh-lightbox-thumb\" width=\"1814\" data-original=\"https:\u002F\u002Fpic4.zhimg.com\u002Fv2-f6b64178ab4eb125fb6353ff25415c53_r.jpg\"\u002F\u003E\u003C\u002Fnoscript\u003E\u003Cimg src=\"data:image\u002Fsvg+xml;utf8,&lt;svg xmlns=&#39;http:\u002F\u002Fwww.w3.org\u002F2000\u002Fsvg&#39; width=&#39;1814&#39; height=&#39;542&#39;&gt;&lt;\u002Fsvg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1814\" data-rawheight=\"542\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1814\" data-original=\"https:\u002F\u002Fpic4.zhimg.com\u002Fv2-f6b64178ab4eb125fb6353ff25415c53_r.jpg\" data-actualsrc=\"https:\u002F\u002Fpic4.zhimg.com\u002Fv2-f6b64178ab4eb125fb6353ff25415c53_b.jpg\"\u002F\u003E\u003C\u002Ffigure\u003E\u003Cfigure data-size=\"normal\"\u003E\u003Cnoscript\u003E\u003Cimg src=\"https:\u002F\u002Fpic1.zhimg.com\u002Fv2-f40ca20b84969e13a2f2908ae5c26a98_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1848\" data-rawheight=\"916\" class=\"origin_image zh-lightbox-thumb\" width=\"1848\" data-original=\"https:\u002F\u002Fpic1.zhimg.com\u002Fv2-f40ca20b84969e13a2f2908ae5c26a98_r.jpg\"\u002F\u003E\u003C\u002Fnoscript\u003E\u003Cimg src=\"data:image\u002Fsvg+xml;utf8,&lt;svg xmlns=&#39;http:\u002F\u002Fwww.w3.org\u002F2000\u002Fsvg&#39; width=&#39;1848&#39; height=&#39;916&#39;&gt;&lt;\u002Fsvg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1848\" data-rawheight=\"916\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1848\" data-original=\"https:\u002F\u002Fpic1.zhimg.com\u002Fv2-f40ca20b84969e13a2f2908ae5c26a98_r.jpg\" data-actualsrc=\"https:\u002F\u002Fpic1.zhimg.com\u002Fv2-f40ca20b84969e13a2f2908ae5c26a98_b.jpg\"\u002F\u003E\u003C\u002Ffigure\u003E\u003Cp data-pid=\"OoW9c2oG\"\u003E▲\u003Cb\u003E论文实验结论：\u003C\u002Fb\u003E\u003C\u002Fp\u003E\u003Cp data-pid=\"sjkX0D1Q\"\u003E（1）预训练能大幅提升在多个下游任务的指标。\u003C\u002Fp\u003E\u003Cp data-pid=\"-yIsQDFf\"\u003E（2）预训练模型中的embedding、序列模型各层参数，都有助于提升下游任务。\u003C\u002Fp\u003E\u003Cp data-pid=\"FEqJNB-K\"\u003E（3）Zero-shot Knowledge Transfer: 仅使用预训练模型，不用下游任务数据finetune, 预训练的越多，下游效果越好。\u003C\u002Fp\u003E\u003Cp data-pid=\"3GNQnL8Q\"\u003E（4）在Finetune阶段也添加Language Modeling的辅助任务，有助于提升模型指标。简单理解是，在下游任务的数据上继续pre-training。\u003C\u002Fp\u003E\u003Cp data-pid=\"MZcLcbjL\"\u003E\u003Cb\u003E[22] 2019 (OpenAI) (Arxiv) (GPT-2) Language Models are Unsupervised Multitask Learners\u003C\u002Fb\u003E [\u003Ca href=\"https:\u002F\u002Flink.zhihu.com\u002F?target=http%3A\u002F\u002Fjalammar.github.io\u002Fillustrated-gpt2\u002F\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\"\u003EGPT2\u003C\u002Fa\u003E]\u003C\u002Fp\u003E\u003Cp data-pid=\"lJMjzcdL\"\u003E作者提出的15亿参数的Transformer预训练模型GPT-2，具有很强的Zero-shot Knowledge Transfer能力：在多个任务上，即使不用下游数据finetune，也取得了比用下游数据监督学习的模型更好的表现。\u003C\u002Fp\u003E\u003Cp data-pid=\"gXYSgz9U\"\u003EGPT-2中采用了Byte Pair Encoding (BPE) 来取代character和word embedding。参数量如下：\u003C\u002Fp\u003E\u003Cfigure data-size=\"normal\"\u003E\u003Cnoscript\u003E\u003Cimg src=\"https:\u002F\u002Fpic1.zhimg.com\u002Fv2-e6e32ab4101cc9c4af8c5aa814e6f9b4_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"846\" data-rawheight=\"336\" class=\"origin_image zh-lightbox-thumb\" width=\"846\" data-original=\"https:\u002F\u002Fpic1.zhimg.com\u002Fv2-e6e32ab4101cc9c4af8c5aa814e6f9b4_r.jpg\"\u002F\u003E\u003C\u002Fnoscript\u003E\u003Cimg src=\"data:image\u002Fsvg+xml;utf8,&lt;svg xmlns=&#39;http:\u002F\u002Fwww.w3.org\u002F2000\u002Fsvg&#39; width=&#39;846&#39; height=&#39;336&#39;&gt;&lt;\u002Fsvg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"846\" data-rawheight=\"336\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"846\" data-original=\"https:\u002F\u002Fpic1.zhimg.com\u002Fv2-e6e32ab4101cc9c4af8c5aa814e6f9b4_r.jpg\" data-actualsrc=\"https:\u002F\u002Fpic1.zhimg.com\u002Fv2-e6e32ab4101cc9c4af8c5aa814e6f9b4_b.jpg\"\u002F\u003E\u003C\u002Ffigure\u003E\u003Cp data-pid=\"jOqLyise\"\u003E第一行参数量1.17亿，和GPT相当。第二行参数量3.45亿，和BERT相当。最大的参数量为15.42亿，为GPT-2。\u003C\u002Fp\u003E\u003Cp data-pid=\"yst4f6oM\"\u003E\u003Cb\u003E[23] 2020 (OpenAI) (Arixv) (GPT-3) Language Models are Few-Shot Learners \u003C\u002Fb\u003E[\u003Ca href=\"https:\u002F\u002Flink.zhihu.com\u002F?target=http%3A\u002F\u002Fjalammar.github.io\u002Fhow-gpt3-works-visualizations-animations\u002F\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\"\u003EGPT3\u003C\u002Fa\u003E]\u003C\u002Fp\u003E\u003Cp data-pid=\"ArneHU0h\"\u003EGPT-3采取的模型和GPT-2相同，但参数量有1750亿。\u003C\u002Fp\u003E\u003Cp data-pid=\"XyicKtKX\"\u003E关于参数量和下游任务的性能的分析图如下：\u003C\u002Fp\u003E\u003Cp data-pid=\"DwtsjOy_\"\u003E可以看出：参数量越大，在Zero-shot, One-shot, Few-shot的情况下指标都不断提升。\u003C\u002Fp\u003E\u003Cfigure data-size=\"normal\"\u003E\u003Cnoscript\u003E\u003Cimg src=\"https:\u002F\u002Fpic3.zhimg.com\u002Fv2-0f061d17b37e74389f4f1e2b8f591c72_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1564\" data-rawheight=\"928\" class=\"origin_image zh-lightbox-thumb\" width=\"1564\" data-original=\"https:\u002F\u002Fpic3.zhimg.com\u002Fv2-0f061d17b37e74389f4f1e2b8f591c72_r.jpg\"\u002F\u003E\u003C\u002Fnoscript\u003E\u003Cimg src=\"data:image\u002Fsvg+xml;utf8,&lt;svg xmlns=&#39;http:\u002F\u002Fwww.w3.org\u002F2000\u002Fsvg&#39; width=&#39;1564&#39; height=&#39;928&#39;&gt;&lt;\u002Fsvg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1564\" data-rawheight=\"928\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1564\" data-original=\"https:\u002F\u002Fpic3.zhimg.com\u002Fv2-0f061d17b37e74389f4f1e2b8f591c72_r.jpg\" data-actualsrc=\"https:\u002F\u002Fpic3.zhimg.com\u002Fv2-0f061d17b37e74389f4f1e2b8f591c72_b.jpg\"\u002F\u003E\u003C\u002Ffigure\u003E\u003Cp data-pid=\"F73BUg9g\"\u003E\u003Cb\u003E[24] 2019 (ACL) Self-Supervised Dialogue Learning\u003C\u002Fb\u003E\u003C\u002Fp\u003E\u003Cp data-pid=\"JK-JxbQV\"\u003E这篇论文介绍，自监督学习如何应用到对话中。\u003C\u002Fp\u003E\u003Cp data-pid=\"ldr9dBCI\"\u003E\u003Cb\u003E[25] 2020 (ACL) Don’t Stop Pretraining: Adapt Language Models to Domains and Tasks\u003C\u002Fb\u003E\u003C\u002Fp\u003E\u003Cp data-pid=\"jrc7xGuR\"\u003EACL这篇论文，指出在通用语料上预训练后，继续在具体的任务语料上预训练，可进一步提升任务上的效果。\u003C\u002Fp\u003E\u003Cfigure data-size=\"normal\"\u003E\u003Cnoscript\u003E\u003Cimg src=\"https:\u002F\u002Fpic2.zhimg.com\u002Fv2-49e616404c8907eca4565b00ead9ba55_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"930\" data-rawheight=\"564\" class=\"origin_image zh-lightbox-thumb\" width=\"930\" data-original=\"https:\u002F\u002Fpic2.zhimg.com\u002Fv2-49e616404c8907eca4565b00ead9ba55_r.jpg\"\u002F\u003E\u003C\u002Fnoscript\u003E\u003Cimg src=\"data:image\u002Fsvg+xml;utf8,&lt;svg xmlns=&#39;http:\u002F\u002Fwww.w3.org\u002F2000\u002Fsvg&#39; width=&#39;930&#39; height=&#39;564&#39;&gt;&lt;\u002Fsvg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"930\" data-rawheight=\"564\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"930\" data-original=\"https:\u002F\u002Fpic2.zhimg.com\u002Fv2-49e616404c8907eca4565b00ead9ba55_r.jpg\" data-actualsrc=\"https:\u002F\u002Fpic2.zhimg.com\u002Fv2-49e616404c8907eca4565b00ead9ba55_b.jpg\"\u002F\u003E\u003C\u002Ffigure\u003E\u003Cp data-pid=\"Ih_qJZTP\"\u003E\u003Cb\u003E2019 (EMNLP) Sentence-BERT - Sentence Embeddings using Siamese BERT-Networks\u003C\u002Fb\u003E\u003C\u002Fp\u003E\u003Cp data-pid=\"lNTXUZIv\"\u003E\u003Cb\u003E2019 (MT-DNN) Multi-Task Deep Neural Networks for Natural Language Understanding\u003C\u002Fb\u003E\u003C\u002Fp\u003E\u003Cfigure data-size=\"normal\"\u003E\u003Cnoscript\u003E\u003Cimg src=\"https:\u002F\u002Fpic4.zhimg.com\u002Fv2-2ce3f752c5ad641b67af8c434cb8f51b_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1732\" data-rawheight=\"1246\" class=\"origin_image zh-lightbox-thumb\" width=\"1732\" data-original=\"https:\u002F\u002Fpic4.zhimg.com\u002Fv2-2ce3f752c5ad641b67af8c434cb8f51b_r.jpg\"\u002F\u003E\u003C\u002Fnoscript\u003E\u003Cimg src=\"data:image\u002Fsvg+xml;utf8,&lt;svg xmlns=&#39;http:\u002F\u002Fwww.w3.org\u002F2000\u002Fsvg&#39; width=&#39;1732&#39; height=&#39;1246&#39;&gt;&lt;\u002Fsvg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1732\" data-rawheight=\"1246\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1732\" data-original=\"https:\u002F\u002Fpic4.zhimg.com\u002Fv2-2ce3f752c5ad641b67af8c434cb8f51b_r.jpg\" data-actualsrc=\"https:\u002F\u002Fpic4.zhimg.com\u002Fv2-2ce3f752c5ad641b67af8c434cb8f51b_b.jpg\"\u002F\u003E\u003C\u002Ffigure\u003E\u003Cp data-pid=\"BchBtMzk\"\u003E\u003Cb\u003E2020 (Alibaba) (Structbert) Structbert - Incorporating language structures into pre-training for deep language understanding\u003C\u002Fb\u003E\u003C\u002Fp\u003E\u003Cp data-pid=\"BAPJ9qn2\"\u003E\u003Cb\u003EArxiv 21 SimCSE - Simple Contrastive Learning of Sentence Embeddings\u003C\u002Fb\u003E\u003C\u002Fp\u003E\u003Cfigure data-size=\"normal\"\u003E\u003Cnoscript\u003E\u003Cimg src=\"https:\u002F\u002Fpic2.zhimg.com\u002Fv2-7d1f30da13c96b0a4457dd7f19f84a85_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"2060\" data-rawheight=\"950\" class=\"origin_image zh-lightbox-thumb\" width=\"2060\" data-original=\"https:\u002F\u002Fpic2.zhimg.com\u002Fv2-7d1f30da13c96b0a4457dd7f19f84a85_r.jpg\"\u002F\u003E\u003C\u002Fnoscript\u003E\u003Cimg src=\"data:image\u002Fsvg+xml;utf8,&lt;svg xmlns=&#39;http:\u002F\u002Fwww.w3.org\u002F2000\u002Fsvg&#39; width=&#39;2060&#39; height=&#39;950&#39;&gt;&lt;\u002Fsvg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"2060\" data-rawheight=\"950\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"2060\" data-original=\"https:\u002F\u002Fpic2.zhimg.com\u002Fv2-7d1f30da13c96b0a4457dd7f19f84a85_r.jpg\" data-actualsrc=\"https:\u002F\u002Fpic2.zhimg.com\u002Fv2-7d1f30da13c96b0a4457dd7f19f84a85_b.jpg\"\u002F\u003E\u003C\u002Ffigure\u003E\u003Ch2\u003E3.3 Graph Learning:\u003C\u002Fh2\u003E\u003Cp data-pid=\"UK2aDfzN\"\u003E自监督学习同样可以用在图表示学习（例如GNN）中，通过预测节点属性、mask掉节点、边等方法，来提升图表示学习的泛化能力、鲁棒性。\u003C\u002Fp\u003E\u003Cp data-pid=\"ERHs3aCM\"\u003E\u003Cb\u003E[26] 2020 (KDD) GPT-GNN: Generative Pre-Training of Graph Neural Networks\u003C\u002Fb\u003E\u003C\u002Fp\u003E\u003Cp data-pid=\"1qv7QVMB\"\u003EGNN对于具体任务，需要大量的标签。这篇论文里提出首先通过Self-supervised learning，学习预测图中的节点属性、边。然后可以在下游任务中通过少量label finetune。\u003C\u002Fp\u003E\u003Cfigure data-size=\"normal\"\u003E\u003Cnoscript\u003E\u003Cimg src=\"https:\u002F\u002Fpic4.zhimg.com\u002Fv2-54e615c441c33df5e086cd2ac5b91e07_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1718\" data-rawheight=\"784\" class=\"origin_image zh-lightbox-thumb\" width=\"1718\" data-original=\"https:\u002F\u002Fpic4.zhimg.com\u002Fv2-54e615c441c33df5e086cd2ac5b91e07_r.jpg\"\u002F\u003E\u003C\u002Fnoscript\u003E\u003Cimg src=\"data:image\u002Fsvg+xml;utf8,&lt;svg xmlns=&#39;http:\u002F\u002Fwww.w3.org\u002F2000\u002Fsvg&#39; width=&#39;1718&#39; height=&#39;784&#39;&gt;&lt;\u002Fsvg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1718\" data-rawheight=\"784\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1718\" data-original=\"https:\u002F\u002Fpic4.zhimg.com\u002Fv2-54e615c441c33df5e086cd2ac5b91e07_r.jpg\" data-actualsrc=\"https:\u002F\u002Fpic4.zhimg.com\u002Fv2-54e615c441c33df5e086cd2ac5b91e07_b.jpg\"\u002F\u003E\u003C\u002Ffigure\u003E\u003Cp data-pid=\"dw11OzoZ\"\u003E\u003Cb\u003E[27] 2020 (Arxiv) Self-supervised Learning on Graphs: Deep Insights and New Direction\u003C\u002Fb\u003E\u003C\u002Fp\u003E\u003Cp data-pid=\"eLEEGSYB\"\u003E\u003Cb\u003E[28] 2020 (WWW) Graph Representation Learning via Graphical Mutual Information Maximization\u003C\u002Fb\u003E\u003C\u002Fp\u003E\u003Ch2\u003E3.4 Recommender Systems:\u003C\u002Fh2\u003E\u003Cp data-pid=\"qVJN8eve\"\u003E推荐系统中通常item都是亿级别，DNN模型中对应的embedding参数是百亿千亿级别、query\u002Fitem-item交互的数据很稀疏，因此即使几百亿的训练数据，仍然显得有些不足。\u003C\u002Fp\u003E\u003Cp data-pid=\"_G3-BE9d\"\u003E通过无监督学习，能更好地利用全网用户的行为，学习更强大的召回、排序模型。\u003C\u002Fp\u003E\u003Cp data-pid=\"DWUFXMuV\"\u003E\u003Cb\u003E[29] 2019 (Alibaba) (CIKM)    BERT4Rec: Sequential Recommendation with Bidirectional Encoder Representations from Transformer \u003C\u002Fb\u003E\u003C\u002Fp\u003E\u003Cp data-pid=\"OlJxa390\"\u003E\u003Cb\u003E[30] 2020 (Google) (Arxiv) Self-supervised Learning for Large-scale Item Recommendations\u003C\u002Fb\u003E\u003C\u002Fp\u003E\u003Cp data-pid=\"dTzy6P0h\"\u003E实际推荐系统中query-item或者item-item交互数据稀疏，而且长尾item表示难以学好。\u003C\u002Fp\u003E\u003Cp data-pid=\"mjPL_E7R\"\u003EGoogle这篇论文将SimCLR思想用来改进召回模型（Two-tower)，该工作对于召回模型优化、长尾item表示学习、纯曝光数据利用等方向具有很好的指导作用。\u003C\u002Fp\u003E\u003Cfigure data-size=\"normal\"\u003E\u003Cnoscript\u003E\u003Cimg src=\"https:\u002F\u002Fpic1.zhimg.com\u002Fv2-d6ea6a308e7ba992c21ca9bb8d620fe4_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"764\" data-rawheight=\"426\" class=\"origin_image zh-lightbox-thumb\" width=\"764\" data-original=\"https:\u002F\u002Fpic1.zhimg.com\u002Fv2-d6ea6a308e7ba992c21ca9bb8d620fe4_r.jpg\"\u002F\u003E\u003C\u002Fnoscript\u003E\u003Cimg src=\"data:image\u002Fsvg+xml;utf8,&lt;svg xmlns=&#39;http:\u002F\u002Fwww.w3.org\u002F2000\u002Fsvg&#39; width=&#39;764&#39; height=&#39;426&#39;&gt;&lt;\u002Fsvg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"764\" data-rawheight=\"426\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"764\" data-original=\"https:\u002F\u002Fpic1.zhimg.com\u002Fv2-d6ea6a308e7ba992c21ca9bb8d620fe4_r.jpg\" data-actualsrc=\"https:\u002F\u002Fpic1.zhimg.com\u002Fv2-d6ea6a308e7ba992c21ca9bb8d620fe4_b.jpg\"\u002F\u003E\u003C\u002Ffigure\u003E\u003Cp data-pid=\"ft3p0f-J\"\u003E提出了两种数据增强模式：Feature Mask，Feature Dropout。具体做法，类似SIMCLR，对于输入特征，分成多组特征，将一些特征mask或dropout掉。\u003C\u002Fp\u003E\u003Cp data-pid=\"xMymOMVl\"\u003Eself-supervised loss: 对于输入x，做两种不同的变换，然后经过encoder，使得它们的相似度比别的输入x&#39;经过变换、encoder后的表示高。\u003C\u002Fp\u003E\u003Cfigure data-size=\"normal\"\u003E\u003Cnoscript\u003E\u003Cimg src=\"https:\u002F\u002Fpic3.zhimg.com\u002Fv2-1b699b05bc5afce16771f7f857973056_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"762\" data-rawheight=\"584\" class=\"origin_image zh-lightbox-thumb\" width=\"762\" data-original=\"https:\u002F\u002Fpic3.zhimg.com\u002Fv2-1b699b05bc5afce16771f7f857973056_r.jpg\"\u002F\u003E\u003C\u002Fnoscript\u003E\u003Cimg src=\"data:image\u002Fsvg+xml;utf8,&lt;svg xmlns=&#39;http:\u002F\u002Fwww.w3.org\u002F2000\u002Fsvg&#39; width=&#39;762&#39; height=&#39;584&#39;&gt;&lt;\u002Fsvg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"762\" data-rawheight=\"584\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"762\" data-original=\"https:\u002F\u002Fpic3.zhimg.com\u002Fv2-1b699b05bc5afce16771f7f857973056_r.jpg\" data-actualsrc=\"https:\u002F\u002Fpic3.zhimg.com\u002Fv2-1b699b05bc5afce16771f7f857973056_b.jpg\"\u002F\u003E\u003C\u002Ffigure\u003E\u003Cp data-pid=\"jZcIXyik\"\u003EFeature Mask方法如下：\u003C\u002Fp\u003E\u003Cfigure data-size=\"normal\"\u003E\u003Cnoscript\u003E\u003Cimg src=\"https:\u002F\u002Fpic3.zhimg.com\u002Fv2-3a26807bdea8ca9152d1e1f8eee93432_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1988\" data-rawheight=\"608\" class=\"origin_image zh-lightbox-thumb\" width=\"1988\" data-original=\"https:\u002F\u002Fpic3.zhimg.com\u002Fv2-3a26807bdea8ca9152d1e1f8eee93432_r.jpg\"\u002F\u003E\u003C\u002Fnoscript\u003E\u003Cimg src=\"data:image\u002Fsvg+xml;utf8,&lt;svg xmlns=&#39;http:\u002F\u002Fwww.w3.org\u002F2000\u002Fsvg&#39; width=&#39;1988&#39; height=&#39;608&#39;&gt;&lt;\u002Fsvg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1988\" data-rawheight=\"608\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1988\" data-original=\"https:\u002F\u002Fpic3.zhimg.com\u002Fv2-3a26807bdea8ca9152d1e1f8eee93432_r.jpg\" data-actualsrc=\"https:\u002F\u002Fpic3.zhimg.com\u002Fv2-3a26807bdea8ca9152d1e1f8eee93432_b.jpg\"\u002F\u003E\u003C\u002Ffigure\u003E\u003Cp data-pid=\"I_8OHNW7\"\u003EFeature Dropout方法如下：\u003C\u002Fp\u003E\u003Cfigure data-size=\"normal\"\u003E\u003Cnoscript\u003E\u003Cimg src=\"https:\u002F\u002Fpic2.zhimg.com\u002Fv2-84fb65ad790872179dd385e3e9023fb1_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"2018\" data-rawheight=\"596\" class=\"origin_image zh-lightbox-thumb\" width=\"2018\" data-original=\"https:\u002F\u002Fpic2.zhimg.com\u002Fv2-84fb65ad790872179dd385e3e9023fb1_r.jpg\"\u002F\u003E\u003C\u002Fnoscript\u003E\u003Cimg src=\"data:image\u002Fsvg+xml;utf8,&lt;svg xmlns=&#39;http:\u002F\u002Fwww.w3.org\u002F2000\u002Fsvg&#39; width=&#39;2018&#39; height=&#39;596&#39;&gt;&lt;\u002Fsvg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"2018\" data-rawheight=\"596\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"2018\" data-original=\"https:\u002F\u002Fpic2.zhimg.com\u002Fv2-84fb65ad790872179dd385e3e9023fb1_r.jpg\" data-actualsrc=\"https:\u002F\u002Fpic2.zhimg.com\u002Fv2-84fb65ad790872179dd385e3e9023fb1_b.jpg\"\u002F\u003E\u003C\u002Ffigure\u003E\u003Cp data-pid=\"fEQrEZv8\"\u003E然后和监督式任务相结合：\u003C\u002Fp\u003E\u003Cfigure data-size=\"normal\"\u003E\u003Cnoscript\u003E\u003Cimg src=\"https:\u002F\u002Fpic1.zhimg.com\u002Fv2-e851e725cd27bbc3c0d1581c856269ec_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"786\" data-rawheight=\"122\" class=\"origin_image zh-lightbox-thumb\" width=\"786\" data-original=\"https:\u002F\u002Fpic1.zhimg.com\u002Fv2-e851e725cd27bbc3c0d1581c856269ec_r.jpg\"\u002F\u003E\u003C\u002Fnoscript\u003E\u003Cimg src=\"data:image\u002Fsvg+xml;utf8,&lt;svg xmlns=&#39;http:\u002F\u002Fwww.w3.org\u002F2000\u002Fsvg&#39; width=&#39;786&#39; height=&#39;122&#39;&gt;&lt;\u002Fsvg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"786\" data-rawheight=\"122\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"786\" data-original=\"https:\u002F\u002Fpic1.zhimg.com\u002Fv2-e851e725cd27bbc3c0d1581c856269ec_r.jpg\" data-actualsrc=\"https:\u002F\u002Fpic1.zhimg.com\u002Fv2-e851e725cd27bbc3c0d1581c856269ec_b.jpg\"\u002F\u003E\u003C\u002Ffigure\u003E\u003Cfigure data-size=\"normal\"\u003E\u003Cnoscript\u003E\u003Cimg src=\"https:\u002F\u002Fpic3.zhimg.com\u002Fv2-c6e4383e0065e827dd08467a5bbe0e9a_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"750\" data-rawheight=\"152\" class=\"origin_image zh-lightbox-thumb\" width=\"750\" data-original=\"https:\u002F\u002Fpic3.zhimg.com\u002Fv2-c6e4383e0065e827dd08467a5bbe0e9a_r.jpg\"\u002F\u003E\u003C\u002Fnoscript\u003E\u003Cimg src=\"data:image\u002Fsvg+xml;utf8,&lt;svg xmlns=&#39;http:\u002F\u002Fwww.w3.org\u002F2000\u002Fsvg&#39; width=&#39;750&#39; height=&#39;152&#39;&gt;&lt;\u002Fsvg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"750\" data-rawheight=\"152\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"750\" data-original=\"https:\u002F\u002Fpic3.zhimg.com\u002Fv2-c6e4383e0065e827dd08467a5bbe0e9a_r.jpg\" data-actualsrc=\"https:\u002F\u002Fpic3.zhimg.com\u002Fv2-c6e4383e0065e827dd08467a5bbe0e9a_b.jpg\"\u002F\u003E\u003C\u002Ffigure\u003E\u003Cfigure data-size=\"normal\"\u003E\u003Cnoscript\u003E\u003Cimg src=\"https:\u002F\u002Fpic3.zhimg.com\u002Fv2-52f992604033964778e982cd882411ea_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"336\" data-rawheight=\"104\" class=\"content_image\" width=\"336\"\u002F\u003E\u003C\u002Fnoscript\u003E\u003Cimg src=\"data:image\u002Fsvg+xml;utf8,&lt;svg xmlns=&#39;http:\u002F\u002Fwww.w3.org\u002F2000\u002Fsvg&#39; width=&#39;336&#39; height=&#39;104&#39;&gt;&lt;\u002Fsvg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"336\" data-rawheight=\"104\" class=\"content_image lazy\" width=\"336\" data-actualsrc=\"https:\u002F\u002Fpic3.zhimg.com\u002Fv2-52f992604033964778e982cd882411ea_b.jpg\"\u002F\u003E\u003C\u002Ffigure\u003E\u003Cp data-pid=\"x8zvNtQb\"\u003E\u003Cb\u003E[31] 2020 (Alibaba) (KDD) Disentangled Self-Supervision in Sequential Recommenders\u003C\u002Fb\u003E\u003C\u002Fp\u003E\u003Cp data-pid=\"Ud8sq4Gc\"\u003E推荐系统一般采取seq2item这种方式来训练，存在对长尾item不能很好学习、短视的缺点。这篇论文里，作者提出seq2seq的思路，根据历史行为序列预测未来的行为序列，基于自监督学习辅助监督学习任务，在candidate generation任务上取得比较好的效果。\u003C\u002Fp\u003E\u003Cfigure data-size=\"normal\"\u003E\u003Cnoscript\u003E\u003Cimg src=\"https:\u002F\u002Fpic3.zhimg.com\u002Fv2-116f4c2318bf6f4a5af43a99dc3421f6_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1842\" data-rawheight=\"770\" class=\"origin_image zh-lightbox-thumb\" width=\"1842\" data-original=\"https:\u002F\u002Fpic3.zhimg.com\u002Fv2-116f4c2318bf6f4a5af43a99dc3421f6_r.jpg\"\u002F\u003E\u003C\u002Fnoscript\u003E\u003Cimg src=\"data:image\u002Fsvg+xml;utf8,&lt;svg xmlns=&#39;http:\u002F\u002Fwww.w3.org\u002F2000\u002Fsvg&#39; width=&#39;1842&#39; height=&#39;770&#39;&gt;&lt;\u002Fsvg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1842\" data-rawheight=\"770\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1842\" data-original=\"https:\u002F\u002Fpic3.zhimg.com\u002Fv2-116f4c2318bf6f4a5af43a99dc3421f6_r.jpg\" data-actualsrc=\"https:\u002F\u002Fpic3.zhimg.com\u002Fv2-116f4c2318bf6f4a5af43a99dc3421f6_b.jpg\"\u002F\u003E\u003C\u002Ffigure\u003E\u003Cp data-pid=\"6FO-wEG9\"\u003E\u003Cb\u003E[32] 2020 (CIKM) [S3Rec] S3-Rec: Self-Supervised Learning for Sequential Recommendation with Mutual Information Maximization\u003C\u002Fb\u003E\u003C\u002Fp\u003E\u003Cp data-pid=\"bIm_hXSv\"\u003E这篇论文基于Mutual Information Maximization的自监督任务，来辅助推荐模型训练。\u003C\u002Fp\u003E\u003Cp data-pid=\"9KA3qycA\"\u003E作者提出了四个自监督辅助任务：基于item-属性、基于序列-item、序列-item属性、序列-小序列，分别计算MIM的InfoNCE loss:\u003C\u002Fp\u003E\u003Cp data-pid=\"11w1Ss3M\"\u003E例如对于item-属性辅助任务，loss函数为：\u003C\u002Fp\u003E\u003Cfigure data-size=\"normal\"\u003E\u003Cnoscript\u003E\u003Cimg src=\"https:\u002F\u002Fpic3.zhimg.com\u002Fv2-96cf31e68115bc338a29d69ed5689f92_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"922\" data-rawheight=\"104\" class=\"origin_image zh-lightbox-thumb\" width=\"922\" data-original=\"https:\u002F\u002Fpic3.zhimg.com\u002Fv2-96cf31e68115bc338a29d69ed5689f92_r.jpg\"\u002F\u003E\u003C\u002Fnoscript\u003E\u003Cimg src=\"data:image\u002Fsvg+xml;utf8,&lt;svg xmlns=&#39;http:\u002F\u002Fwww.w3.org\u002F2000\u002Fsvg&#39; width=&#39;922&#39; height=&#39;104&#39;&gt;&lt;\u002Fsvg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"922\" data-rawheight=\"104\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"922\" data-original=\"https:\u002F\u002Fpic3.zhimg.com\u002Fv2-96cf31e68115bc338a29d69ed5689f92_r.jpg\" data-actualsrc=\"https:\u002F\u002Fpic3.zhimg.com\u002Fv2-96cf31e68115bc338a29d69ed5689f92_b.jpg\"\u002F\u003E\u003C\u002Ffigure\u003E\u003Cfigure data-size=\"normal\"\u003E\u003Cnoscript\u003E\u003Cimg src=\"https:\u002F\u002Fpic2.zhimg.com\u002Fv2-bbc9ec87959f502cb589d53b671096bd_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"468\" data-rawheight=\"84\" class=\"origin_image zh-lightbox-thumb\" width=\"468\" data-original=\"https:\u002F\u002Fpic2.zhimg.com\u002Fv2-bbc9ec87959f502cb589d53b671096bd_r.jpg\"\u002F\u003E\u003C\u002Fnoscript\u003E\u003Cimg src=\"data:image\u002Fsvg+xml;utf8,&lt;svg xmlns=&#39;http:\u002F\u002Fwww.w3.org\u002F2000\u002Fsvg&#39; width=&#39;468&#39; height=&#39;84&#39;&gt;&lt;\u002Fsvg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"468\" data-rawheight=\"84\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"468\" data-original=\"https:\u002F\u002Fpic2.zhimg.com\u002Fv2-bbc9ec87959f502cb589d53b671096bd_r.jpg\" data-actualsrc=\"https:\u002F\u002Fpic2.zhimg.com\u002Fv2-bbc9ec87959f502cb589d53b671096bd_b.jpg\"\u002F\u003E\u003C\u002Ffigure\u003E\u003Cfigure data-size=\"normal\"\u003E\u003Cnoscript\u003E\u003Cimg src=\"https:\u002F\u002Fpic3.zhimg.com\u002Fv2-8f394b57214959086563116ff56affb2_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"2548\" data-rawheight=\"910\" class=\"origin_image zh-lightbox-thumb\" width=\"2548\" data-original=\"https:\u002F\u002Fpic3.zhimg.com\u002Fv2-8f394b57214959086563116ff56affb2_r.jpg\"\u002F\u003E\u003C\u002Fnoscript\u003E\u003Cimg src=\"data:image\u002Fsvg+xml;utf8,&lt;svg xmlns=&#39;http:\u002F\u002Fwww.w3.org\u002F2000\u002Fsvg&#39; width=&#39;2548&#39; height=&#39;910&#39;&gt;&lt;\u002Fsvg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"2548\" data-rawheight=\"910\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"2548\" data-original=\"https:\u002F\u002Fpic3.zhimg.com\u002Fv2-8f394b57214959086563116ff56affb2_r.jpg\" data-actualsrc=\"https:\u002F\u002Fpic3.zhimg.com\u002Fv2-8f394b57214959086563116ff56affb2_b.jpg\"\u002F\u003E\u003C\u002Ffigure\u003E\u003Cp data-pid=\"dea_Cbat\"\u003E自监督训练后，再基于监督训练来finetune推荐模型。\u003C\u002Fp\u003E\u003Cp data-pid=\"gohSy_zU\"\u003E\u003Cb\u003E[33] 2020 (Arxiv) Self-supervised Graph Learning for Recommendation\u003C\u002Fb\u003E\u003C\u002Fp\u003E\u003Cp data-pid=\"AVLE7AZg\"\u003E这篇论文里，作者提出在GNN中使用自监督辅助任务来改进模型学习。\u003C\u002Fp\u003E\u003Cp data-pid=\"k3HFYDfL\"\u003E提出了四种方法：embedding masking, embedding dropout, node dropout, edge dropout。\u003C\u002Fp\u003E\u003Cp data-pid=\"x0bKPWFL\"\u003E\u003Cb\u003E[34] 2020 (SIGIR) Self-Supervised Reinforcement Learning for Recommender Systems\u003C\u002Fb\u003E\u003C\u002Fp\u003E\u003Cp data-pid=\"akugs4DT\"\u003E这篇论文里，作者将自监督学习和RL结合，用于推荐。\u003C\u002Fp\u003E\u003Cp data-pid=\"abA4kQBJ\"\u003E\u003Cb\u003E2020 (Alibaba) (Arxiv) Contrastive Learning for Debiased Candidate Generation in Large-Scale Recommender Systems\u003C\u002Fb\u003E\u003C\u002Fp\u003E\u003Cp data-pid=\"WiFsd6W7\"\u003E\u003Cb\u003EWSDM 2021 PROP - Pre-training with Representative Words Prediction for Ad-hoc Retrieval\u003C\u002Fb\u003E \u003C\u002Fp\u003E\u003Cp data-pid=\"G6BQSriF\"\u003E\u003Cb\u003E2020 (Arxiv) USERBERT - SELF-SUPERVISED USER REPRESENTATION LEARNING\u003C\u002Fb\u003E\u003C\u002Fp\u003E\u003Cp data-pid=\"Yo5OG-v7\"\u003E\u003Cb\u003E2019 (Alibaba) (AAAI) Deep interest evolution network for click-through rate prediction\u003C\u002Fb\u003E\u003C\u002Fp\u003E\u003Cp data-pid=\"j8QoITnr\"\u003E\u003Cb\u003E2020 (Alibaba) (AAAI) Deep Match to Rank Model for Personalized Click-Through Rate Prediction\u003C\u002Fb\u003E\u003C\u002Fp\u003E\u003Cp data-pid=\"dzSbIS1F\"\u003E\u003Cb\u003E2020 (EMNLP) PTUM - Pre-training User Model from Unlabeled User Behaviors via Self-supervision\u003C\u002Fb\u003E\u003C\u002Fp\u003E\u003Ch2\u003E3.5 Machine Learning Perspective: \u003C\u002Fh2\u003E\u003Cp data-pid=\"3X4Pxoco\"\u003ESelf-supervised learning与许多机器学习方法有着丰富的联系，例如与Semi-supervised learning、Pre-training、Self-training，都是希望利用更多的有标签或无标签数据，来提升某个任务下的性能。\u003C\u002Fp\u003E\u003Cp data-pid=\"qzS3cP0f\"\u003E具体关注的重点在于：\u003C\u002Fp\u003E\u003Cp data-pid=\"l6BFhcsR\"\u003ESemi-supervised learning: 同时利用无标签和有标签数据进行学习。\u003C\u002Fp\u003E\u003Cp data-pid=\"U9TvSayp\"\u003ESelf-supervised learning: 使用无监督数据，通过定义pretext task来学习。\u003C\u002Fp\u003E\u003Cp data-pid=\"nDbHQn8e\"\u003EPre-training: 预训练。一般先在大量数据上学习，再在下游任务finetune。\u003C\u002Fp\u003E\u003Cp data-pid=\"1N9KXfaQ\"\u003ESelf-training: 自训练。用学习的模型，在无标记数据上标注生成伪标签，然后借助这些伪标签数据，改进监督模型效果。\u003C\u002Fp\u003E\u003Cp data-pid=\"5XaEmLkN\"\u003E下面我们介绍一些与自监督学习关系比较密切的机器学习方法相关的论文。\u003C\u002Fp\u003E\u003Cul\u003E\u003Cli data-pid=\"Kk7YmIXM\"\u003E\u003Cb\u003ESelf-supervised Learning:\u003C\u002Fb\u003E\u003C\u002Fli\u003E\u003C\u002Ful\u003E\u003Cp data-pid=\"rUxRfSSE\"\u003E\u003Cb\u003E[35] 2020 (ICLR) A Mutual Information Maximization Perspective of Language Representation Learning\u003C\u002Fb\u003E\u003C\u002Fp\u003E\u003Cul\u003E\u003Cli data-pid=\"M9Iy9mYj\"\u003E\u003Cb\u003ETask Related Self-Supervised Learning:\u003C\u002Fb\u003E\u003C\u002Fli\u003E\u003C\u002Ful\u003E\u003Cp data-pid=\"lr7qbQ4J\"\u003E\u003Cb\u003E[36] 2019 (ICCV) Boosting Few-Shot Visual Learning with Self-Supervision\u003C\u002Fb\u003E\u003C\u002Fp\u003E\u003Cp data-pid=\"1MtgPiu7\"\u003E\u003Cb\u003E[37] 2019 (Arxiv) Rethinking Data Augmentation: Self-Supervision and Self-Distillation\u003C\u002Fb\u003E\u003C\u002Fp\u003E\u003Cul\u003E\u003Cli data-pid=\"hAeEB7J_\"\u003E\u003Cb\u003ESelf-supervised and Semi-supervised Learning:\u003C\u002Fb\u003E\u003C\u002Fli\u003E\u003C\u002Ful\u003E\u003Cp data-pid=\"ZLSJ9YEp\"\u003E\u003Cb\u003E[38] 2013 (ICML Workshop) Pseudo-Label - The Simple and Efficient Semi-Supervised Learning Method for Deep Neural Networks\u003C\u002Fb\u003E\u003C\u002Fp\u003E\u003Cp data-pid=\"MZrO1Tb7\"\u003E使用模型在无标签数据生成伪label，选取最大的预测结果作为分类结果。\u003C\u002Fp\u003E\u003Cfigure data-size=\"normal\"\u003E\u003Cnoscript\u003E\u003Cimg src=\"https:\u002F\u002Fpic1.zhimg.com\u002Fv2-2c988e417d852cef0379f96296acf8f0_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"782\" data-rawheight=\"288\" class=\"origin_image zh-lightbox-thumb\" width=\"782\" data-original=\"https:\u002F\u002Fpic1.zhimg.com\u002Fv2-2c988e417d852cef0379f96296acf8f0_r.jpg\"\u002F\u003E\u003C\u002Fnoscript\u003E\u003Cimg src=\"data:image\u002Fsvg+xml;utf8,&lt;svg xmlns=&#39;http:\u002F\u002Fwww.w3.org\u002F2000\u002Fsvg&#39; width=&#39;782&#39; height=&#39;288&#39;&gt;&lt;\u002Fsvg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"782\" data-rawheight=\"288\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"782\" data-original=\"https:\u002F\u002Fpic1.zhimg.com\u002Fv2-2c988e417d852cef0379f96296acf8f0_r.jpg\" data-actualsrc=\"https:\u002F\u002Fpic1.zhimg.com\u002Fv2-2c988e417d852cef0379f96296acf8f0_b.jpg\"\u002F\u003E\u003C\u002Ffigure\u003E\u003Cp data-pid=\"SlXrzAZI\"\u003E然后基于有标签和伪标签数据联合训练：\u003C\u002Fp\u003E\u003Cfigure data-size=\"normal\"\u003E\u003Cnoscript\u003E\u003Cimg src=\"https:\u002F\u002Fpic1.zhimg.com\u002Fv2-647441df5cb7ecd606b19689e2c27660_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"766\" data-rawheight=\"116\" class=\"origin_image zh-lightbox-thumb\" width=\"766\" data-original=\"https:\u002F\u002Fpic1.zhimg.com\u002Fv2-647441df5cb7ecd606b19689e2c27660_r.jpg\"\u002F\u003E\u003C\u002Fnoscript\u003E\u003Cimg src=\"data:image\u002Fsvg+xml;utf8,&lt;svg xmlns=&#39;http:\u002F\u002Fwww.w3.org\u002F2000\u002Fsvg&#39; width=&#39;766&#39; height=&#39;116&#39;&gt;&lt;\u002Fsvg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"766\" data-rawheight=\"116\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"766\" data-original=\"https:\u002F\u002Fpic1.zhimg.com\u002Fv2-647441df5cb7ecd606b19689e2c27660_r.jpg\" data-actualsrc=\"https:\u002F\u002Fpic1.zhimg.com\u002Fv2-647441df5cb7ecd606b19689e2c27660_b.jpg\"\u002F\u003E\u003C\u002Ffigure\u003E\u003Cp data-pid=\"Yw60cQVB\"\u003E逐渐增大伪标签的权重：\u003C\u002Fp\u003E\u003Cfigure data-size=\"normal\"\u003E\u003Cnoscript\u003E\u003Cimg src=\"https:\u002F\u002Fpic1.zhimg.com\u002Fv2-a13d7f683dd6d75e8ef181fd462b28c0_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"768\" data-rawheight=\"176\" class=\"origin_image zh-lightbox-thumb\" width=\"768\" data-original=\"https:\u002F\u002Fpic1.zhimg.com\u002Fv2-a13d7f683dd6d75e8ef181fd462b28c0_r.jpg\"\u002F\u003E\u003C\u002Fnoscript\u003E\u003Cimg src=\"data:image\u002Fsvg+xml;utf8,&lt;svg xmlns=&#39;http:\u002F\u002Fwww.w3.org\u002F2000\u002Fsvg&#39; width=&#39;768&#39; height=&#39;176&#39;&gt;&lt;\u002Fsvg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"768\" data-rawheight=\"176\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"768\" data-original=\"https:\u002F\u002Fpic1.zhimg.com\u002Fv2-a13d7f683dd6d75e8ef181fd462b28c0_r.jpg\" data-actualsrc=\"https:\u002F\u002Fpic1.zhimg.com\u002Fv2-a13d7f683dd6d75e8ef181fd462b28c0_b.jpg\"\u002F\u003E\u003C\u002Ffigure\u003E\u003Cp data-pid=\"0kvsrQ5s\"\u003E\u003Cb\u003E[39] 2019 (Google) (ICCV) S​4L: Self-Supervised Semi-Supervised Learning\u003C\u002Fb\u003E\u003C\u002Fp\u003E\u003Cp data-pid=\"FdVlPMcZ\"\u003Eself-supervised learning: 同时利用无标签和有标签数据进行学习。\u003C\u002Fp\u003E\u003Cp data-pid=\"h4RIiW9d\"\u003ESelf-supervised learning: 使用无监督数据，通过定义pretext task来学习。\u003C\u002Fp\u003E\u003Cp data-pid=\"_lPy7jt6\"\u003EGoogle这篇论文里，指出self-supervised有助于semi-supervised learning学习。\u003C\u002Fp\u003E\u003Cfigure data-size=\"normal\"\u003E\u003Cnoscript\u003E\u003Cimg src=\"https:\u002F\u002Fpic2.zhimg.com\u002Fv2-2bb2655406d589c7dfd7b3ac014f740d_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1354\" data-rawheight=\"1014\" class=\"origin_image zh-lightbox-thumb\" width=\"1354\" data-original=\"https:\u002F\u002Fpic2.zhimg.com\u002Fv2-2bb2655406d589c7dfd7b3ac014f740d_r.jpg\"\u002F\u003E\u003C\u002Fnoscript\u003E\u003Cimg src=\"data:image\u002Fsvg+xml;utf8,&lt;svg xmlns=&#39;http:\u002F\u002Fwww.w3.org\u002F2000\u002Fsvg&#39; width=&#39;1354&#39; height=&#39;1014&#39;&gt;&lt;\u002Fsvg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1354\" data-rawheight=\"1014\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1354\" data-original=\"https:\u002F\u002Fpic2.zhimg.com\u002Fv2-2bb2655406d589c7dfd7b3ac014f740d_r.jpg\" data-actualsrc=\"https:\u002F\u002Fpic2.zhimg.com\u002Fv2-2bb2655406d589c7dfd7b3ac014f740d_b.jpg\"\u002F\u003E\u003C\u002Ffigure\u003E\u003Cul\u003E\u003Cli data-pid=\"9Hq-otCG\"\u003E\u003Cb\u003ESemi-supervised :\u003C\u002Fb\u003E\u003C\u002Fli\u003E\u003C\u002Ful\u003E\u003Cp data-pid=\"LWtl5QRU\"\u003E\u003Cb\u003E[40] 2019 (Facebook) (Arxiv) Billion-scale semi-supervised learning for image classification\u003C\u002Fb\u003E\u003C\u002Fp\u003E\u003Cp data-pid=\"5zMJXQbv\"\u003EFacebook在这篇论文里，介绍如何利用有标签数据和无标签数据，共同学习，一个图像分类模型。首先基于有标签数据训练teacher模型，然后在无标签数据打分后，选出每个类别top K构成新的有标签数据，用于训练student model, 然后再用有标签数据对student model finetune。\u003C\u002Fp\u003E\u003Cfigure data-size=\"normal\"\u003E\u003Cnoscript\u003E\u003Cimg src=\"https:\u002F\u002Fpic3.zhimg.com\u002Fv2-f8977b98b5b3cb0307721d04f70e8fea_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1376\" data-rawheight=\"1316\" class=\"origin_image zh-lightbox-thumb\" width=\"1376\" data-original=\"https:\u002F\u002Fpic3.zhimg.com\u002Fv2-f8977b98b5b3cb0307721d04f70e8fea_r.jpg\"\u002F\u003E\u003C\u002Fnoscript\u003E\u003Cimg src=\"data:image\u002Fsvg+xml;utf8,&lt;svg xmlns=&#39;http:\u002F\u002Fwww.w3.org\u002F2000\u002Fsvg&#39; width=&#39;1376&#39; height=&#39;1316&#39;&gt;&lt;\u002Fsvg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1376\" data-rawheight=\"1316\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1376\" data-original=\"https:\u002F\u002Fpic3.zhimg.com\u002Fv2-f8977b98b5b3cb0307721d04f70e8fea_r.jpg\" data-actualsrc=\"https:\u002F\u002Fpic3.zhimg.com\u002Fv2-f8977b98b5b3cb0307721d04f70e8fea_b.jpg\"\u002F\u003E\u003C\u002Ffigure\u003E\u003Cul\u003E\u003Cli data-pid=\"XIhrXg5Q\"\u003E\u003Cb\u003ESelf training:\u003C\u002Fb\u003E\u003C\u002Fli\u003E\u003C\u002Ful\u003E\u003Cp data-pid=\"802Wuhvi\"\u003E\u003Cb\u003E[41] 2020 (Google) (NIPS) NIPS 2020 Rethinking the Value of Labels for Improving Class-Imbalanced Learning\u003C\u002Fb\u003E\u003C\u002Fp\u003E\u003Cp data-pid=\"qidepb4I\"\u003E\u003Cb\u003E[42] 2020 (Google) (CVPR) Self-Training With Noisy Student Improves ImageNet Classification\u003C\u002Fb\u003E\u003C\u002Fp\u003E\u003Cp data-pid=\"vv115S7D\"\u003E这篇论文里，作者先用label数据训练teacher模型，然后在无标签数据上生成伪label，基于标签数据和伪label，训练noisy student模型，再把这个noisy student当作teacher模型，继续生成伪label，注意teacher model不添加noise，这样伪label尽可能准确。多次迭代，使得noisy的student模型与更强大的无noise的teacher模型一致，从而performance越来越好。\u003C\u002Fp\u003E\u003Cp data-pid=\"b_Y8DVOl\"\u003Enoise: (1) 输入: 数据增强   （2）模型：dropout，stochastic depth。\u003C\u002Fp\u003E\u003Cp data-pid=\"i2V1PgFy\"\u003Etrick: （1）data filtering：过滤掉teacher模型置信度低的  （2） balancing: 每个类别，无标签数据规模接近。 \u003C\u002Fp\u003E\u003Cp class=\"ztext-empty-paragraph\"\u003E\u003Cbr\u002F\u003E\u003C\u002Fp\u003E\u003Cfigure data-size=\"normal\"\u003E\u003Cnoscript\u003E\u003Cimg src=\"https:\u002F\u002Fpic1.zhimg.com\u002Fv2-9b543fe71df7d0ca42c4d9f632b3b1e0_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1252\" data-rawheight=\"770\" class=\"origin_image zh-lightbox-thumb\" width=\"1252\" data-original=\"https:\u002F\u002Fpic1.zhimg.com\u002Fv2-9b543fe71df7d0ca42c4d9f632b3b1e0_r.jpg\"\u002F\u003E\u003C\u002Fnoscript\u003E\u003Cimg src=\"data:image\u002Fsvg+xml;utf8,&lt;svg xmlns=&#39;http:\u002F\u002Fwww.w3.org\u002F2000\u002Fsvg&#39; width=&#39;1252&#39; height=&#39;770&#39;&gt;&lt;\u002Fsvg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1252\" data-rawheight=\"770\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1252\" data-original=\"https:\u002F\u002Fpic1.zhimg.com\u002Fv2-9b543fe71df7d0ca42c4d9f632b3b1e0_r.jpg\" data-actualsrc=\"https:\u002F\u002Fpic1.zhimg.com\u002Fv2-9b543fe71df7d0ca42c4d9f632b3b1e0_b.jpg\"\u002F\u003E\u003C\u002Ffigure\u003E\u003Cp data-pid=\"SWMsYWVZ\"\u003E\u003Cb\u003E[43] 2020 (Google) (NIPS) Rethinking Pre-training and Self-training\u003C\u002Fb\u003E\u003C\u002Fp\u003E\u003Cul\u003E\u003Cli data-pid=\"YQU5TVrV\"\u003E\u003Cb\u003EMulti-task Self-supervised Learning:\u003C\u002Fb\u003E\u003C\u002Fli\u003E\u003C\u002Ful\u003E\u003Cp data-pid=\"pabgpNhS\"\u003E\u003Cb\u003E[44] 2017 (ICCV) Multi-task Self-Supervised Visual Learning\u003C\u002Fb\u003E\u003C\u002Fp\u003E\u003Cp data-pid=\"1p2MWDeL\"\u003E多个任务联合self-supervised learning:\u003C\u002Fp\u003E\u003Cfigure data-size=\"normal\"\u003E\u003Cnoscript\u003E\u003Cimg src=\"https:\u002F\u002Fpic4.zhimg.com\u002Fv2-c05ae8983334c39440910063959c7c83_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1840\" data-rawheight=\"684\" class=\"origin_image zh-lightbox-thumb\" width=\"1840\" data-original=\"https:\u002F\u002Fpic4.zhimg.com\u002Fv2-c05ae8983334c39440910063959c7c83_r.jpg\"\u002F\u003E\u003C\u002Fnoscript\u003E\u003Cimg src=\"data:image\u002Fsvg+xml;utf8,&lt;svg xmlns=&#39;http:\u002F\u002Fwww.w3.org\u002F2000\u002Fsvg&#39; width=&#39;1840&#39; height=&#39;684&#39;&gt;&lt;\u002Fsvg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1840\" data-rawheight=\"684\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1840\" data-original=\"https:\u002F\u002Fpic4.zhimg.com\u002Fv2-c05ae8983334c39440910063959c7c83_r.jpg\" data-actualsrc=\"https:\u002F\u002Fpic4.zhimg.com\u002Fv2-c05ae8983334c39440910063959c7c83_b.jpg\"\u002F\u003E\u003C\u002Ffigure\u003E\u003Cp data-pid=\"ntlJ5dwD\"\u003E\u003Cb\u003E[45] 2018 (CVPR) Cross-domain Self-supervised Multi-task Feature Learning Using Synthetic Imagery\u003C\u002Fb\u003E\u003C\u002Fp\u003E\u003Cp data-pid=\"cwTHTduQ\"\u003E利用合成图片多个任务联合self-supervised learning，并使用adversarial learning接近实际图片。\u003C\u002Fp\u003E\u003Cfigure data-size=\"normal\"\u003E\u003Cnoscript\u003E\u003Cimg src=\"https:\u002F\u002Fpic4.zhimg.com\u002Fv2-bee06eba8fbfced8d483062e246fb223_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1252\" data-rawheight=\"696\" class=\"origin_image zh-lightbox-thumb\" width=\"1252\" data-original=\"https:\u002F\u002Fpic4.zhimg.com\u002Fv2-bee06eba8fbfced8d483062e246fb223_r.jpg\"\u002F\u003E\u003C\u002Fnoscript\u003E\u003Cimg src=\"data:image\u002Fsvg+xml;utf8,&lt;svg xmlns=&#39;http:\u002F\u002Fwww.w3.org\u002F2000\u002Fsvg&#39; width=&#39;1252&#39; height=&#39;696&#39;&gt;&lt;\u002Fsvg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1252\" data-rawheight=\"696\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1252\" data-original=\"https:\u002F\u002Fpic4.zhimg.com\u002Fv2-bee06eba8fbfced8d483062e246fb223_r.jpg\" data-actualsrc=\"https:\u002F\u002Fpic4.zhimg.com\u002Fv2-bee06eba8fbfced8d483062e246fb223_b.jpg\"\u002F\u003E\u003C\u002Ffigure\u003E\u003Cfigure data-size=\"normal\"\u003E\u003Cnoscript\u003E\u003Cimg src=\"https:\u002F\u002Fpic2.zhimg.com\u002Fv2-d71e46b6ed8b238a1e4091d610401a51_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1338\" data-rawheight=\"670\" class=\"origin_image zh-lightbox-thumb\" width=\"1338\" data-original=\"https:\u002F\u002Fpic2.zhimg.com\u002Fv2-d71e46b6ed8b238a1e4091d610401a51_r.jpg\"\u002F\u003E\u003C\u002Fnoscript\u003E\u003Cimg src=\"data:image\u002Fsvg+xml;utf8,&lt;svg xmlns=&#39;http:\u002F\u002Fwww.w3.org\u002F2000\u002Fsvg&#39; width=&#39;1338&#39; height=&#39;670&#39;&gt;&lt;\u002Fsvg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1338\" data-rawheight=\"670\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1338\" data-original=\"https:\u002F\u002Fpic2.zhimg.com\u002Fv2-d71e46b6ed8b238a1e4091d610401a51_r.jpg\" data-actualsrc=\"https:\u002F\u002Fpic2.zhimg.com\u002Fv2-d71e46b6ed8b238a1e4091d610401a51_b.jpg\"\u002F\u003E\u003C\u002Ffigure\u003E\u003Cp data-pid=\"9IrEDc2p\"\u003E\u003Cb\u003EReinforcement Learning:\u003C\u002Fb\u003E\u003C\u002Fp\u003E\u003Cp data-pid=\"1G6HdGKB\"\u003E\u003Cb\u003E[46] 2019 (NIPS) Unsupervised State Representation Learning in Atari\u003C\u002Fb\u003E\u003C\u002Fp\u003E\u003Cp data-pid=\"f_oAJfEd\"\u003EBengio等人在这篇论文里，将Self-supervised learning和Reinforcement learning结合，从输入学习更好的状态编码器，改进泛化性能，从而在reward数据有限时，也能改进RL效果。\u003C\u002Fp\u003E\u003Ch2\u003E\u003Cbr\u002F\u003E3.6 Survey:\u003C\u002Fh2\u003E\u003Cp data-pid=\"YaWR96s7\"\u003E自监督学习的综述论文:\u003C\u002Fp\u003E\u003Cp data-pid=\"fNjiK4qQ\"\u003E\u003Cb\u003E[47] 2020 (Arxiv) Self-supervised Learning: Generative or Contrastive.\u003C\u002Fb\u003E\u003C\u002Fp\u003E\u003Cp data-pid=\"eDEYckRx\"\u003E\u003Cb\u003E[48] 2020 (T-PAMI) Self-supervised Visual Feature Learning with Deep Neural Networks: A Survey\u003C\u002Fb\u003E\u003C\u002Fp\u003E\u003Ch2\u003E3.7 More Materials：\u003C\u002Fh2\u003E\u003Cp data-pid=\"S7vw46Ni\"\u003EGithub: \u003C\u002Fp\u003E\u003Cp data-pid=\"TmBvUBYa\"\u003EFacebook: \u003Ca href=\"https:\u002F\u002Flink.zhihu.com\u002F?target=https%3A\u002F\u002Fgithub.com\u002Ffacebookresearch\u002Ffair_self_supervision_benchmark\" class=\" external\" target=\"_blank\" rel=\"nofollow noreferrer\"\u003E\u003Cspan class=\"invisible\"\u003Ehttps:\u002F\u002F\u003C\u002Fspan\u003E\u003Cspan class=\"visible\"\u003Egithub.com\u002Ffacebookrese\u003C\u002Fspan\u003E\u003Cspan class=\"invisible\"\u003Earch\u002Ffair_self_supervision_benchmark\u003C\u002Fspan\u003E\u003Cspan class=\"ellipsis\"\u003E\u003C\u002Fspan\u003E\u003C\u002Fa\u003E\u003Cbr\u002F\u003E\u003Cbr\u002F\u003Eblogs:\u003C\u002Fp\u003E\u003Cp data-pid=\"JKmvUz1D\"\u003E\u003Ca href=\"https:\u002F\u002Flink.zhihu.com\u002F?target=https%3A\u002F\u002Flilianweng.github.io\u002Flil-log\u002F2019\u002F11\u002F10\u002Fself-supervised-learning.html\" class=\" external\" target=\"_blank\" rel=\"nofollow noreferrer\"\u003E\u003Cspan class=\"invisible\"\u003Ehttps:\u002F\u002F\u003C\u002Fspan\u003E\u003Cspan class=\"visible\"\u003Elilianweng.github.io\u002Fli\u003C\u002Fspan\u003E\u003Cspan class=\"invisible\"\u003El-log\u002F2019\u002F11\u002F10\u002Fself-supervised-learning.html\u003C\u002Fspan\u003E\u003Cspan class=\"ellipsis\"\u003E\u003C\u002Fspan\u003E\u003C\u002Fa\u003E\u003C\u002Fp\u003E\u003Cp data-pid=\"T75EkvFh\"\u003ETalks:\u003C\u002Fp\u003E\u003Cp data-pid=\"ZbZIEZXi\"\u003ESelf-Supervised Learning. Yan Lecun. AAAI 2020. \u003Ca href=\"https:\u002F\u002Flink.zhihu.com\u002F?target=https%3A\u002F\u002Fdrive.google.com\u002Ffile\u002Fd\u002F1r-mDL4IX_hzZLDBKp8_e8VZqD7fOzBkF\u002Fview%3Fusp%3Dsharing\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\"\u003E[pdf]\u003C\u002Fa\u003E\u003C\u002Fp\u003E\u003Cp data-pid=\"am_c9_3F\"\u003EGraph Embeddings, Content Understanding, &amp; Self-Supervised Learning. Yann LeCun. (NYU &amp; FAIR) \u003Ca href=\"https:\u002F\u002Flink.zhihu.com\u002F?target=https%3A\u002F\u002Fdrive.google.com\u002Ffile\u002Fd\u002F12pDCno02FJPDEBk4iGuuaj8b2rr48Hh0\u002Fview\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\"\u003E[pdf]\u003C\u002Fa\u003E\u003Ca href=\"https:\u002F\u002Flink.zhihu.com\u002F?target=https%3A\u002F\u002Fwww.youtube.com\u002Fwatch%3Fv%3DUGPT64wo7lU\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\"\u003E[video]\u003C\u002Fa\u003E\u003C\u002Fp\u003E\u003Cp data-pid=\"a-xeNWAl\"\u003ESelf-supervised learning: could machines learn like humans? Yann LeCun @EPFL. \u003Ca href=\"https:\u002F\u002Flink.zhihu.com\u002F?target=https%3A\u002F\u002Fwww.youtube.com\u002Fwatch%3Fv%3D7I0Qt7GALVk\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\"\u003E[video]\u003C\u002Fa\u003E\u003C\u002Fp\u003E\u003Chr\u002F\u003E\u003Ch2\u003E后记：\u003C\u002Fh2\u003E\u003Cp data-pid=\"GGc3HhlA\"\u003ESelf-supervised Learning在CV、NLP、Graph、RL、RecSys等领域已经取得了很awesome的效果。如何更好的挖掘无标签数据中的知识？如何和有监督数据更好地结合学习？仍然都是开放的问题，通过已有的研究工作，我们有充足的理由相信: \u003C\u002Fp\u003E\u003Cp data-pid=\"ndYxEwLC\"\u003ESelf-supervised Learning is the future of AI！\u003C\u002Fp\u003E\u003Ch2\u003E本文介绍的论文集合：\u003C\u002Fh2\u003E\u003Cp data-pid=\"0cEFgTiz\"\u003E\u003Ca href=\"https:\u002F\u002Flink.zhihu.com\u002F?target=https%3A\u002F\u002Fgithub.com\u002Fguyulongcs\u002FAwesome-Self-supervised-Learning-papers\" class=\" external\" target=\"_blank\" rel=\"nofollow noreferrer\"\u003E\u003Cspan class=\"invisible\"\u003Ehttps:\u002F\u002F\u003C\u002Fspan\u003E\u003Cspan class=\"visible\"\u003Egithub.com\u002Fguyulongcs\u002FA\u003C\u002Fspan\u003E\u003Cspan class=\"invisible\"\u003Ewesome-Self-supervised-Learning-papers\u003C\u002Fspan\u003E\u003Cspan class=\"ellipsis\"\u003E\u003C\u002Fspan\u003E\u003C\u002Fa\u003E\u003C\u002Fp\u003E\u003Chr\u002F\u003E\u003Ch2\u003E关于搜索推荐广告排序艺术：\u003C\u002Fh2\u003E\u003Cp data-pid=\"DSflUMrk\"\u003E搜索推荐广告排序艺术，聚焦人工智能、深度学习在互联网搜索、推荐、广告中的应用，这里有最前沿的学术论文、最新的互联网公司技术分享，欢迎对互联网、搜索、推荐、广告、排序算法、深度学习、AI感兴趣的朋友们，关注微信公众号\u002F知乎专栏：搜索推荐广告排序艺术。\u003C\u002Fp\u003E\u003Cp data-pid=\"BIHfgqu5\"\u003E微信公众号：搜索推荐广告排序艺术\u003C\u002Fp\u003E\u003Cp data-pid=\"uAykUPFg\"\u003E知乎专栏：\u003C\u002Fp\u003E\u003Ca href=\"https:\u002F\u002Fwww.zhihu.com\u002Fcolumn\u002Fc_1288235772122718208\" data-draft-node=\"block\" data-draft-type=\"link-card\" data-image=\"https:\u002F\u002Fpic2.zhimg.com\u002Fv2-b6546a28845e426a25b971ce7fcab919_ipico.jpg\" data-image-width=\"200\" data-image-height=\"200\" class=\"internal\"\u003E搜索推荐广告排序艺术\u003C\u002Fa\u003E\u003Cp\u003E\u003C\u002Fp\u003E","adminClosedComment":false,"topics":[{"url":"https:\u002F\u002Fwww.zhihu.com\u002Fapi\u002Fv4\u002Ftopics\u002F19813032","type":"topic","id":"19813032","name":"深度学习（Deep Learning）"},{"url":"https:\u002F\u002Fwww.zhihu.com\u002Fapi\u002Fv4\u002Ftopics\u002F20088404","type":"topic","id":"20088404","name":"半监督学习"},{"url":"https:\u002F\u002Fwww.zhihu.com\u002Fapi\u002Fv4\u002Ftopics\u002F19590194","type":"topic","id":"19590194","name":"无监督学习"}],"voteupCount":58,"voting":0,"heavyUpStatus":"allow_heavy_up","column":{"description":"","canManage":false,"intro":"聚焦深度学习在互联网搜索推荐广告的排序算法。","isFollowing":false,"urlToken":"c_1288235772122718208","id":"c_1288235772122718208","articlesCount":10,"acceptSubmission":true,"title":"互联网学社","url":"https:\u002F\u002Fzhuanlan.zhihu.com\u002Fc_1288235772122718208","commentPermission":"all","created":1599919343,"updated":1623234949,"imageUrl":"https:\u002F\u002Fpicx.zhimg.com\u002F4b70deef7_720w.jpg?source=172ae18b","author":{"isFollowed":false,"avatarUrlTemplate":"https:\u002F\u002Fpic1.zhimg.com\u002Fv2-5e488c2bdf6db9763d3defb3b20d6089.jpg?source=172ae18b","uid":"926456305063985152","userType":"people","isFollowing":false,"urlToken":"gu-yu-long-eric","id":"bcbc6dd5b960ac12029354ab6fa06769","description":"guyulongcs@gmail.com","name":"谷育龙Eric","isAdvertiser":false,"headline":"字节跳动抖音推荐长期招聘算法工程师和实习生，欢迎联系!","gender":1,"url":"\u002Fpeople\u002Fbcbc6dd5b960ac12029354ab6fa06769","avatarUrl":"https:\u002F\u002Fpicx.zhimg.com\u002Fv2-5e488c2bdf6db9763d3defb3b20d6089_l.jpg?source=172ae18b","isOrg":false,"type":"people"},"followers":32,"type":"column"},"commentCount":0,"contributions":[{"id":27463434,"state":"accepted","type":"first_publish","column":{"description":"","canManage":false,"intro":"聚焦深度学习在互联网搜索推荐广告的排序算法。","isFollowing":false,"urlToken":"c_1288235772122718208","id":"c_1288235772122718208","articlesCount":10,"acceptSubmission":true,"title":"互联网学社","url":"https:\u002F\u002Fzhuanlan.zhihu.com\u002Fc_1288235772122718208","commentPermission":"all","created":1599919343,"updated":1623234949,"imageUrl":"https:\u002F\u002Fpicx.zhimg.com\u002F4b70deef7_720w.jpg?source=172ae18b","author":{"isFollowed":false,"avatarUrlTemplate":"https:\u002F\u002Fpic1.zhimg.com\u002Fv2-5e488c2bdf6db9763d3defb3b20d6089.jpg?source=172ae18b","uid":"926456305063985152","userType":"people","isFollowing":false,"urlToken":"gu-yu-long-eric","id":"bcbc6dd5b960ac12029354ab6fa06769","description":"guyulongcs@gmail.com","name":"谷育龙Eric","isAdvertiser":false,"headline":"字节跳动抖音推荐长期招聘算法工程师和实习生，欢迎联系!","gender":1,"url":"\u002Fpeople\u002Fbcbc6dd5b960ac12029354ab6fa06769","avatarUrl":"https:\u002F\u002Fpicx.zhimg.com\u002Fv2-5e488c2bdf6db9763d3defb3b20d6089_l.jpg?source=172ae18b","isOrg":false,"type":"people"},"followers":32,"type":"column"}},{"id":43024682,"state":"accepted","type":"first_publish","column":{"description":"","canManage":false,"intro":"搜广推文章合集","isFollowing":false,"urlToken":"c_1290942732261769216","id":"c_1290942732261769216","articlesCount":305,"acceptSubmission":false,"title":"推荐+广告+搜索","url":"https:\u002F\u002Fzhuanlan.zhihu.com\u002Fc_1290942732261769216","commentPermission":"all","created":1600564732,"updated":1642161673,"imageUrl":"https:\u002F\u002Fpicx.zhimg.com\u002F4b70deef7_720w.jpg?source=172ae18b","author":{"isFollowed":false,"avatarUrlTemplate":"https:\u002F\u002Fpic1.zhimg.com\u002Fv2-25deab0d86c6a5c67cb5803494771736.jpg?source=172ae18b","uid":"748889366583074816","userType":"people","isFollowing":false,"urlToken":"Sing-F","id":"4c7d0fa5cfea8b2f3e201cc74b5d02e6","description":"GNN，推荐，广告，搜索, KG, NLP, CV","name":"郭达森","isAdvertiser":false,"headline":"","gender":1,"url":"\u002Fpeople\u002F4c7d0fa5cfea8b2f3e201cc74b5d02e6","avatarUrl":"https:\u002F\u002Fpic1.zhimg.com\u002Fv2-25deab0d86c6a5c67cb5803494771736_l.jpg?source=172ae18b","isOrg":false,"type":"people"},"followers":290,"type":"column"}}],"isTitleImageFullScreen":false,"upvotedFollowees":[],"commercialInfo":{"isCommercial":false,"plugin":{}},"suggestEdit":{"status":false,"reason":"","tip":"","url":"","title":""},"reason":"","annotationAction":[],"canTip":true,"tipjarorsCount":0,"isLabeled":false,"hasPublishingDraft":false,"isFavorited":true,"favlistsCount":100,"isNormal":true,"status":0,"shareText":"自监督学习: 人工智能的未来 - 来自知乎专栏「互联网学社」，作者: 谷育龙Eric https:\u002F\u002Fzhuanlan.zhihu.com\u002Fp\u002F270547809 （想看更多？下载 @知乎 App：http:\u002F\u002Fweibo.com\u002Fp\u002F100404711598 ）","canComment":{"status":true,"reason":"请输入评论，您的评论将会由作者筛选后显示"},"mcnFpShow":-1,"isVisible":true,"isLiked":false,"likedCount":11,"hasColumn":true,"republishers":[{"isFollowed":false,"avatarUrlTemplate":"https:\u002F\u002Fpic1.zhimg.com\u002Fv2-25deab0d86c6a5c67cb5803494771736.jpg?source=172ae18b","uid":"748889366583074816","userType":"people","isFollowing":false,"urlToken":"Sing-F","id":"4c7d0fa5cfea8b2f3e201cc74b5d02e6","description":"GNN，推荐，广告，搜索, KG, NLP, CV","name":"郭达森","isAdvertiser":false,"headline":"","gender":1,"url":"\u002Fpeople\u002F4c7d0fa5cfea8b2f3e201cc74b5d02e6","avatarUrl":"https:\u002F\u002Fpic1.zhimg.com\u002Fv2-25deab0d86c6a5c67cb5803494771736_l.jpg?source=172ae18b","isOrg":false,"type":"people"}],"isNewLinkCard":true,"emojiReaction":{"cryFaceCount":0,"cryFaceHasSet":false,"hugCount":0,"hugHasSet":false,"likeCount":11,"likeHasSet":false,"onlookerCount":0,"onlookerHasSet":false},"abParam":{"qaHiddenVoteup":"0"},"attachedInfo":"kgIkCgkxNDgyNTIwNDkSCTI3MDU0NzgwORgHIgpJTUFHRV9URVhU","shareGuide":{"hasPositiveBubble":false,"hasTimeBubble":false,"hitShareGuideCluster":false},"settings":{"tableOfContents":{"enabled":false}},"canReference":false,"reactionInstruction":{},"entityWords":[{"name":"self-supervised learning","mention":"self-supervised learning","matchorder":1,"begin":42520,"end":42544,"entityid":0,"isBookMark":false,"link":{"linkType":0,"linkUrl":"","docType":"","topicToken":""},"entityClass":"Medical","score":0,"attachedInfoBytes":"sgJlChhzZWxmLXN1cGVydmlzZWQgbGVhcm5pbmcSB01lZGljYWwYmMwCILDMAigBNQAAAAA6B2FydGljbGVAAEgAUiRhYjU0Yjc2MC00N2I2LTQ3ZWEtODBlMy1mYjYzYmZjOTNmMDU=","isOnAB":false,"isNatural":1,"isDelete":false},{"name":"finetune","mention":"finetune","matchorder":1,"begin":990,"end":998,"entityid":0,"isBookMark":false,"link":{"linkType":0,"linkUrl":"","docType":"","topicToken":""},"entityClass":"OtherTerm","score":0,"attachedInfoBytes":"sgJVCghmaW5ldHVuZRIJT3RoZXJUZXJtGN4HIOYHKAE1AAAAADoHYXJ0aWNsZUAASABSJGFiNTRiNzYwLTQ3YjYtNDdlYS04MGUzLWZiNjNiZmM5M2YwNQ==","isOnAB":false,"isNatural":1,"isDelete":false},{"name":"Zero-shot","mention":"Zero-shot","matchorder":3,"begin":28941,"end":28950,"entityid":0,"isBookMark":false,"link":{"linkType":0,"linkUrl":"","docType":"","topicToken":""},"entityClass":"OtherTerm","score":0,"attachedInfoBytes":"sgJYCglaZXJvLXNob3QSCU90aGVyVGVybRiN4gEgluIBKAM1AAAAADoHYXJ0aWNsZUAASABSJGFiNTRiNzYwLTQ3YjYtNDdlYS04MGUzLWZiNjNiZmM5M2YwNQ==","isOnAB":false,"isNatural":1,"isDelete":false},{"name":"self-attention head","mention":"self-attention head","matchorder":1,"begin":25905,"end":25924,"entityid":0,"isBookMark":false,"link":{"linkType":0,"linkUrl":"","docType":"","topicToken":""},"entityClass":"OtherTerm","score":0,"attachedInfoBytes":"sgJiChNzZWxmLWF0dGVudGlvbiBoZWFkEglPdGhlclRlcm0YscoBIMTKASgBNQAAAAA6B2FydGljbGVAAEgAUiRhYjU0Yjc2MC00N2I2LTQ3ZWEtODBlMy1mYjYzYmZjOTNmMDU=","isOnAB":false,"isNatural":1,"isDelete":false},{"name":"query-item","mention":"query-item","matchorder":1,"begin":34071,"end":34081,"entityid":0,"isBookMark":false,"link":{"linkType":0,"linkUrl":"","docType":"","topicToken":""},"entityClass":"Math","score":0,"attachedInfoBytes":"sgJUCgpxdWVyeS1pdGVtEgRNYXRoGJeKAiChigIoATUAAAAAOgdhcnRpY2xlQABIAFIkYWI1NGI3NjAtNDdiNi00N2VhLTgwZTMtZmI2M2JmYzkzZjA1","isOnAB":false,"isNatural":1,"isDelete":false},{"name":"Self-supervised Learning","mention":"Self-supervised Learning","matchorder":6,"begin":401,"end":425,"entityid":0,"isBookMark":false,"link":{"linkType":0,"linkUrl":"","docType":"","topicToken":""},"entityClass":"Medical","score":0,"attachedInfoBytes":"sgJjChhTZWxmLXN1cGVydmlzZWQgTGVhcm5pbmcSB01lZGljYWwYkQMgqQMoBjUAAAAAOgdhcnRpY2xlQABIAFIkYWI1NGI3NjAtNDdiNi00N2VhLTgwZTMtZmI2M2JmYzkzZjA1","isOnAB":false,"isNatural":1,"isDelete":false},{"name":"Self-supervised learning","mention":"Self-supervised learning","matchorder":2,"begin":32906,"end":32930,"entityid":0,"isBookMark":false,"link":{"linkType":0,"linkUrl":"","docType":"","topicToken":""},"entityClass":"OtherTerm","score":0,"attachedInfoBytes":"sgJnChhTZWxmLXN1cGVydmlzZWQgbGVhcm5pbmcSCU90aGVyVGVybRiKgQIgooECKAI1AAAAADoHYXJ0aWNsZUAASABSJGFiNTRiNzYwLTQ3YjYtNDdlYS04MGUzLWZiNjNiZmM5M2YwNQ==","isOnAB":false,"isNatural":1,"isDelete":false},{"name":"Named-entity recognition","mention":"Named-entity recognition","matchorder":1,"begin":19096,"end":19120,"entityid":0,"isBookMark":false,"link":{"linkType":0,"linkUrl":"","docType":"","topicToken":""},"entityClass":"Medical","score":0,"attachedInfoBytes":"sgJlChhOYW1lZC1lbnRpdHkgcmVjb2duaXRpb24SB01lZGljYWwYmJUBILCVASgBNQAAAAA6B2FydGljbGVAAEgAUiRhYjU0Yjc2MC00N2I2LTQ3ZWEtODBlMy1mYjYzYmZjOTNmMDU=","isOnAB":false,"isNatural":1,"isDelete":false},{"name":"Few-shot","mention":"Few-shot","matchorder":1,"begin":30505,"end":30513,"entityid":0,"isBookMark":false,"link":{"linkType":0,"linkUrl":"","docType":"","topicToken":""},"entityClass":"OtherTerm","score":0,"attachedInfoBytes":"sgJXCghGZXctc2hvdBIJT3RoZXJUZXJtGKnuASCx7gEoATUAAAAAOgdhcnRpY2xlQABIAFIkYWI1NGI3NjAtNDdiNi00N2VhLTgwZTMtZmI2M2JmYzkzZjA1","isOnAB":false,"isNatural":1,"isDelete":false}]}},"columns":{"c_1288235772122718208":{"description":"","canManage":false,"intro":"聚焦深度学习在互联网搜索推荐广告的排序算法。","isFollowing":false,"urlToken":"c_1288235772122718208","id":"c_1288235772122718208","articlesCount":10,"acceptSubmission":true,"title":"互联网学社","url":"https:\u002F\u002Fzhuanlan.zhihu.com\u002Fc_1288235772122718208","commentPermission":"all","created":1599919343,"updated":1623234949,"imageUrl":"https:\u002F\u002Fpicx.zhimg.com\u002F4b70deef7_720w.jpg?source=172ae18b","author":{"isFollowed":false,"avatarUrlTemplate":"https:\u002F\u002Fpic1.zhimg.com\u002Fv2-5e488c2bdf6db9763d3defb3b20d6089.jpg?source=172ae18b","uid":"926456305063985152","userType":"people","isFollowing":false,"urlToken":"gu-yu-long-eric","id":"bcbc6dd5b960ac12029354ab6fa06769","description":"guyulongcs@gmail.com","name":"谷育龙Eric","isAdvertiser":false,"headline":"字节跳动抖音推荐长期招聘算法工程师和实习生，欢迎联系!","gender":1,"url":"\u002Fpeople\u002Fbcbc6dd5b960ac12029354ab6fa06769","avatarUrl":"https:\u002F\u002Fpicx.zhimg.com\u002Fv2-5e488c2bdf6db9763d3defb3b20d6089_l.jpg?source=172ae18b","isOrg":false,"type":"people"},"followers":32,"type":"column"},"c_1290942732261769216":{"description":"","canManage":false,"intro":"搜广推文章合集","isFollowing":false,"urlToken":"c_1290942732261769216","id":"c_1290942732261769216","articlesCount":305,"acceptSubmission":false,"title":"推荐+广告+搜索","url":"https:\u002F\u002Fzhuanlan.zhihu.com\u002Fc_1290942732261769216","commentPermission":"all","created":1600564732,"updated":1642161673,"imageUrl":"https:\u002F\u002Fpicx.zhimg.com\u002F4b70deef7_720w.jpg?source=172ae18b","author":{"isFollowed":false,"avatarUrlTemplate":"https:\u002F\u002Fpic1.zhimg.com\u002Fv2-25deab0d86c6a5c67cb5803494771736.jpg?source=172ae18b","uid":"748889366583074816","userType":"people","isFollowing":false,"urlToken":"Sing-F","id":"4c7d0fa5cfea8b2f3e201cc74b5d02e6","description":"GNN，推荐，广告，搜索, KG, NLP, CV","name":"郭达森","isAdvertiser":false,"headline":"","gender":1,"url":"\u002Fpeople\u002F4c7d0fa5cfea8b2f3e201cc74b5d02e6","avatarUrl":"https:\u002F\u002Fpic1.zhimg.com\u002Fv2-25deab0d86c6a5c67cb5803494771736_l.jpg?source=172ae18b","isOrg":false,"type":"people"},"followers":290,"type":"column"}},"topics":{},"roundtables":{},"favlists":{},"comments":{},"notifications":{},"ebooks":{},"activities":{},"feeds":{},"pins":{},"promotions":{},"drafts":{},"chats":{},"posts":{},"clubs":{},"clubTags":{},"zvideos":{},"zvideoContributions":{},"briefs":{},"eduCourses":{}},"currentUser":"9f2f6a5227e07ce2e98d5ab7c43ce1f5","account":{"lockLevel":{},"unlockTicketStatus":false,"unlockTicket":null,"challenge":[],"errorStatus":false,"message":"","isFetching":false,"accountInfo":{},"urlToken":{"loading":false},"cardUserInfo":{"vipInfo":{}},"handleWidget":{},"widgetList":[],"userWidgetId":""},"settings":{"socialBind":null,"inboxMsg":null,"notification":{},"email":{},"privacyFlag":null,"blockedUsers":{"isFetching":false,"paging":{"pageNo":1,"pageSize":6},"data":[]},"blockedFollowees":{"isFetching":false,"paging":{"pageNo":1,"pageSize":6},"data":[]},"ignoredTopics":{"isFetching":false,"paging":{"pageNo":1,"pageSize":6},"data":[]},"restrictedTopics":null,"laboratory":{}},"notification":{},"people":{"profileStatus":{},"activitiesByUser":{},"answersByUser":{},"answersSortByVotesByUser":{},"answersIncludedByUser":{},"votedAnswersByUser":{},"thankedAnswersByUser":{},"voteAnswersByUser":{},"thankAnswersByUser":{},"topicAnswersByUser":{},"zvideosByUser":{},"articlesByUser":{},"articlesSortByVotesByUser":{},"articlesIncludedByUser":{},"pinsByUser":{},"questionsByUser":{},"commercialQuestionsByUser":{},"favlistsByUser":{},"followingByUser":{},"followersByUser":{},"mutualsByUser":{},"followingColumnsByUser":{},"followingQuestionsByUser":{},"followingFavlistsByUser":{},"followingTopicsByUser":{},"publicationsByUser":{},"columnsByUser":{},"allFavlistsByUser":{},"brands":null,"creationsByUser":{},"creationsSortByVotesByUser":{},"creationsFeed":{},"infinity":{},"batchUsers":{},"profileInfinity":null},"env":{"ab":{"config":{"params":[{"id":"pc_ppt_publish","type":"Int","value":"1","layerId":"pc_ppt_publish"},{"id":"pinvideo","type":"Int","value":"1","layerId":"pinvideo"},{"id":"player_vendors","type":"Int","value":"1","layerId":"player_vendors"},{"id":"ks_prefetch","type":"Int","value":"1","layerId":"ks_prefetch"},{"id":"pc_player_hevc","type":"Int","value":"1","layerId":"pc_player_hevc"},{"id":"update_react","type":"Int","value":"0","layerId":"update_react"}],"experiments":[{"expId":"pc_ppt_publish-2_v2","expPrefix":"pc_ppt_publish","isDynamicallyUpdated":false,"isRuntime":false,"includeTriggerInfo":false},{"expId":"player_vendors-2_v22","expPrefix":"player_vendors","isDynamicallyUpdated":false,"isRuntime":false,"includeTriggerInfo":false},{"expId":"ks_prefetch-2_v7","expPrefix":"ks_prefetch","isDynamicallyUpdated":false,"isRuntime":false,"includeTriggerInfo":false},{"expId":"pc_player_hevc-2_v4","expPrefix":"pc_player_hevc","isDynamicallyUpdated":false,"isRuntime":false,"includeTriggerInfo":false},{"expId":"update_react-1_v35","expPrefix":"update_react","isDynamicallyUpdated":false,"isRuntime":false,"includeTriggerInfo":false}],"chains":[{"chainId":"_all_"}],"encodedParams":"ClwbAD8ARwC0AGkBagF0ATsCzALXAtgCtwPWBBEFUQWLBYwFngUxBusGJwd0CHYIeQg\u002FCWAJ9AlJCmUKawq+CkMLcQuHC40L1wvgC+UL5gs4DHEMjwysDMMMyQz4DBIuBwAAAAABAAAAAAAABAABAAABAAAAAAAGAAIDAAAAAAEAAAUCAQAAAgYAAAIAAA=="},"triggers":{}},"abV2":{"config":{"paramMap":{"ws_report":{"value":"v2","abId":"rl-report_web-1"},"in_editor_title":{"value":"1","abId":"rl-pineditor_title-1"},"ws_pc_route":{"value":"0"}},"abMap":{"rl-report_web-1":{"abId":"rl-report_web-1","layerId":"rl-report_web","diversionType":2},"rl-pineditor_title-1":{"abId":"rl-pineditor_title-1","layerId":"rl-pineditor_title","diversionType":2}}},"triggers":{}},"userAgent":{"Edge":false,"IE":false,"Wechat":false,"Weibo":false,"QQ":false,"MQQBrowser":false,"Qzone":false,"Mobile":false,"Android":false,"iOS":false,"isAppleDevice":false,"Zhihu":false,"ZhihuHybrid":false,"isBot":false,"Tablet":false,"UC":false,"Quark":false,"Sogou":false,"Qihoo":false,"Baidu":false,"BaiduApp":false,"Safari":false,"GoogleBot":false,"AndroidDaily":false,"iOSDaily":false,"WxMiniProgram":false,"BaiduMiniProgram":false,"QQMiniProgram":false,"JDMiniProgram":false,"isWebView":false,"isMiniProgram":false,"origin":"Mozilla\u002F5.0 (Windows NT 10.0; Win64; x64) AppleWebKit\u002F537.36 (KHTML, like Gecko) Chrome\u002F110.0.0.0 Safari\u002F537.36 Edg\u002F110.0.1587.50"},"appViewConfig":{},"ctx":{"path":"\u002Fp\u002F270547809","query":{},"href":"http:\u002F\u002Fzhuanlan.zhihu.com\u002Fp\u002F270547809","host":"zhuanlan.zhihu.com"},"trafficSource":"production","edition":{"beijing":true,"baidu":false,"sogou":false,"baiduBeijing":false,"sogouBeijing":false,"sogouInput":false,"oppoSearch":false,"baiduSearch":false,"googleSearch":false,"shenma":false,"miniProgram":false,"xiaomi":false},"theme":"light","appHeaderTheme":{"current":"normal","disable":true,"normal":{"bgColor":"GBK99A"},"custom":{"bgColor":"GBK99A"}},"enableShortcut":true,"referer":"","xUDId":"AIBTRF8e0BWPTsQ04MtvS7zrmLZF8G7_4hc=","mode":"ssr","conf":{},"xTrafficFreeOrigin":"","ipInfo":{"cityName":"北京","countryName":"中国","regionName":"北京","countryCode":"CN"},"logged":true,"vars":{"passThroughHeaders":{}}},"me":{"columnContributions":[]},"label":{"recognizerLists":{}},"ecommerce":{},"comments":{"pagination":{},"collapsed":{},"reverse":{},"reviewing":{},"conversation":{},"parent":{}},"commentsV2":{"stickers":[],"commentWithPicPermission":{},"notificationsComments":{},"pagination":{},"collapsed":{},"reverse":{},"reviewing":{},"conversation":{},"conversationMore":{},"parent":{}},"pushNotifications":{"default":{"isFetching":false,"isDrained":false,"ids":[]},"follow":{"isFetching":false,"isDrained":false,"ids":[]},"vote_thank":{"isFetching":false,"isDrained":false,"ids":[]},"currentTab":"default","notificationsCount":{"default":0,"follow":0,"vote_thank":0}},"messages":{"data":{},"currentTab":"common","messageCount":0},"register":{"registerValidateSucceeded":null,"registerValidateErrors":{},"registerConfirmError":null,"sendDigitsError":null,"registerConfirmSucceeded":null},"login":{"loginUnregisteredError":false,"loginBindWechatError":false,"loginConfirmError":null,"sendDigitsError":null,"needSMSIdentify":false,"validateDigitsError":false,"loginConfirmSucceeded":null,"qrcodeLoginToken":"","qrcodeLoginScanStatus":0,"qrcodeLoginError":null,"qrcodeLoginReturnNewToken":false},"switches":{},"captcha":{"captchaNeeded":false,"captchaValidated":false},"sms":{"supportedCountries":[]},"chat":{"chats":{},"inbox":{"recents":{"isFetching":false,"isDrained":false,"isPrevDrained":false,"result":[],"next":null,"key":null},"strangers":{"isFetching":false,"isDrained":false,"isPrevDrained":false,"result":[],"next":null,"key":null},"friends":{"isFetching":false,"isDrained":false,"isPrevDrained":false,"result":[],"next":null,"key":null},"search":{"isFetching":false,"isDrained":false,"isPrevDrained":false,"result":[],"next":null,"key":null},"config":{"newCount":0,"strangerMessageSwitch":false,"strangerMessageUnread":false,"friendCount":0}},"global":{"isChatMqttExisted":false}},"emoticons":{"emoticonGroupList":[],"emoticonGroupDetail":{}},"creator":{"tools":{"question":{"invitationCount":{"questionFolloweeCount":0,"questionTotalCount":0}},"customPromotion":{"itemLists":{}},"recommend":{"recommendTimes":{}}},"explore":{},"rightsStatus":{},"levelUpperLimit":10,"mcn":{},"videoSupport":{},"textBenefit":{},"mcnManage":{},"tasks":{},"recentlyCreated":[],"announcement":{},"bannerList":[],"creatorsRecommendInfo":{}},"creators":{"bayesDomains":{"status":{},"options":{"topDomains":null,"allDomains":null,"editable":0},"contents":null},"school":{"tabs":[],"contents":[],"banner":null,"entities":{}},"faq":{"tabs":[],"article":{}},"knowledgeIncome":{},"safeguardRights":{},"analytics":{"all":{},"answer":{},"zvideo":{},"article":{},"pin":{},"singleContent":{}},"account":{"growthLevel":{}},"KMResource":{},"training":{},"ToolsQuestion":{"goodatTopics":[]},"ToolsHotspot":{"domains":[]},"ToolsSearchQuestion":{},"editorSetting":{},"MCNManage":{},"knowledgeTasks":{},"incomeAnalysis":{"income":{"aggregation":{}}},"creationManage":{"editModal":{"status":false}},"activity":{},"announcement":{},"home":{"currentCreatorUrlToken":null,"rights":[],"newRights":[],"applyStatus":{},"scoreInfo":{},"menusShowControlByServer":{"bVipRecomend":false,"creationRelationship":false},"newTasks":{"creatorTask":{"tasks":[],"des":[]}}}},"answers":{"voters":{},"copyrightApplicants":{},"favlists":{},"newAnswer":{},"entityWords":{},"concernedUpvoters":{},"simpleConcernedUpvoters":{},"paidContent":{},"settings":{}},"recommendation":{"homeRecommendations":[]},"shareTexts":{},"articles":{"voters":{},"concernedUpvoters":{}},"previewPost":{},"favlists":{"relations":{}},"columns":{"voters":{}},"reward":{"answer":{},"article":{},"question":{}},"video":{"data":{},"shareVideoDetail":{},"last":{}},"topstory":{"recommend":{"isFetching":false,"isDrained":false,"afterId":0,"items":[],"next":null},"follow":{"isFetching":false,"isDrained":false,"afterId":0,"items":[],"next":null},"room":{"meta":{},"isFetching":false,"afterId":0,"items":[],"next":null},"followWonderful":{"isFetching":false,"isDrained":false,"afterId":0,"items":[],"next":null},"sidebar":null,"announcement":{},"hotList":[],"guestFeeds":{"isFetching":false,"isDrained":false,"afterId":0,"items":[],"next":null},"followExtra":{"isNewUser":null,"isFetched":false,"followCount":0,"followers":[]},"hotDaily":{"data":[],"paging":{}},"hotHighlight":{"isFetching":false,"isDrained":false,"data":[],"paging":{}},"banner":{},"commercialBanner":{"show":false,"banner":{},"trackData":{}},"video":{"items":[],"next":null,"isLoading":false,"isDrained":false}},"readStatus":{},"column":{},"requestColumn":{"categories":[],"error":null},"articleContribution":{"contributeRequests":[],"deleteContributeIdList":[],"handledContributeIdList":[],"recommendedColumns":[],"pinnedColumns":[],"sentContributeRequestsIdList":[null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,"c_1288235772122718208","c_1290942732261769216",null]},"columnContribution":{"contributeRequests":[],"autoInviteEnabled":false,"recommendedContributors":[],"contributionInvitation":null},"draftHistory":{"history":{},"drafts":{}},"upload":{},"articleDraft":{"titleImage":"","titleImageSize":{},"isTitleImageFullScreen":false,"canTitleImageFullScreen":false,"title":"","titleImageUploading":false,"error":"","content":"","draftLoading":false,"updating":false,"globalLoading":false,"pendingVideo":{"resource":null,"error":null},"deleteFail":{"fail":false},"recommendTopics":[],"selectedColumn":0,"articleDisclaimers":[]},"articleDrafts":{"isDrained":false,"isLoading":false,"items":[]},"columnAutocomplete":{"users":[],"friends":[]},"columnCollection":{},"userProfit":{"permission":{"permissionStatus":{"zhiZixuan":0,"recommend":-1,"task":0,"plugin":0,"infinity":0},"visible":false}},"mcn":{"bindInfo":{},"memberCategoryList":[],"producerList":[],"categoryList":[],"lists":{},"banners":{},"protocolStatus":{"isAgreedNew":true,"isAgreedOld":true},"probationCountdownDays":0},"zvideos":{"campaignVideoList":{},"campaigns":{},"tagoreCategory":[],"recommendations":{},"insertable":{},"recruit":{"form":{"platform":"","nickname":"","followerCount":"","domain":"","contact":""},"submited":false,"ranking":[]},"club":{},"qyActivityData":{},"talkActivityData":{},"party2022ActivityData":{},"batchVideos":{},"contribution":{"selectedContribution":null,"campaign":null,"configs":{},"contributionLists":{},"recommendQuestions":{"isLoading":true,"paging":{"isEnd":false,"isStart":true,"totals":0},"data":[]},"questionSearchResults":{"isLoading":true,"paging":{"isEnd":false,"isStart":true,"totals":0},"data":[]}},"creationReferences":{},"zvideoCollection":{},"zvideoGrant":{},"collectData":{"isFetching":false,"list":[]},"videoSource":{"isLoaded":false}},"republish":{},"commentPermission":{},"creatorRightStatus":{"list":[]},"adPromotion":{"answer":{},"article":{}}},"fetchHost":"www.zhihu.com","subAppName":"column","spanName":"Post"}</script><script crossorigin="" src="./自监督学习_ 人工智能的未来 - 知乎_files/vendor.7248aedc1642e2c41466.js.下载"></script><script crossorigin="" src="./自监督学习_ 人工智能的未来 - 知乎_files/react.production.min.js.下载"></script><script crossorigin="" src="./自监督学习_ 人工智能的未来 - 知乎_files/react-dom.production.min.js.下载"></script><script crossorigin="" src="./自监督学习_ 人工智能的未来 - 知乎_files/react-dom-server.browser.production.min.js.下载"></script><script crossorigin="" src="./自监督学习_ 人工智能的未来 - 知乎_files/runtime.app.8951980b651d90ece530.js.下载"></script><script crossorigin="" src="./自监督学习_ 人工智能的未来 - 知乎_files/lib-2ec050f6.app.b6ce73129a02794d58c4.js.下载"></script><script crossorigin="" src="./自监督学习_ 人工智能的未来 - 知乎_files/lib-29107295.app.a5891096834538104d7e.js.下载"></script><script crossorigin="" src="./自监督学习_ 人工智能的未来 - 知乎_files/lib-79b5cf47.app.7ec9842ac1b68669fb3a.js.下载"></script><script crossorigin="" src="./自监督学习_ 人工智能的未来 - 知乎_files/lib-330004dc.app.d43e5dcb9f8ddaa160fd.js.下载"></script><script crossorigin="" src="./自监督学习_ 人工智能的未来 - 知乎_files/lib-0e5ce61e.app.04627552afda3b578ffb.js.下载"></script><script crossorigin="" src="./自监督学习_ 人工智能的未来 - 知乎_files/6335.app.2e6e1e2cd6970e66432f.js.下载"></script><script crossorigin="" src="./自监督学习_ 人工智能的未来 - 知乎_files/column.app.4b22994ff24875813122.js.下载"></script><script defer="" src="./自监督学习_ 人工智能的未来 - 知乎_files/aria.js.下载" id="ariascripts"></script><script src="./自监督学习_ 人工智能的未来 - 知乎_files/hm.js.下载" async=""></script><script crossorigin="" src="./自监督学习_ 人工智能的未来 - 知乎_files/zap.js.下载"></script><div><div style="display: none;"><i>想来知乎工作？请发送邮件到 jobs@zhihu.com</i></div></div><script src="./自监督学习_ 人工智能的未来 - 知乎_files/push.js.下载"></script><div><div><div class="css-8pdeid"></div></div></div><script crossorigin="" src="./自监督学习_ 人工智能的未来 - 知乎_files/js"></script><script crossorigin="" src="./自监督学习_ 人工智能的未来 - 知乎_files/emoticon.js.下载"></script><div><div><div class="Editable-languageSuggestions" style="left: -1179px; top: -999px;"><div><div class="Popover"><label class="Editable-languageSuggestionsInput Input-wrapper"><input autocomplete="off" role="combobox" aria-expanded="false" aria-autocomplete="list" aria-activedescendant="AutoComplete8-0" id="Popover7-toggle" aria-haspopup="true" aria-owns="Popover7-content" class="Input" placeholder="选择语言" value=""><svg width="24" height="24" viewBox="0 0 24 24" fill="#afbdcf" class="Zi Zi--Select"><path fill-rule="evenodd" d="M12.53 3.47a.75.75 0 0 0-1.06 0l-5 5a.75.75 0 0 0 1.06 1.06L12 5.06l4.47 4.47a.75.75 0 1 0 1.06-1.06l-5-5Zm-5 11a.75.75 0 0 0-1.06 1.06l5 5a.75.75 0 0 0 1.06 0l5-5a.75.75 0 1 0-1.06-1.06L12 18.94l-4.47-4.47Z" clip-rule="evenodd"></path></svg></label></div></div></div></div></div><script src="./自监督学习_ 人工智能的未来 - 知乎_files/MathJax.js.下载"></script></body></html>